Args: /home/lfr/LLCompiler/.setuptools-cmake-build/bin/llc-opt --dump-pass-pipeline -o=/home/lfr/LLCompiler/out.mlir --log-lever=debug --log-root=C:codingLLCompilerlog --mlir-print-ir-tree-dir=/home/lfr/LLCompiler/ir_tree --mlir-print-ir-after-all -basic-pipeline /home/lfr/LLCompiler/test/model_ir/resnet18.mlir --debug 
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ElementsAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistinctAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ResourceBlobManagerDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineBinaryOpExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineConstantExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineDimExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineMapStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::IntegerSetStorage)
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroOperands<mlir::TypeID::get<mlir::OpTrait::ZeroOperands>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneRegion<mlir::TypeID::get<mlir::OpTrait::OneRegion>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroResults<mlir::TypeID::get<mlir::OpTrait::ZeroResults>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroSuccessors<mlir::TypeID::get<mlir::OpTrait::ZeroSuccessors>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoRegionArguments<mlir::TypeID::get<mlir::OpTrait::NoRegionArguments>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoTerminator<mlir::TypeID::get<mlir::OpTrait::NoTerminator>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlock<mlir::TypeID::get<mlir::OpTrait::SingleBlock>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OpInvariants<mlir::TypeID::get<mlir::OpTrait::OpInvariants>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface::Trait<mlir::TypeID::get<mlir::BytecodeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AffineScope<mlir::TypeID::get<mlir::OpTrait::AffineScope>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIsolatedFromAbove<mlir::TypeID::get<mlir::OpTrait::IsIsolatedFromAbove>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SymbolTable<mlir::TypeID::get<mlir::OpTrait::SymbolTable>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface::Trait<mlir::TypeID::get<mlir::SymbolOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface::Trait<mlir::TypeID::get<mlir::OpAsmOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface::Trait<mlir::TypeID::get<mlir::RegionKindInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasOnlyGraphRegion<mlir::TypeID::get<mlir::OpTrait::HasOnlyGraphRegion>()::Empty>)
Load new dialect in Context func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectInlinerInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConvertToLLVMPatternInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface)
Load new dialect in Context cf
Load new dialect in Context arith
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithIntegerOverflowFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithRoundingModeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SelectLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferDeallocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueBoundsOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AutomaticAllocationScope<mlir::TypeID::get<mlir::OpTrait::AutomaticAllocationScope>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface::Trait<mlir::TypeID::get<mlir::CallableOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface::Trait<mlir::TypeID::get<mlir::FunctionOpInterface::Trait>()::Empty>)
Load new dialect in Context llh
Load new dialect in Context mlir_ex
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferShapedTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolicInferShapeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroRegions<mlir::TypeID::get<mlir::OpTrait::ZeroRegions>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneResult<mlir::TypeID::get<mlir::OpTrait::OneResult>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::Type>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::Type>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferShapedTypeOpInterface::Trait<mlir::TypeID::get<mlir::InferShapedTypeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolicInferShapeOpInterface::Trait<mlir::TypeID::get<mlir::SymbolicInferShapeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::IntegerType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::IntegerType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface::Trait<mlir::TypeID::get<mlir::InferTypeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ConstantLike<mlir::TypeID::get<mlir::OpTrait::ConstantLike>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicOperands<mlir::TypeID::get<mlir::OpTrait::VariadicOperands>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl<mlir::TypeID::get<mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable::Trait<mlir::TypeID::get<mlir::ConditionallySpeculatable::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AlwaysSpeculatableImplTrait<mlir::TypeID::get<mlir::OpTrait::AlwaysSpeculatableImplTrait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface::Trait<mlir::TypeID::get<mlir::MemoryEffectOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::MemRefsNormalizable<mlir::TypeID::get<mlir::OpTrait::MemRefsNormalizable>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface::Trait<mlir::TypeID::get<mlir::RegionBranchTerminatorOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ReturnLike<mlir::TypeID::get<mlir::OpTrait::ReturnLike>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsTerminator<mlir::TypeID::get<mlir::OpTrait::IsTerminator>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutSpecInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<1>::Impl<mlir::TypeID::get<mlir::OpTrait::AtLeastNOperands<1>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<2>::Impl<mlir::TypeID::get<mlir::OpTrait::NOperands<2>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<5>::Impl<mlir::TypeID::get<mlir::OpTrait::NOperands<5>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneOperand<mlir::TypeID::get<mlir::OpTrait::OneOperand>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultType<mlir::TypeID::get<mlir::OpTrait::SameOperandsAndResultType>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultRank<mlir::TypeID::get<mlir::OpTrait::SameOperandsAndResultRank>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ResultsBroadcastableShape<mlir::TypeID::get<mlir::OpTrait::ResultsBroadcastableShape>()::Empty>)
Pass Manager with 5 passes:
builtin.module(ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::OpToOpPassAdaptor)
operation-legalization,inline{default-pipeline=canonicalize inlining-threshold=4294967295 max-iterations=4 },infer-symbol-shape,load-weight,canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})

[2024-09-28 23:47:04.511] [info] ----- run in pass: Operationlegalization -----
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectFoldInterface)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x558277e52e50) {
  "func.return"(%195) : (tensor<?x1000xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e52db0) {
  "llh.symbolic_bind"(%195, %123) <{expressions = affine_map<()[s0, s1] -> (s0, 1000)>}> : (tensor<?x1000xf32>, i64) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasRecursiveMemoryEffects<mlir::TypeID::get<mlir::OpTrait::HasRecursiveMemoryEffects>()::Empty>)

  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%187, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%188, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%189, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%190, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  "llh.symbolic_bind"(%191, %123) <{expressions = affine_map<()[s0, s1] -> (s0, 512, 1, 1)>}> : (tensor<?x512x1x1xf32>, i64) -> ()
  %192 = "llh.flatten"(%191, %0) : (tensor<?x512x1x1xf32>, i64) -> tensor<?x512xf32>
  "llh.symbolic_bind"(%192, %123) <{expressions = affine_map<()[s0, s1] -> (s0, 512)>}> : (tensor<?x512xf32>, i64) -> ()
  %193 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %194 = "llh.matmul"(%192, %193) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %195 = "llh.add"(%194, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %195 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e52cb0) {
  %195 = "llh.add"(%194, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.matmul'(0x558277e52bc0) {
  %194 = "llh.matmul"(%192, %193) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.transpose'(0x558277e52af0) {
  %193 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e51910) {
  "llh.symbolic_bind"(%192, %123) <{expressions = affine_map<()[s0, s1] -> (s0, 512)>}> : (tensor<?x512xf32>, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%187, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%188, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%189, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%190, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  "llh.symbolic_bind"(%191, %123) <{expressions = affine_map<()[s0, s1] -> (s0, 512, 1, 1)>}> : (tensor<?x512x1x1xf32>, i64) -> ()
  %192 = "llh.flatten"(%191, %0) : (tensor<?x512x1x1xf32>, i64) -> tensor<?x512xf32>
  %193 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %194 = "llh.matmul"(%192, %193) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %195 = "llh.add"(%194, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %195 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.flatten'(0x558277e51810) {
  %192 = "llh.flatten"(%191, %0) : (tensor<?x512x1x1xf32>, i64) -> tensor<?x512xf32>


  * Pattern {anonymous}::replaceFlattenOp : 'llh.flatten -> ()' {
Trying to match "{anonymous}::replaceFlattenOp"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::llh::detail::ConstantOpGenericAdaptorBase::Properties)
"{anonymous}::replaceFlattenOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%187, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%188, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%189, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%190, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  "llh.symbolic_bind"(%191, %123) <{expressions = affine_map<()[s0, s1] -> (s0, 512, 1, 1)>}> : (tensor<?x512x1x1xf32>, i64) -> ()
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e66190) {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64


  * Pattern {anonymous}::BraodcastableScalarToTensor : 'llh.constant -> ()' {
Trying to match "{anonymous}::BraodcastableScalarToTensor"
"{anonymous}::BraodcastableScalarToTensor" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e3ef70) {
  "llh.symbolic_bind"(%191, %123) <{expressions = affine_map<()[s0, s1] -> (s0, 512, 1, 1)>}> : (tensor<?x512x1x1xf32>, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%187, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%188, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%189, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%190, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.adaptive_average_pool'(0x558277e660a0) {
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e65fd0) {
  "llh.symbolic_bind"(%190, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%187, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%188, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%189, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e65ef0) {
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e65e20) {
  "llh.symbolic_bind"(%189, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%187, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%188, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e65d20) {
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e65c50) {
  "llh.symbolic_bind"(%188, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%187, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e65b00) {
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e65a30) {
  "llh.symbolic_bind"(%187, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e65930) {
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e65820) {
  "llh.symbolic_bind"(%186, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e64730) {
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e64660) {
  "llh.symbolic_bind"(%185, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e64510) {
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e64440) {
  "llh.symbolic_bind"(%184, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e64340) {
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e64230) {
  "llh.symbolic_bind"(%183, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e64150) {
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e64080) {
  "llh.symbolic_bind"(%182, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e63f80) {
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e63eb0) {
  "llh.symbolic_bind"(%181, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63d60) {
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e63c90) {
  "llh.symbolic_bind"(%180, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63b90) {
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e63a80) {
  "llh.symbolic_bind"(%179, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63930) {
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e63860) {
  "llh.symbolic_bind"(%178, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63760) {
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e63650) {
  "llh.symbolic_bind"(%177, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e63570) {
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e634a0) {
  "llh.symbolic_bind"(%176, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63350) {
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e63280) {
  "llh.symbolic_bind"(%175, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 512, (s1 - 1) floordiv 32 + 1, (s1 - 1) floordiv 32 + 1)>}> : (tensor<?x512x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63180) {
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e63070) {
  "llh.symbolic_bind"(%174, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e62f90) {
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e62ec0) {
  "llh.symbolic_bind"(%173, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e62dc0) {
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e62cf0) {
  "llh.symbolic_bind"(%172, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e62ba0) {
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e62ad0) {
  "llh.symbolic_bind"(%171, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e629d0) {
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e385b0) {
  "llh.symbolic_bind"(%170, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e384d0) {
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e38400) {
  "llh.symbolic_bind"(%169, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e382b0) {
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e620f0) {
  "llh.symbolic_bind"(%168, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e61ff0) {
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e61ee0) {
  "llh.symbolic_bind"(%167, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e61e00) {
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e61d30) {
  "llh.symbolic_bind"(%166, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e61c30) {
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e61b60) {
  "llh.symbolic_bind"(%165, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e61a10) {
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e61940) {
  "llh.symbolic_bind"(%164, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e61840) {
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e60f20) {
  "llh.symbolic_bind"(%163, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e605c0) {
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e604f0) {
  "llh.symbolic_bind"(%162, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e603f0) {
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e602e0) {
  "llh.symbolic_bind"(%161, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e60200) {
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e60130) {
  "llh.symbolic_bind"(%160, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5ffe0) {
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5ff10) {
  "llh.symbolic_bind"(%159, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 256, (s1 - 1) floordiv 16 + 1, (s1 - 1) floordiv 16 + 1)>}> : (tensor<?x256x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5fe10) {
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5f4f0) {
  "llh.symbolic_bind"(%158, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5f410) {
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5f340) {
  "llh.symbolic_bind"(%157, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5f240) {
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5e960) {
  "llh.symbolic_bind"(%156, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5e810) {
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5e740) {
  "llh.symbolic_bind"(%155, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5e640) {
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5e530) {
  "llh.symbolic_bind"(%154, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5e450) {
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5e380) {
  "llh.symbolic_bind"(%153, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5e230) {
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5e160) {
  "llh.symbolic_bind"(%152, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5e060) {
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5df50) {
  "llh.symbolic_bind"(%151, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5d660) {
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5d590) {
  "llh.symbolic_bind"(%150, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5d490) {
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5d3c0) {
  "llh.symbolic_bind"(%149, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5d270) {
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5c990) {
  "llh.symbolic_bind"(%148, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5c890) {
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5c780) {
  "llh.symbolic_bind"(%147, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5c630) {
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5c560) {
  "llh.symbolic_bind"(%146, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5b450) {
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5b340) {
  "llh.symbolic_bind"(%145, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5b260) {
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5b190) {
  "llh.symbolic_bind"(%144, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5b040) {
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5af70) {
  "llh.symbolic_bind"(%143, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 128, (s1 - 1) floordiv 8 + 1, (s1 - 1) floordiv 8 + 1)>}> : (tensor<?x128x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5ae70) {
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5ad60) {
  "llh.symbolic_bind"(%142, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5ac80) {
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5a3a0) {
  "llh.symbolic_bind"(%141, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5a2a0) {
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e5a1d0) {
  "llh.symbolic_bind"(%140, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5a080) {
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e59fb0) {
  "llh.symbolic_bind"(%139, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e59eb0) {
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e59da0) {
  "llh.symbolic_bind"(%138, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e59cc0) {
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e59bf0) {
  "llh.symbolic_bind"(%137, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e59aa0) {
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e599d0) {
  "llh.symbolic_bind"(%136, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e598d0) {
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e597c0) {
  "llh.symbolic_bind"(%135, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e596e0) {
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e59610) {
  "llh.symbolic_bind"(%134, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e59510) {
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e59440) {
  "llh.symbolic_bind"(%133, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e592f0) {
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e59220) {
  "llh.symbolic_bind"(%132, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e59120) {
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e59010) {
  "llh.symbolic_bind"(%131, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e58f30) {
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e58e60) {
  "llh.symbolic_bind"(%130, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e58d10) {
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e58c40) {
  "llh.symbolic_bind"(%129, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e58b40) {
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e58a50) {
  "llh.symbolic_bind"(%128, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 4 + 1, (s1 - 1) floordiv 4 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.max_pool'(0x558277e4e180) {
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e58010) {
  "llh.symbolic_bind"(%127, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e3cf30) {
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e57ef0) {
  "llh.symbolic_bind"(%126, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277de38e0) {
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e574f0) {
  "llh.symbolic_bind"(%125, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 64, (s1 - 1) floordiv 2 + 1, (s1 - 1) floordiv 2 + 1)>}> : (tensor<?x64x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e573f0) {
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x558277e56900) {
  "llh.symbolic_bind"(%arg0, %123, %124) <{expressions = affine_map<()[s0, s1] -> (s0, 3, s1, s1)>}> : (tensor<?x3x?x?xf32>, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  %125 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  %126 = "llh.batch_norm"(%125, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %127 = "llh.relu"(%126) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %128 = "llh.max_pool"(%127) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %129 = "llh.conv"(%128, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %130 = "llh.batch_norm"(%129, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.relu"(%130) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.conv"(%131, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.batch_norm"(%132, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.add"(%133, %128) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.relu"(%134) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.conv"(%135, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.batch_norm"(%136, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.relu"(%137) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.conv"(%138, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.batch_norm"(%139, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.add"(%140, %135) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.relu"(%141) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %143 = "llh.conv"(%142, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.batch_norm"(%143, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.relu"(%144) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%145, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.conv"(%142, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.batch_norm"(%148, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.add"(%147, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.relu"(%150) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.conv"(%151, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.batch_norm"(%152, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.relu"(%153) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.conv"(%154, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.batch_norm"(%155, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.add"(%156, %151) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.relu"(%157) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %159 = "llh.conv"(%158, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.batch_norm"(%159, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.relu"(%160) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%161, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.conv"(%158, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.batch_norm"(%164, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.add"(%163, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.relu"(%166) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.conv"(%167, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.batch_norm"(%168, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.relu"(%169) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.conv"(%170, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.batch_norm"(%171, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.add"(%172, %167) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.relu"(%173) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %175 = "llh.conv"(%174, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.batch_norm"(%175, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.relu"(%176) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%177, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.conv"(%174, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.batch_norm"(%180, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.add"(%179, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.relu"(%182) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.conv"(%183, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.batch_norm"(%184, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.relu"(%185) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.conv"(%186, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.batch_norm"(%187, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.add"(%188, %183) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.relu"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %191 = "llh.adaptive_average_pool"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %192 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %193 = "llh.dim"(%191, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %195 = "llh.dim"(%191, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %197 = "llh.dim"(%191, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %199 = "llh.dim"(%191, %198) : (tensor<?x512x1x1xf32>, i64) -> i64
  %200 = "llh.div"(%195, %197) : (i64, i64) -> i64
  %201 = "llh.div"(%200, %199) : (i64, i64) -> i64
  %202 = "llh.reshape"(%191, %193, %201) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %203 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %204 = "llh.matmul"(%202, %203) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %205 = "llh.add"(%204, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %205 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.torch_symbolic_int'(0x558277e56850) {
  %124 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64


  * Pattern {anonymous}::replaceTorchSymbolicIntOp : 'llh.torch_symbolic_int -> ()' {
Trying to match "{anonymous}::replaceTorchSymbolicIntOp"
"{anonymous}::replaceTorchSymbolicIntOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %124 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  %125 = "llh.batch_norm"(%124, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %126 = "llh.relu"(%125) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %127 = "llh.max_pool"(%126) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %128 = "llh.conv"(%127, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %129 = "llh.batch_norm"(%128, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %130 = "llh.relu"(%129) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.conv"(%130, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.batch_norm"(%131, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.add"(%132, %127) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.relu"(%133) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.conv"(%134, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.batch_norm"(%135, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.relu"(%136) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.conv"(%137, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.batch_norm"(%138, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.add"(%139, %134) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.relu"(%140) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %142 = "llh.conv"(%141, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %143 = "llh.batch_norm"(%142, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.relu"(%143) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.conv"(%144, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.batch_norm"(%145, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.conv"(%141, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.batch_norm"(%147, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.add"(%146, %148) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.relu"(%149) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.conv"(%150, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.batch_norm"(%151, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.relu"(%152) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.conv"(%153, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.batch_norm"(%154, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.add"(%155, %150) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.relu"(%156) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %158 = "llh.conv"(%157, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %159 = "llh.batch_norm"(%158, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.relu"(%159) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.conv"(%160, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.batch_norm"(%161, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.conv"(%157, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.batch_norm"(%163, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.add"(%162, %164) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.relu"(%165) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.conv"(%166, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.batch_norm"(%167, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.relu"(%168) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.conv"(%169, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.batch_norm"(%170, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.add"(%171, %166) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.relu"(%172) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %174 = "llh.conv"(%173, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %175 = "llh.batch_norm"(%174, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.relu"(%175) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.conv"(%176, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.batch_norm"(%177, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.conv"(%173, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.batch_norm"(%179, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.add"(%178, %180) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.relu"(%181) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.conv"(%182, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.batch_norm"(%183, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.relu"(%184) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.conv"(%185, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.batch_norm"(%186, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.add"(%187, %182) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.relu"(%188) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %190 = "llh.adaptive_average_pool"(%189) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %191 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %192 = "llh.dim"(%190, %191) : (tensor<?x512x1x1xf32>, i64) -> i64
  %193 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %194 = "llh.dim"(%190, %193) : (tensor<?x512x1x1xf32>, i64) -> i64
  %195 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %196 = "llh.dim"(%190, %195) : (tensor<?x512x1x1xf32>, i64) -> i64
  %197 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %198 = "llh.dim"(%190, %197) : (tensor<?x512x1x1xf32>, i64) -> i64
  %199 = "llh.div"(%194, %196) : (i64, i64) -> i64
  %200 = "llh.div"(%199, %198) : (i64, i64) -> i64
  %201 = "llh.reshape"(%190, %192, %200) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %202 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %203 = "llh.matmul"(%201, %202) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %204 = "llh.add"(%203, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %204 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.torch_symbolic_int'(0x558277e56790) {
  %123 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64


  * Pattern {anonymous}::replaceTorchSymbolicIntOp : 'llh.torch_symbolic_int -> ()' {
Trying to match "{anonymous}::replaceTorchSymbolicIntOp"
"{anonymous}::replaceTorchSymbolicIntOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x3x?x?xf32>) -> tensor<?x1000xf32> attributes {entrance} {
  %0 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>
  %123 = "llh.conv"(%arg0, %1) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>
  %124 = "llh.batch_norm"(%123, %2, %3, %63, %64) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %125 = "llh.relu"(%124) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %126 = "llh.max_pool"(%125) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %127 = "llh.conv"(%126, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %128 = "llh.batch_norm"(%127, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %129 = "llh.relu"(%128) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %130 = "llh.conv"(%129, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %131 = "llh.batch_norm"(%130, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %132 = "llh.add"(%131, %126) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %133 = "llh.relu"(%132) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %134 = "llh.conv"(%133, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %135 = "llh.batch_norm"(%134, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %136 = "llh.relu"(%135) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %137 = "llh.conv"(%136, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>
  %138 = "llh.batch_norm"(%137, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>
  %139 = "llh.add"(%138, %133) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %140 = "llh.relu"(%139) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>
  %141 = "llh.conv"(%140, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>
  %142 = "llh.batch_norm"(%141, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %143 = "llh.relu"(%142) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %144 = "llh.conv"(%143, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %145 = "llh.batch_norm"(%144, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %146 = "llh.conv"(%140, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>
  %147 = "llh.batch_norm"(%146, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %148 = "llh.add"(%145, %147) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %149 = "llh.relu"(%148) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %150 = "llh.conv"(%149, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %151 = "llh.batch_norm"(%150, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %152 = "llh.relu"(%151) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %153 = "llh.conv"(%152, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>
  %154 = "llh.batch_norm"(%153, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>
  %155 = "llh.add"(%154, %149) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %156 = "llh.relu"(%155) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>
  %157 = "llh.conv"(%156, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>
  %158 = "llh.batch_norm"(%157, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %159 = "llh.relu"(%158) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %160 = "llh.conv"(%159, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %161 = "llh.batch_norm"(%160, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %162 = "llh.conv"(%156, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>
  %163 = "llh.batch_norm"(%162, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %164 = "llh.add"(%161, %163) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %165 = "llh.relu"(%164) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %166 = "llh.conv"(%165, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %167 = "llh.batch_norm"(%166, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %168 = "llh.relu"(%167) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %169 = "llh.conv"(%168, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>
  %170 = "llh.batch_norm"(%169, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>
  %171 = "llh.add"(%170, %165) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %172 = "llh.relu"(%171) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>
  %173 = "llh.conv"(%172, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>
  %174 = "llh.batch_norm"(%173, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %175 = "llh.relu"(%174) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %176 = "llh.conv"(%175, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %177 = "llh.batch_norm"(%176, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %178 = "llh.conv"(%172, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>
  %179 = "llh.batch_norm"(%178, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %180 = "llh.add"(%177, %179) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %181 = "llh.relu"(%180) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %182 = "llh.conv"(%181, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %183 = "llh.batch_norm"(%182, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %184 = "llh.relu"(%183) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %185 = "llh.conv"(%184, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>
  %186 = "llh.batch_norm"(%185, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>
  %187 = "llh.add"(%186, %181) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %188 = "llh.relu"(%187) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>
  %189 = "llh.adaptive_average_pool"(%188) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>
  %190 = "llh.constant"() <{value = 0 : i64}> : () -> i64
  %191 = "llh.dim"(%189, %190) : (tensor<?x512x1x1xf32>, i64) -> i64
  %192 = "llh.constant"() <{value = 1 : i64}> : () -> i64
  %193 = "llh.dim"(%189, %192) : (tensor<?x512x1x1xf32>, i64) -> i64
  %194 = "llh.constant"() <{value = 2 : i64}> : () -> i64
  %195 = "llh.dim"(%189, %194) : (tensor<?x512x1x1xf32>, i64) -> i64
  %196 = "llh.constant"() <{value = 3 : i64}> : () -> i64
  %197 = "llh.dim"(%189, %196) : (tensor<?x512x1x1xf32>, i64) -> i64
  %198 = "llh.div"(%193, %195) : (i64, i64) -> i64
  %199 = "llh.div"(%198, %197) : (i64, i64) -> i64
  %200 = "llh.reshape"(%189, %191, %199) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>
  %201 = "llh.transpose"(%61) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>
  %202 = "llh.matmul"(%200, %201) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>
  %203 = "llh.add"(%202, %62) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>
  return %203 : tensor<?x1000xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e566a0) {
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e565e0) {
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55510) {
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55450) {
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55390) {
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e552d0) {
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55210) {
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55150) {
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55090) {
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e54fd0) {
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e54f50) {
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a690) {
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a5d0) {
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a510) {
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a450) {
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a390) {
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a230) {
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a170) {
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a090) {
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49fd0) {
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49f10) {
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49e50) {
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49d90) {
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49cd0) {
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49c10) {
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49b50) {
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51750) {
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51690) {
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e515d0) {
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51510) {
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50440) {
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50380) {
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e502c0) {
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50200) {
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50140) {
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50080) {
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ffc0) {
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ff00) {
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fe40) {
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fd80) {
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fcc0) {
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fc00) {
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fb40) {
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fa80) {
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e9b0) {
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e8f0) {
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e830) {
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e770) {
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e6b0) {
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e5f0) {
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e530) {
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e470) {
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e3b0) {
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e2f0) {
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e230) {
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e0c0) {
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e000) {
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4df40) {
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4de80) {
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cdb0) {
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ccf0) {
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cc30) {
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cb70) {
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cab0) {
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c9f0) {
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c930) {
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c870) {
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c430) {
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c370) {
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c2b0) {
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c1f0) {
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b920) {
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b860) {
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b7a0) {
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49a90) {
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e499d0) {
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49010) {
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48650) {
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48590) {
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e484d0) {
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48410) {
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48350) {
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48290) {
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e481d0) {
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48110) {
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48050) {
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47f90) {
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47ed0) {
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47e10) {
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46cf0) {
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46c30) {
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46b70) {
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46ab0) {
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46a10) {
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e430f0) {
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46790) {
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e466d0) {
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46610) {
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45f10) {
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45e50) {
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45d90) {
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45850) {
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45790) {
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e446c0) {
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43d00) {
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43c40) {
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43700) {
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43640) {
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e431d0) {
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e39c10) {
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e42500) {
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e42440) {
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41f00) {
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e419c0) {
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41900) {
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41840) {
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e0d660) {
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2b430) {
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2c9a0) {
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2cb70) {
  %3 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e39e70) {
  %2 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x558277e4a6e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e3a040) {
  %1 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//
** Replace : 'llh.constant'(0x558277e4dee0)
** Modified: 'llh.dim'(0x558277e39920)
** Erase   : 'llh.constant'(0x558277e4dee0)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x558277e52e50) {
  "func.return"(%202) : (tensor<?x1000xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e52cb0) {
  %202 = "llh.add"(%201, %65) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.matmul'(0x558277e52bc0) {
  %201 = "llh.matmul"(%199, %200) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.transpose'(0x558277e52af0) {
  %200 = "llh.transpose"(%64) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.reshape'(0x558277e71c50) {
  %199 = "llh.reshape"(%192, %193, %198) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.div'(0x558277e71ba0) {
  %198 = "llh.div"(%197, %196) : (i64, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.div'(0x558277e71af0) {
  %197 = "llh.div"(%194, %195) : (i64, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.dim'(0x558277e71a40) {
  %196 = "llh.dim"(%192, %0) : (tensor<?x512x1x1xf32>, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e4a4b0) {
  %0 = "llh.constant"() <{value = 3 : i64}> : () -> i64


  * Pattern {anonymous}::BraodcastableScalarToTensor : 'llh.constant -> ()' {
Trying to match "{anonymous}::BraodcastableScalarToTensor"
"{anonymous}::BraodcastableScalarToTensor" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.dim'(0x558277e71990) {
  %195 = "llh.dim"(%192, %1) : (tensor<?x512x1x1xf32>, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e57fc0) {
  %1 = "llh.constant"() <{value = 2 : i64}> : () -> i64


  * Pattern {anonymous}::BraodcastableScalarToTensor : 'llh.constant -> ()' {
Trying to match "{anonymous}::BraodcastableScalarToTensor"
"{anonymous}::BraodcastableScalarToTensor" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.dim'(0x558277e39920) {
  %194 = "llh.dim"(%192, %3) : (tensor<?x512x1x1xf32>, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.dim'(0x558277e39870) {
  %193 = "llh.dim"(%192, %2) : (tensor<?x512x1x1xf32>, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e554b0) {
  %2 = "llh.constant"() <{value = 0 : i64}> : () -> i64


  * Pattern {anonymous}::BraodcastableScalarToTensor : 'llh.constant -> ()' {
Trying to match "{anonymous}::BraodcastableScalarToTensor"
"{anonymous}::BraodcastableScalarToTensor" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.adaptive_average_pool'(0x558277e660a0) {
  %192 = "llh.adaptive_average_pool"(%191) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e65ef0) {
  %191 = "llh.relu"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e65d20) {
  %190 = "llh.add"(%189, %184) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e65b00) {
  %189 = "llh.batch_norm"(%188, %62, %63, %123, %124) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e65930) {
  %188 = "llh.conv"(%187, %61) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e64730) {
  %187 = "llh.relu"(%186) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e64510) {
  %186 = "llh.batch_norm"(%185, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e64340) {
  %185 = "llh.conv"(%184, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e64150) {
  %184 = "llh.relu"(%183) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e63f80) {
  %183 = "llh.add"(%180, %182) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63d60) {
  %182 = "llh.batch_norm"(%181, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63b90) {
  %181 = "llh.conv"(%175, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63930) {
  %180 = "llh.batch_norm"(%179, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63760) {
  %179 = "llh.conv"(%178, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e63570) {
  %178 = "llh.relu"(%177) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63350) {
  %177 = "llh.batch_norm"(%176, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63180) {
  %176 = "llh.conv"(%175, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e62f90) {
  %175 = "llh.relu"(%174) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e62dc0) {
  %174 = "llh.add"(%173, %168) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e62ba0) {
  %173 = "llh.batch_norm"(%172, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e629d0) {
  %172 = "llh.conv"(%171, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e384d0) {
  %171 = "llh.relu"(%170) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e382b0) {
  %170 = "llh.batch_norm"(%169, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e61ff0) {
  %169 = "llh.conv"(%168, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e61e00) {
  %168 = "llh.relu"(%167) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e61c30) {
  %167 = "llh.add"(%164, %166) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e61a10) {
  %166 = "llh.batch_norm"(%165, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e61840) {
  %165 = "llh.conv"(%159, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e605c0) {
  %164 = "llh.batch_norm"(%163, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e603f0) {
  %163 = "llh.conv"(%162, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e60200) {
  %162 = "llh.relu"(%161) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5ffe0) {
  %161 = "llh.batch_norm"(%160, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5fe10) {
  %160 = "llh.conv"(%159, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5f410) {
  %159 = "llh.relu"(%158) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5f240) {
  %158 = "llh.add"(%157, %152) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5e810) {
  %157 = "llh.batch_norm"(%156, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5e640) {
  %156 = "llh.conv"(%155, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5e450) {
  %155 = "llh.relu"(%154) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5e230) {
  %154 = "llh.batch_norm"(%153, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5e060) {
  %153 = "llh.conv"(%152, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5d660) {
  %152 = "llh.relu"(%151) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5d490) {
  %151 = "llh.add"(%148, %150) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5d270) {
  %150 = "llh.batch_norm"(%149, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5c890) {
  %149 = "llh.conv"(%143, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5c630) {
  %148 = "llh.batch_norm"(%147, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5b450) {
  %147 = "llh.conv"(%146, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5b260) {
  %146 = "llh.relu"(%145) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5b040) {
  %145 = "llh.batch_norm"(%144, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5ae70) {
  %144 = "llh.conv"(%143, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5ac80) {
  %143 = "llh.relu"(%142) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5a2a0) {
  %142 = "llh.add"(%141, %136) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5a080) {
  %141 = "llh.batch_norm"(%140, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e59eb0) {
  %140 = "llh.conv"(%139, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e59cc0) {
  %139 = "llh.relu"(%138) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e59aa0) {
  %138 = "llh.batch_norm"(%137, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e598d0) {
  %137 = "llh.conv"(%136, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e596e0) {
  %136 = "llh.relu"(%135) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e59510) {
  %135 = "llh.add"(%134, %129) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e592f0) {
  %134 = "llh.batch_norm"(%133, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e59120) {
  %133 = "llh.conv"(%132, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e58f30) {
  %132 = "llh.relu"(%131) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e58d10) {
  %131 = "llh.batch_norm"(%130, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e58b40) {
  %130 = "llh.conv"(%129, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.max_pool'(0x558277e4e180) {
  %129 = "llh.max_pool"(%128) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e3cf30) {
  %128 = "llh.relu"(%127) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277de38e0) {
  %127 = "llh.batch_norm"(%126, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e573f0) {
  %126 = "llh.conv"(%arg0, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e566a0) {
  %125 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e565e0) {
  %124 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55510) {
  %123 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55450) {
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55390) {
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e552d0) {
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55210) {
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55150) {
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55090) {
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e54fd0) {
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e54f50) {
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a690) {
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a5d0) {
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a510) {
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a450) {
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a390) {
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a230) {
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a170) {
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a090) {
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49fd0) {
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49f10) {
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49e50) {
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49d90) {
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49cd0) {
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49c10) {
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49b50) {
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51750) {
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51690) {
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e515d0) {
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51510) {
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50440) {
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50380) {
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e502c0) {
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50200) {
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50140) {
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50080) {
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ffc0) {
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ff00) {
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fe40) {
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fd80) {
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fcc0) {
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fc00) {
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fb40) {
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fa80) {
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e9b0) {
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e8f0) {
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e830) {
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e770) {
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e6b0) {
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e5f0) {
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e530) {
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e470) {
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e3b0) {
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e2f0) {
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e230) {
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e0c0) {
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e000) {
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4df40) {
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4de80) {
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cdb0) {
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ccf0) {
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cc30) {
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cb70) {
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cab0) {
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c9f0) {
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c930) {
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c870) {
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c430) {
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c370) {
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c2b0) {
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c1f0) {
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b920) {
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b860) {
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b7a0) {
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49a90) {
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e499d0) {
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49010) {
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48650) {
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48590) {
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e484d0) {
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48410) {
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48350) {
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48290) {
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e481d0) {
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48110) {
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48050) {
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47f90) {
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47ed0) {
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47e10) {
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46cf0) {
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46c30) {
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46b70) {
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46ab0) {
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46a10) {
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e430f0) {
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46790) {
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e466d0) {
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46610) {
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45f10) {
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45e50) {
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45d90) {
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45850) {
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45790) {
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e446c0) {
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43d00) {
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43c40) {
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43700) {
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43640) {
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e431d0) {
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e39c10) {
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e42500) {
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e42440) {
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41f00) {
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e419c0) {
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41900) {
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41840) {
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e0d660) {
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2b430) {
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2c9a0) {
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2cb70) {
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e39e70) {
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e3a040) {
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x558277e4a6e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e66190) {
  %3 = "llh.constant"() <{value = 1 : i64}> : () -> i64


  * Pattern {anonymous}::BraodcastableScalarToTensor : 'llh.constant -> ()' {
Trying to match "{anonymous}::BraodcastableScalarToTensor"
"{anonymous}::BraodcastableScalarToTensor" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//
[2024-09-28 23:47:07.346] [info] ----- run out pass: Operationlegalization -----
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::PreservedAnalyses::AllAnalysesType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallGraph)

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e4a4b0) {
  %0 = "llh.constant"() <{value = 3 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e57fc0) {
  %1 = "llh.constant"() <{value = 2 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e554b0) {
  %2 = "llh.constant"() <{value = 0 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.constant'(0x558277e66190) {
  %3 = "llh.constant"() <{value = 1 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e3a040) {
  %4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e39e70) {
  %5 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2cb70) {
  %6 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2c9a0) {
  %7 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e2b430) {
  %8 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e0d660) {
  %9 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41840) {
  %10 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41900) {
  %11 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e419c0) {
  %12 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e41f00) {
  %13 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv1.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e42440) {
  %14 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e42500) {
  %15 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e39c10) {
  %16 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___conv2.weight.npy"}> : () -> tensor<64x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e431d0) {
  %17 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.weight.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43640) {
  %18 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.bias.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43700) {
  %19 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv1.weight.npy"}> : () -> tensor<128x64x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43c40) {
  %20 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e43d00) {
  %21 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e446c0) {
  %22 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45790) {
  %23 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45850) {
  %24 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45d90) {
  %25 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_0.weight.npy"}> : () -> tensor<128x64x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45e50) {
  %26 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e45f10) {
  %27 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46610) {
  %28 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv1.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e466d0) {
  %29 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46790) {
  %30 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e430f0) {
  %31 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___conv2.weight.npy"}> : () -> tensor<128x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46a10) {
  %32 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.weight.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46ab0) {
  %33 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.bias.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46b70) {
  %34 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv1.weight.npy"}> : () -> tensor<256x128x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46c30) {
  %35 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e46cf0) {
  %36 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47e10) {
  %37 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47ed0) {
  %38 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e47f90) {
  %39 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48050) {
  %40 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_0.weight.npy"}> : () -> tensor<256x128x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48110) {
  %41 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e481d0) {
  %42 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48290) {
  %43 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv1.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48350) {
  %44 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48410) {
  %45 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e484d0) {
  %46 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___conv2.weight.npy"}> : () -> tensor<256x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48590) {
  %47 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.weight.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e48650) {
  %48 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.bias.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49010) {
  %49 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv1.weight.npy"}> : () -> tensor<512x256x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e499d0) {
  %50 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49a90) {
  %51 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b7a0) {
  %52 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b860) {
  %53 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4b920) {
  %54 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c1f0) {
  %55 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_0.weight.npy"}> : () -> tensor<512x256x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c2b0) {
  %56 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c370) {
  %57 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c430) {
  %58 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv1.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c870) {
  %59 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c930) {
  %60 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4c9f0) {
  %61 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___conv2.weight.npy"}> : () -> tensor<512x512x3x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cab0) {
  %62 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.weight.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cb70) {
  %63 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.bias.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cc30) {
  %64 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.weight.npy"}> : () -> tensor<1000x512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ccf0) {
  %65 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___fc.bias.npy"}> : () -> tensor<1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4cdb0) {
  %66 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4de80) {
  %67 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4df40) {
  %68 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e000) {
  %69 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e0c0) {
  %70 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e230) {
  %71 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e2f0) {
  %72 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e3b0) {
  %73 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e470) {
  %74 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e530) {
  %75 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e5f0) {
  %76 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e6b0) {
  %77 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e770) {
  %78 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_mean.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e830) {
  %79 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.running_var.npy"}> : () -> tensor<64xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e8f0) {
  %80 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer1___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4e9b0) {
  %81 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fa80) {
  %82 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fb40) {
  %83 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fc00) {
  %84 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fcc0) {
  %85 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fd80) {
  %86 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4fe40) {
  %87 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ff00) {
  %88 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4ffc0) {
  %89 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50080) {
  %90 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50140) {
  %91 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50200) {
  %92 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e502c0) {
  %93 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_mean.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50380) {
  %94 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.running_var.npy"}> : () -> tensor<128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e50440) {
  %95 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer2___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51510) {
  %96 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e515d0) {
  %97 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51690) {
  %98 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e51750) {
  %99 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49b50) {
  %100 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49c10) {
  %101 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49cd0) {
  %102 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49d90) {
  %103 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49e50) {
  %104 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49f10) {
  %105 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e49fd0) {
  %106 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a090) {
  %107 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a170) {
  %108 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_mean.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a230) {
  %109 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.running_var.npy"}> : () -> tensor<256xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a390) {
  %110 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer3___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a450) {
  %111 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a510) {
  %112 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a5d0) {
  %113 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e4a690) {
  %114 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e54f50) {
  %115 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e54fd0) {
  %116 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55090) {
  %117 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55150) {
  %118 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55210) {
  %119 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___0___downsample_1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e552d0) {
  %120 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55390) {
  %121 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55450) {
  %122 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn1.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e55510) {
  %123 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_mean.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e565e0) {
  %124 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.running_var.npy"}> : () -> tensor<512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.weight'(0x558277e566a0) {
  %125 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/getattr_L__self___layer4___1___bn2.num_batches_tracked.npy"}> : () -> tensor<i64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e573f0) {
  %126 = "llh.conv"(%arg0, %4) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 7, 7>, pad = array<i64: 3, 3, 3, 3>, stride = array<i64: 2, 2>}> : (tensor<?x3x?x?xf32>, tensor<64x3x7x7xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277de38e0) {
  %127 = "llh.batch_norm"(%126, %5, %6, %66, %67) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e3cf30) {
  %128 = "llh.relu"(%127) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.max_pool'(0x558277e4e180) {
  %129 = "llh.max_pool"(%128) <{ceil_mode = false, dilation = array<i64: 1, 1>, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e58b40) {
  %130 = "llh.conv"(%129, %7) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e58d10) {
  %131 = "llh.batch_norm"(%130, %8, %9, %69, %70) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e58f30) {
  %132 = "llh.relu"(%131) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e59120) {
  %133 = "llh.conv"(%132, %10) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e592f0) {
  %134 = "llh.batch_norm"(%133, %11, %12, %72, %73) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e59510) {
  %135 = "llh.add"(%134, %129) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e596e0) {
  %136 = "llh.relu"(%135) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e598d0) {
  %137 = "llh.conv"(%136, %13) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e59aa0) {
  %138 = "llh.batch_norm"(%137, %14, %15, %75, %76) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e59cc0) {
  %139 = "llh.relu"(%138) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e59eb0) {
  %140 = "llh.conv"(%139, %16) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x64x?x?xf32>, tensor<64x64x3x3xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5a080) {
  %141 = "llh.batch_norm"(%140, %17, %18, %78, %79) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x64x?x?xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5a2a0) {
  %142 = "llh.add"(%141, %136) : (tensor<?x64x?x?xf32>, tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5ac80) {
  %143 = "llh.relu"(%142) : (tensor<?x64x?x?xf32>) -> tensor<?x64x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5ae70) {
  %144 = "llh.conv"(%143, %19) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5b040) {
  %145 = "llh.batch_norm"(%144, %20, %21, %81, %82) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5b260) {
  %146 = "llh.relu"(%145) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5b450) {
  %147 = "llh.conv"(%146, %22) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5c630) {
  %148 = "llh.batch_norm"(%147, %23, %24, %84, %85) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5c890) {
  %149 = "llh.conv"(%143, %25) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x64x?x?xf32>, tensor<128x64x1x1xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5d270) {
  %150 = "llh.batch_norm"(%149, %26, %27, %87, %88) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5d490) {
  %151 = "llh.add"(%148, %150) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5d660) {
  %152 = "llh.relu"(%151) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5e060) {
  %153 = "llh.conv"(%152, %28) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5e230) {
  %154 = "llh.batch_norm"(%153, %29, %30, %90, %91) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5e450) {
  %155 = "llh.relu"(%154) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5e640) {
  %156 = "llh.conv"(%155, %31) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x128x?x?xf32>, tensor<128x128x3x3xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5e810) {
  %157 = "llh.batch_norm"(%156, %32, %33, %93, %94) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x128x?x?xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e5f240) {
  %158 = "llh.add"(%157, %152) : (tensor<?x128x?x?xf32>, tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e5f410) {
  %159 = "llh.relu"(%158) : (tensor<?x128x?x?xf32>) -> tensor<?x128x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e5fe10) {
  %160 = "llh.conv"(%159, %34) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e5ffe0) {
  %161 = "llh.batch_norm"(%160, %35, %36, %96, %97) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e60200) {
  %162 = "llh.relu"(%161) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e603f0) {
  %163 = "llh.conv"(%162, %37) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e605c0) {
  %164 = "llh.batch_norm"(%163, %38, %39, %99, %100) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e61840) {
  %165 = "llh.conv"(%159, %40) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x128x?x?xf32>, tensor<256x128x1x1xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e61a10) {
  %166 = "llh.batch_norm"(%165, %41, %42, %102, %103) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e61c30) {
  %167 = "llh.add"(%164, %166) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e61e00) {
  %168 = "llh.relu"(%167) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e61ff0) {
  %169 = "llh.conv"(%168, %43) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e382b0) {
  %170 = "llh.batch_norm"(%169, %44, %45, %105, %106) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e384d0) {
  %171 = "llh.relu"(%170) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e629d0) {
  %172 = "llh.conv"(%171, %46) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x256x?x?xf32>, tensor<256x256x3x3xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e62ba0) {
  %173 = "llh.batch_norm"(%172, %47, %48, %108, %109) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x256x?x?xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e62dc0) {
  %174 = "llh.add"(%173, %168) : (tensor<?x256x?x?xf32>, tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e62f90) {
  %175 = "llh.relu"(%174) : (tensor<?x256x?x?xf32>) -> tensor<?x256x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63180) {
  %176 = "llh.conv"(%175, %49) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63350) {
  %177 = "llh.batch_norm"(%176, %50, %51, %111, %112) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e63570) {
  %178 = "llh.relu"(%177) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63760) {
  %179 = "llh.conv"(%178, %52) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63930) {
  %180 = "llh.batch_norm"(%179, %53, %54, %114, %115) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e63b90) {
  %181 = "llh.conv"(%175, %55) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 1, 1>, pad = array<i64: 0, 0, 0, 0>, stride = array<i64: 2, 2>}> : (tensor<?x256x?x?xf32>, tensor<512x256x1x1xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e63d60) {
  %182 = "llh.batch_norm"(%181, %56, %57, %117, %118) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e63f80) {
  %183 = "llh.add"(%180, %182) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e64150) {
  %184 = "llh.relu"(%183) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e64340) {
  %185 = "llh.conv"(%184, %58) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e64510) {
  %186 = "llh.batch_norm"(%185, %59, %60, %120, %121) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e64730) {
  %187 = "llh.relu"(%186) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.conv'(0x558277e65930) {
  %188 = "llh.conv"(%187, %61) <{dilation = array<i64: 1, 1>, group = 1 : i64, kernel_shape = array<i64: 3, 3>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}> : (tensor<?x512x?x?xf32>, tensor<512x512x3x3xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.batch_norm'(0x558277e65b00) {
  %189 = "llh.batch_norm"(%188, %62, %63, %123, %124) <{epsilon = 1.000000e-05 : f64, momentum = 1.000000e-01 : f64}> : (tensor<?x512x?x?xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e65d20) {
  %190 = "llh.add"(%189, %184) : (tensor<?x512x?x?xf32>, tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.relu'(0x558277e65ef0) {
  %191 = "llh.relu"(%190) : (tensor<?x512x?x?xf32>) -> tensor<?x512x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.adaptive_average_pool'(0x558277e660a0) {
  %192 = "llh.adaptive_average_pool"(%191) : (tensor<?x512x?x?xf32>) -> tensor<?x512x1x1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.dim'(0x558277e39870) {
  %193 = "llh.dim"(%192, %2) : (tensor<?x512x1x1xf32>, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.dim'(0x558277e39920) {
  %194 = "llh.dim"(%192, %3) : (tensor<?x512x1x1xf32>, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.dim'(0x558277e71990) {
  %195 = "llh.dim"(%192, %1) : (tensor<?x512x1x1xf32>, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.dim'(0x558277e71a40) {
  %196 = "llh.dim"(%192, %0) : (tensor<?x512x1x1xf32>, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.div'(0x558277e71af0) {
  %197 = "llh.div"(%194, %195) : (i64, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.div'(0x558277e71ba0) {
  %198 = "llh.div"(%197, %196) : (i64, i64) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.reshape'(0x558277e71c50) {
  %199 = "llh.reshape"(%192, %193, %198) : (tensor<?x512x1x1xf32>, i64, i64) -> tensor<?x512xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.transpose'(0x558277e52af0) {
  %200 = "llh.transpose"(%64) <{perms = array<i64: 1, 0>}> : (tensor<1000x512xf32>) -> tensor<512x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.matmul'(0x558277e52bc0) {
  %201 = "llh.matmul"(%199, %200) : (tensor<?x512xf32>, tensor<512x1000xf32>) -> tensor<?x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x558277e52cb0) {
  %202 = "llh.add"(%201, %65) : (tensor<?x1000xf32>, tensor<1000xf32>) -> tensor<?x1000xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x558277e52e50) {
  "func.return"(%202) : (tensor<?x1000xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
* Inliner: Initial calls in SCC are: {
}
* Inliner: Initial calls in SCC are: {
}
[2024-09-28 23:47:07.387] [info] ----- run in pass: InferSymbolShape -----
ImplicitTypeIDRegistry::lookupOrInsert(mlir::llh::detail::SymbolicIntOpGenericAdaptorBase::Properties)
#llh.encoding<shapes = @s0, @c3, @s1, @s2>
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VerifiableTensorEncoding)
<block argument> of type 'tensor<?x3x?x?xf32, #llh.encoding<shapes = @s0, @c3, @s1, @s2>>' at index: 0
[2024-09-28 23:47:07.388] [info] c3
"llh.symbolic_int"() <{sym_name = "c3"}> : () -> ()
[2024-09-28 23:47:07.388] [info] s0
"llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
[2024-09-28 23:47:07.388] [info] s1
"llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
[2024-09-28 23:47:07.388] [info] s2
"llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
[2024-09-28 23:47:07.388] [warning] /home/lfr/LLCompiler/src/Dialect/LLH/IR/LLHinfersymbolShape.cpp<49>: 
	function [inferSymbolicShape] Unimplemented!
[2024-09-28 23:47:07.388] [warning] /home/lfr/LLCompiler/src/Dialect/LLH/IR/LLHinfersymbolShape.cpp<49>: 
	function [inferSymbolicShape] Unimplemented!
[2024-09-28 23:47:07.388] [warning] /home/lfr/LLCompiler/src/Dialect/LLH/IR/LLHinfersymbolShape.cpp<49>: 
	function [inferSymbolicShape] Unimplemented!
[2024-09-28 23:47:07.388] [warning] /home/lfr/LLCompiler/src/Dialect/LLH/IR/LLHinfersymbolShape.cpp<49>: 
	function [inferSymbolicShape] Unimplemented!
#llh.encoding<shapes = @c64, @c3, @c7, @c7>
%4 = "llh.weight"() <{weight_file = "/home/lfr/LLCompiler/llcompiler/importer/LLcompiler_weight_temp/2024-09-28T17:08:16.961145+08:00/L__self___conv1.weight.npy"}> : () -> tensor<64x3x7x7xf32, #llh.encoding<shapes = @c64, @c3, @c7, @c7>>
*** stack smashing detected ***: terminated
PLEASE submit a bug report to https://github.com/llvm/llvm-project/issues/ and include the crash backtrace.
Stack dump:
0.	Program arguments: /home/lfr/LLCompiler/.setuptools-cmake-build/bin/llc-opt --dump-pass-pipeline -o=/home/lfr/LLCompiler/out.mlir --log-lever=debug --log-root=C:codingLLCompilerlog --mlir-print-ir-tree-dir=/home/lfr/LLCompiler/ir_tree --mlir-print-ir-after-all -basic-pipeline /home/lfr/LLCompiler/test/model_ir/resnet18.mlir --debug
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
0  libLLVMSupport.so.20.0git 0x00007f8793dfc800 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 240
1  libLLVMSupport.so.20.0git 0x00007f8793df9d6a llvm::sys::RunSignalHandlers() + 58
2  libLLVMSupport.so.20.0git 0x00007f8793df9fd5
3  libc.so.6                 0x00007f87937d0520
4  libc.so.6                 0x00007f87938249fc pthread_kill + 300
5  libc.so.6                 0x00007f87937d0476 raise + 22
6  libc.so.6                 0x00007f87937b67f3 abort + 211
7  libc.so.6                 0x00007f8793817676
8  libc.so.6                 0x00007f87938c459a __fortify_fail + 42
9  libc.so.6                 0x00007f87938c4566
10 libMLIRLLHDialect.so      0x00007f879589eb8e
11 libMLIRLLHDialect.so      0x00007f87958683d4 mlir::detail::SymbolicInferShapeOpInterfaceInterfaceTraits::Model<mlir::llh::WeightOp>::inferSymbolicShape(mlir::detail::SymbolicInferShapeOpInterfaceInterfaceTraits::Concept const*, mlir::Operation*) + 36
12 libMLIRLLHTransforms.so   0x00007f87958fc526
13 libMLIRLLHTransforms.so   0x00007f87958fc4cf
14 libMLIRLLHTransforms.so   0x00007f87958fc4cf
15 libMLIRLLHTransforms.so   0x00007f87958fc9c0
16 libMLIRPass.so.20.0git    0x00007f87941713d9 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) + 1193
17 libMLIRPass.so.20.0git    0x00007f8794171891 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) + 321
18 libMLIRPass.so.20.0git    0x00007f87941728e5 mlir::PassManager::run(mlir::Operation*) + 1445
19 libMLIROptLib.so.20.0git  0x00007f879597e2b7
20 libMLIROptLib.so.20.0git  0x00007f879597eccc
21 libMLIROptLib.so.20.0git  0x00007f879597ee2d
22 libMLIRSupport.so.20.0git 0x00007f87932338de mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<llvm::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, llvm::StringRef, llvm::StringRef) + 174
23 libMLIROptLib.so.20.0git  0x00007f87959754bc mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&) + 220
24 libMLIROptLib.so.20.0git  0x00007f879597ef90 mlir::MlirOptMain(int, char**, llvm::StringRef, llvm::StringRef, mlir::DialectRegistry&) + 304
25 libMLIROptLib.so.20.0git  0x00007f879597f4b7 mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&) + 391
26 llc-opt                   0x0000558277d64a7b
27 libc.so.6                 0x00007f87937b7d90
28 libc.so.6                 0x00007f87937b7e40 __libc_start_main + 128
29 llc-opt                   0x0000558277d64b25
