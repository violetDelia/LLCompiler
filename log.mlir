Args: /home/lfr/LLCompiler/build/bin/llc-opt /home/lfr/LLCompiler/test/model_ir/Add.mlir -basic-pipeline --debug --mlir-print-ir-after-all 
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ElementsAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistinctAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ResourceBlobManagerDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineBinaryOpExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineConstantExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineDimExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineMapStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::IntegerSetStorage)
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroOperands<mlir::TypeID::get<mlir::OpTrait::ZeroOperands>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneRegion<mlir::TypeID::get<mlir::OpTrait::OneRegion>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroResults<mlir::TypeID::get<mlir::OpTrait::ZeroResults>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroSuccessors<mlir::TypeID::get<mlir::OpTrait::ZeroSuccessors>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoRegionArguments<mlir::TypeID::get<mlir::OpTrait::NoRegionArguments>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoTerminator<mlir::TypeID::get<mlir::OpTrait::NoTerminator>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlock<mlir::TypeID::get<mlir::OpTrait::SingleBlock>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OpInvariants<mlir::TypeID::get<mlir::OpTrait::OpInvariants>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface::Trait<mlir::TypeID::get<mlir::BytecodeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AffineScope<mlir::TypeID::get<mlir::OpTrait::AffineScope>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIsolatedFromAbove<mlir::TypeID::get<mlir::OpTrait::IsIsolatedFromAbove>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SymbolTable<mlir::TypeID::get<mlir::OpTrait::SymbolTable>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface::Trait<mlir::TypeID::get<mlir::SymbolOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface::Trait<mlir::TypeID::get<mlir::OpAsmOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface::Trait<mlir::TypeID::get<mlir::RegionKindInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasOnlyGraphRegion<mlir::TypeID::get<mlir::OpTrait::HasOnlyGraphRegion>()::Empty>)
Load new dialect in Context func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectInlinerInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConvertToLLVMPatternInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface)
Load new dialect in Context cf
Load new dialect in Context arith
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithIntegerOverflowFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithRoundingModeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SelectLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferDeallocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueBoundsOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AutomaticAllocationScope<mlir::TypeID::get<mlir::OpTrait::AutomaticAllocationScope>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface::Trait<mlir::TypeID::get<mlir::CallableOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface::Trait<mlir::TypeID::get<mlir::FunctionOpInterface::Trait>()::Empty>)
Load new dialect in Context llh
Load new dialect in Context mlir_ex
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolicInferShapeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BraodcastableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroRegions<mlir::TypeID::get<mlir::OpTrait::ZeroRegions>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneResult<mlir::TypeID::get<mlir::OpTrait::OneResult>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::IntegerType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::IntegerType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface::Trait<mlir::TypeID::get<mlir::InferTypeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicOperands<mlir::TypeID::get<mlir::OpTrait::VariadicOperands>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl<mlir::TypeID::get<mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable::Trait<mlir::TypeID::get<mlir::ConditionallySpeculatable::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AlwaysSpeculatableImplTrait<mlir::TypeID::get<mlir::OpTrait::AlwaysSpeculatableImplTrait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface::Trait<mlir::TypeID::get<mlir::MemoryEffectOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::MemRefsNormalizable<mlir::TypeID::get<mlir::OpTrait::MemRefsNormalizable>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface::Trait<mlir::TypeID::get<mlir::RegionBranchTerminatorOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ReturnLike<mlir::TypeID::get<mlir::OpTrait::ReturnLike>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsTerminator<mlir::TypeID::get<mlir::OpTrait::IsTerminator>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutSpecInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<1>::Impl<mlir::TypeID::get<mlir::OpTrait::AtLeastNOperands<1>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::Type>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::Type>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<2>::Impl<mlir::TypeID::get<mlir::OpTrait::NOperands<2>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ResultsBroadcastableShape<mlir::TypeID::get<mlir::OpTrait::ResultsBroadcastableShape>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BraodcastableOpInterface::Trait<mlir::TypeID::get<mlir::BraodcastableOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolicInferShapeOpInterface::Trait<mlir::TypeID::get<mlir::SymbolicInferShapeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::OpToOpPassAdaptor)
Load new dialect in Context affine
Load new dialect in Context ub
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ub::PoisonAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineMapAccessInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaStartOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaWaitOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LoopLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineReadOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineWriteOpInterface)
Load new dialect in Context bufferization
Load new dialect in Context memref
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CopyOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableMemOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAccessorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ViewLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedDimOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OffsetSizeAndStrideOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::AllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RuntimeVerifiableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableTypeInterface)
Load new dialect in Context tensor
Load new dialect in Context complex
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::FindPayloadReplacementOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetInsertionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetExtractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TilingInterface)
Load new dialect in Context linalg
Load new dialect in Context math
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::AggregatedOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::LinalgOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ContractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ConvolutionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::FillOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::mesh::ShardingInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PartialReductionOpInterface)
Load new dialect in Context index
Load new dialect in Context llvm
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::DIRecursiveTypeAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMVoidType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMPPCFP128Type)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMTokenType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMLabelType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMMetadataType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMStructType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::IntegerOverflowFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::AccessGroupOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::AliasAnalysisOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::FastmathFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::BranchWeightOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SafeMemorySlotAccessOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::FPExceptionBehaviorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::RoundingModeOpInterface)
Load new dialect in Context scf
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ParallelCombiningOpInterface)
Load new dialect in Context tosa
Load new dialect in Context quant
ImplicitTypeIDRegistry::lookupOrInsert(mlir::quant::AnyQuantizedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::quant::CalibratedQuantizedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::quant::UniformQuantizedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::quant::UniformQuantizedPerAxisType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferShapedTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::tosa::TosaOp)
Load new dialect in Context vector
ImplicitTypeIDRegistry::lookupOrInsert(mlir::vector::MaskableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::vector::MaskingOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorTransferOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectFoldInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ConstantLike<mlir::TypeID::get<mlir::OpTrait::ConstantLike>()::Empty>)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%3) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x55db9a909e20) {
  "llh.symbolic_bind"(%3, %0, %1, %2) <{expressions = affine_map<()[s0, s1, s2] -> (s0, s1, s2, s2)>}> : (tensor<?x?x?x?xf32>, i64, i64, i64) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasRecursiveMemoryEffects<mlir::TypeID::get<mlir::OpTrait::HasRecursiveMemoryEffects>()::Empty>)
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %3 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x55db9a914be0) {
  "llh.symbolic_bind"(%arg0, %0, %1, %2) <{expressions = affine_map<()[s0, s1, s2] -> (s0, s1, s2, s2)>}> : (tensor<?x?x?x?xf32>, i64, i64, i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.torch_symbolic_int'(0x55db9a912380) {
  %2 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.torch_symbolic_int'(0x55db9a91f230) {
  %1 = "llh.torch_symbolic_int"() <{sym_name = "s1"}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.torch_symbolic_int'(0x55db9a91f400) {
  %0 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::PreservedAnalyses::AllAnalysesType)
// -----// IR Dump After OperationlegalizationPass (operation-legalization) //----- //
#map = ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::StorageUserTrait::IsMutable<mlir::TypeID::get<mlir::detail::StorageUserTrait::IsMutable>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface::Trait<mlir::TypeID::get<mlir::MemRefLayoutAttrInterface::Trait>()::Empty>)
affine_map<()[s0, s1, s2] -> (s0, s1, s2, s2)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %0 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
    %1 = "llh.torch_symbolic_int"() <{sym_name = "s1"}> : () -> i64
    %2 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
    "llh.symbolic_bind"(%arg0, %0, %1, %2) <{expressions = #map}> : (tensor<?x?x?x?xf32>, i64, i64, i64) -> ()
    %3 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    "llh.symbolic_bind"(%3, %0, %1, %2) <{expressions = #map}> : (tensor<?x?x?x?xf32>, i64, i64, i64) -> ()
    return %3 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%3) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x55db9a909e20) {
  "llh.symbolic_bind"(%3, %0, %1, %2) <{expressions = affine_map<()[s0, s1, s2] -> (s0, s1, s2, s2)>}> : (tensor<?x?x?x?xf32>, i64, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %1 = "llh.torch_symbolic_int"() <{sym_name = "s1"}> : () -> i64
  %2 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  "llh.symbolic_bind"(%arg0, %0, %1, %2) <{expressions = affine_map<()[s0, s1, s2] -> (s0, s1, s2, s2)>}> : (tensor<?x?x?x?xf32>, i64, i64, i64) -> ()
  %3 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  return %3 : tensor<?x?x?x?xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %3 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_bind'(0x55db9a914be0) {
  "llh.symbolic_bind"(%arg0, %0, %1, %2) <{expressions = affine_map<()[s0, s1, s2] -> (s0, s1, s2, s2)>}> : (tensor<?x?x?x?xf32>, i64, i64, i64) -> ()


  * Pattern {anonymous}::replaceSymbolicBindOp : 'llh.symbolic_bind -> ()' {
Trying to match "{anonymous}::replaceSymbolicBindOp"
"{anonymous}::replaceSymbolicBindOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %1 = "llh.torch_symbolic_int"() <{sym_name = "s1"}> : () -> i64
  %2 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64
  %3 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  return %3 : tensor<?x?x?x?xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.torch_symbolic_int'(0x55db9a912380) {
  %2 = "llh.torch_symbolic_int"() <{sym_name = "s2"}> : () -> i64


  * Pattern {anonymous}::replaceTorchSymbolicIntOp : 'llh.torch_symbolic_int -> ()' {
Trying to match "{anonymous}::replaceTorchSymbolicIntOp"
"{anonymous}::replaceTorchSymbolicIntOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %1 = "llh.torch_symbolic_int"() <{sym_name = "s1"}> : () -> i64
  %2 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  return %2 : tensor<?x?x?x?xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.torch_symbolic_int'(0x55db9a91f230) {
  %1 = "llh.torch_symbolic_int"() <{sym_name = "s1"}> : () -> i64


  * Pattern {anonymous}::replaceTorchSymbolicIntOp : 'llh.torch_symbolic_int -> ()' {
Trying to match "{anonymous}::replaceTorchSymbolicIntOp"
"{anonymous}::replaceTorchSymbolicIntOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64
  %1 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  return %1 : tensor<?x?x?x?xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.torch_symbolic_int'(0x55db9a91f400) {
  %0 = "llh.torch_symbolic_int"() <{sym_name = "s0"}> : () -> i64


  * Pattern {anonymous}::replaceTorchSymbolicIntOp : 'llh.torch_symbolic_int -> ()' {
Trying to match "{anonymous}::replaceTorchSymbolicIntOp"
"{anonymous}::replaceTorchSymbolicIntOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  return %0 : tensor<?x?x?x?xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After RemoveRedundantOpsPass (remove-redundant-ops) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    return %0 : tensor<?x?x?x?xf32>
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallGraph)

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  return %0 : tensor<?x?x?x?xf32>
}

* Inliner: Initial calls in SCC are: {
}
* Inliner: Initial calls in SCC are: {
}
// -----// IR Dump After Inliner (inline) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    return %0 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>


  * Pattern {anonymous}::SimplyBinaryOp<mlir::llh::AddOp> : 'llh.add -> ()' {
Trying to match "{anonymous}::SimplyBinaryOp<mlir::llh::AddOp>"
"{anonymous}::SimplyBinaryOp<mlir::llh::AddOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ReshapeBeforeBraodcastPass (reshape-before-braodcast) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    return %0 : tensor<?x?x?x?xf32>
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::llh::detail::SymbolicIntOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VerifiableTensorEncoding)
// -----// IR Dump After InferSymbolShapePass (infer-symbol-shape) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>> attributes {entrance} {
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
    return %0 : tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
  }
}



//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>> attributes {entrance} {
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
    return %0 : tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After LoadWeightPass (load-weight) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>> attributes {entrance} {
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
    return %0 : tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
  }
}



//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>> attributes {entrance} {
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
    return %0 : tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After TransformLayoutToNHWC (transform-layout-to-nhwc) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>> attributes {entrance} {
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>, tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>) -> tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
    return %0 : tensor<?x?x?x?xf32, #llh.encoding<shapes = @s0, @s1, @s2, @s3>>
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::llh::detail::EncodingBindOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneOperand<mlir::TypeID::get<mlir::OpTrait::OneOperand>()::Empty>)
// -----// IR Dump After UnloadAndBindEncoding (unload-and-bind-encoding) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %0 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x55db9a9287a0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ConvertLLHToArithPass (convert-llh-to-arith) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %0 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x55db9a9287a0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ConvertLLHToTensorPass (convert-llh-to-tensor) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %0 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x55db9a9287a0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.add'(0x55db9a927bc0) {
  %0 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'llh.add -> ()' {
Trying to match "mlir::SimplyFullLowing<mlir::llh::AddOp, mlir::tosa::AddOp>"
    ** Insert  : 'tosa.add'(0x55db9aa5c0b0)
    ** Replace : 'llh.add'(0x55db9a927bc0)
"mlir::SimplyFullLowing<mlir::llh::AddOp, mlir::tosa::AddOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'tosa.add'(0x55db9aa5c0b0) {
      %0 = "tosa.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  %1 = "llh.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%1) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %1 : tensor<?x?x?x?xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%1) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%1) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::TensorType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::TensorType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsCommutative<mlir::TypeID::get<mlir::OpTrait::IsCommutative>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultElementType<mlir::TypeID::get<mlir::OpTrait::SameOperandsAndResultElementType>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferShapedTypeOpInterface::Trait<mlir::TypeID::get<mlir::InferShapedTypeOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::tosa::TosaElementwiseOperator<mlir::TypeID::get<mlir::OpTrait::tosa::TosaElementwiseOperator>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::tosa::TosaOp::Trait<mlir::TypeID::get<mlir::tosa::TosaOp::Trait>()::Empty>)
// -----// IR Dump After ConvertLLHToTosaPass (convert-llh-to-tosa) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %0 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tosa.add'(0x55db9aa5c0b0) {
  %0 = "tosa.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %0 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tosa.add'(0x55db9aa5c0b0) {
  %0 = "tosa.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After FoldIndexCastPass (fold-index-cast) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %0 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tosa.add'(0x55db9aa5c0b0) {
  %0 = "tosa.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After TosaLayerwiseConstantFoldPass (tosa-layerwise-constant-fold) //----- //
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %0 : tensor<?x?x?x?xf32>
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tosa.add'(0x55db9aa5c0b0) {
  %0 = "tosa.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After TosaOptionalDecompositions (tosa-optional-decompositions) //----- //
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %0 : tensor<?x?x?x?xf32>
}

// -----// IR Dump After TosaReduceTransposes (tosa-reduce-transposes) //----- //
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %0 : tensor<?x?x?x?xf32>
}


//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tosa.add'(0x55db9aa5c0b0) {
  %0 = "tosa.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %0 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tosa.add'(0x55db9aa5c0b0) {
  %0 = "tosa.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After TosaToLinalgNamed (tosa-to-linalg-named) //----- //
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %0 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %0 : tensor<?x?x?x?xf32>
}


//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tosa.add'(0x55db9aa5c0b0) {
  %0 = "tosa.add"(%arg0, %arg0) : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'tosa.add -> ()' {
Trying to match "{anonymous}::PointwiseConverter<mlir::tosa::AddOp>"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::ConstantOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface::Trait<mlir::TypeID::get<mlir::InferIntRangeInterface::Trait>()::Empty>)
    ** Insert  : 'arith.constant'(0x55db9aa8c790)
    ** Insert  : 'tensor.dim'(0x55db9a927bc0)
    ** Insert  : 'tensor.dim'(0x55db9a91ec00)
    ** Insert  : 'arith.maxui'(0x55db9a91ecb0)
    ** Insert  : 'arith.constant'(0x55db9aa8da10)
    ** Insert  : 'tensor.dim'(0x55db9aa8da80)
    ** Insert  : 'tensor.dim'(0x55db9aa8db60)
    ** Insert  : 'arith.maxui'(0x55db9aa8dcb0)
    ** Insert  : 'arith.constant'(0x55db9aa8dd90)
    ** Insert  : 'tensor.dim'(0x55db9aa8de30)
    ** Insert  : 'tensor.dim'(0x55db9aa8df10)
    ** Insert  : 'arith.maxui'(0x55db9aa8dff0)
    ** Insert  : 'arith.constant'(0x55db9aa8e550)
    ** Insert  : 'tensor.dim'(0x55db9aa8e5f0)
    ** Insert  : 'tensor.dim'(0x55db9aa8e6d0)
    ** Insert  : 'arith.maxui'(0x55db9aa8e7b0)
    ** Insert  : 'tensor.dim'(0x55db9aa93550)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::CmpIOpGenericAdaptorBase::Properties)
    ** Insert  : 'arith.cmpi'(0x55db9aa93ab0)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.constant'(0x55db9aa93ff0)
    ** Insert  : 'tensor.dim'(0x55db9aa94090)
    ** Insert  : 'arith.constant'(0x55db9aa94170)
    ** Insert  : 'tensor.dim'(0x55db9aa94210)
    ** Insert  : 'arith.constant'(0x55db9aa942f0)
    ** Insert  : 'tensor.dim'(0x55db9aa94390)
    ** Insert  : 'tensor.empty'(0x55db9aa69a50)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::detail::GenericOpGenericAdaptorBase::Properties)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'linalg.yield'(0x55db9aa8bf60)
    ** Insert  : 'linalg.generic'(0x55db9aa73210)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface::Trait<mlir::TypeID::get<mlir::CastOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicResults<mlir::TypeID::get<mlir::OpTrait::VariadicResults>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlockImplicitTerminator<mlir::linalg::YieldOp>::Impl<mlir::TypeID::get<mlir::OpTrait::SingleBlockImplicitTerminator<mlir::linalg::YieldOp>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AttrSizedOperandSegments<mlir::TypeID::get<mlir::OpTrait::AttrSizedOperandSegments>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface::Trait<mlir::TypeID::get<mlir::DestinationStyleOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::LinalgOp::Trait<mlir::TypeID::get<mlir::linalg::LinalgOp::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface::Trait<mlir::TypeID::get<mlir::ReifyRankedShapedTypeOpInterface::Trait>()::Empty>)
    ** Insert  : 'scf.yield'(0x55db9aa73180)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'scf.yield'(0x55db9aa5b820)
    ** Insert  : 'scf.if'(0x55db9aa734e0)
    ** Insert  : 'tensor.dim'(0x55db9aa96ad0)
    ** Insert  : 'arith.cmpi'(0x55db9aa96bb0)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.constant'(0x55db9aa97120)
    ** Insert  : 'tensor.dim'(0x55db9aa971c0)
    ** Insert  : 'arith.constant'(0x55db9aa972a0)
    ** Insert  : 'tensor.dim'(0x55db9aa97340)
    ** Insert  : 'arith.constant'(0x55db9aa97420)
    ** Insert  : 'tensor.dim'(0x55db9aa974c0)
    ** Insert  : 'tensor.empty'(0x55db9a9123e0)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'linalg.yield'(0x55db9aa97b50)
    ** Insert  : 'linalg.generic'(0x55db9a912210)
    ** Insert  : 'scf.yield'(0x55db9aa97c30)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'scf.yield'(0x55db9aa97d50)
    ** Insert  : 'scf.if'(0x55db9aa97e10)
    ** Insert  : 'tensor.dim'(0x55db9aa97f70)
    ** Insert  : 'arith.cmpi'(0x55db9aa98050)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.constant'(0x55db9aa985c0)
    ** Insert  : 'tensor.dim'(0x55db9aa98660)
    ** Insert  : 'arith.constant'(0x55db9aa98740)
    ** Insert  : 'tensor.dim'(0x55db9aa987e0)
    ** Insert  : 'arith.constant'(0x55db9aa988c0)
    ** Insert  : 'tensor.dim'(0x55db9aa98960)
    ** Insert  : 'tensor.empty'(0x55db9a910790)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'linalg.yield'(0x55db9aa9a000)
    ** Insert  : 'linalg.generic'(0x55db9a8fc4d0)
    ** Insert  : 'scf.yield'(0x55db9aa9a0e0)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'scf.yield'(0x55db9aa9a200)
    ** Insert  : 'scf.if'(0x55db9aa9a2c0)
    ** Insert  : 'tensor.dim'(0x55db9aa9a420)
    ** Insert  : 'arith.cmpi'(0x55db9aa9a500)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.constant'(0x55db9aa9aa70)
    ** Insert  : 'tensor.dim'(0x55db9aa9ab10)
    ** Insert  : 'arith.constant'(0x55db9aa9abf0)
    ** Insert  : 'tensor.dim'(0x55db9aa9ac90)
    ** Insert  : 'arith.constant'(0x55db9aa9ad70)
    ** Insert  : 'tensor.dim'(0x55db9aa9ae10)
    ** Insert  : 'tensor.empty'(0x55db9a8e7330)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'linalg.yield'(0x55db9aa9b4a0)
    ** Insert  : 'linalg.generic'(0x55db9aa9b560)
    ** Insert  : 'scf.yield'(0x55db9aa9b670)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'scf.yield'(0x55db9aa9b790)
    ** Insert  : 'scf.if'(0x55db9aa9b850)
    ** Insert  : 'tensor.dim'(0x55db9aa9b940)
    ** Insert  : 'arith.cmpi'(0x55db9aa9ba20)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.constant'(0x55db9aa9bb80)
    ** Insert  : 'tensor.dim'(0x55db9aa9bc20)
    ** Insert  : 'arith.constant'(0x55db9aa9bd00)
    ** Insert  : 'tensor.dim'(0x55db9aa9bda0)
    ** Insert  : 'arith.constant'(0x55db9aa9be80)
    ** Insert  : 'tensor.dim'(0x55db9aa9bf20)
    ** Insert  : 'tensor.empty'(0x55db9aa9c000)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'linalg.yield'(0x55db9aa9c220)
    ** Insert  : 'linalg.generic'(0x55db9aa9c2e0)
    ** Insert  : 'scf.yield'(0x55db9aa9c3f0)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'scf.yield'(0x55db9aa9c510)
    ** Insert  : 'scf.if'(0x55db9aa9c5d0)
    ** Insert  : 'tensor.dim'(0x55db9aa9c6c0)
    ** Insert  : 'arith.cmpi'(0x55db9aa9c7a0)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.constant'(0x55db9aa9c900)
    ** Insert  : 'tensor.dim'(0x55db9aa9c9a0)
    ** Insert  : 'arith.constant'(0x55db9aa9ca80)
    ** Insert  : 'tensor.dim'(0x55db9aa9cb20)
    ** Insert  : 'arith.constant'(0x55db9aa9cc00)
    ** Insert  : 'tensor.dim'(0x55db9aa9cca0)
    ** Insert  : 'tensor.empty'(0x55db9aa9cd80)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'linalg.yield'(0x55db9aa9d6a0)
    ** Insert  : 'linalg.generic'(0x55db9aa9d760)
    ** Insert  : 'scf.yield'(0x55db9aa9d870)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'scf.yield'(0x55db9aa9d990)
    ** Insert  : 'scf.if'(0x55db9aa9da50)
    ** Insert  : 'tensor.dim'(0x55db9aa9db40)
    ** Insert  : 'arith.cmpi'(0x55db9aa9dc20)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.constant'(0x55db9aa9dd80)
    ** Insert  : 'tensor.dim'(0x55db9aa9de20)
    ** Insert  : 'arith.constant'(0x55db9aa9df00)
    ** Insert  : 'tensor.dim'(0x55db9aa9dfa0)
    ** Insert  : 'arith.constant'(0x55db9aa9e080)
    ** Insert  : 'tensor.dim'(0x55db9aa9e120)
    ** Insert  : 'tensor.empty'(0x55db9aa9e200)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'linalg.yield'(0x55db9aa9e420)
    ** Insert  : 'linalg.generic'(0x55db9aa9e4e0)
    ** Insert  : 'scf.yield'(0x55db9aa9e5f0)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'scf.yield'(0x55db9aa9e710)
    ** Insert  : 'scf.if'(0x55db9aa9e7d0)
    ** Insert  : 'tensor.dim'(0x55db9aa9e8c0)
    ** Insert  : 'arith.cmpi'(0x55db9aa9e9a0)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.constant'(0x55db9aa9eb00)
    ** Insert  : 'tensor.dim'(0x55db9aa9eba0)
    ** Insert  : 'arith.constant'(0x55db9aa9ec80)
    ** Insert  : 'tensor.dim'(0x55db9aa9ed20)
    ** Insert  : 'arith.constant'(0x55db9aa9ee00)
    ** Insert  : 'tensor.dim'(0x55db9aa9eea0)
    ** Insert  : 'tensor.empty'(0x55db9aa9ef80)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'linalg.yield'(0x55db9aa9f1a0)
    ** Insert  : 'linalg.generic'(0x55db9aa9f260)
    ** Insert  : 'scf.yield'(0x55db9aa9f370)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'scf.yield'(0x55db9aa9f490)
    ** Insert  : 'scf.if'(0x55db9aa9f550)
    ** Insert  : 'tensor.empty'(0x55db9aa9f640)
    ** Insert Block into detached Region (nullptr parent op)'    ** Insert  : 'arith.addf'(0x55db9aa9f910)
    ** Insert  : 'linalg.yield'(0x55db9aaa02d0)
    ** Insert  : 'linalg.generic'(0x55db9a8fc5c0)
    ** Replace : 'tosa.add'(0x55db9aa5c0b0)
"{anonymous}::PointwiseConverter<mlir::tosa::AddOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa8c790) {
      %0 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9a927bc0) {
      %1 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9a91ec00) {
      %2 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.maxui'(0x55db9a91ecb0) {
      %3 = "arith.maxui"(%1, %2) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa8da10) {
      %4 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa8da80) {
      %5 = "tensor.dim"(%arg0, %4) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa8db60) {
      %6 = "tensor.dim"(%arg0, %4) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.maxui'(0x55db9aa8dcb0) {
      %7 = "arith.maxui"(%5, %6) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa8dd90) {
      %8 = "arith.constant"() <{value = 2 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa8de30) {
      %9 = "tensor.dim"(%arg0, %8) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa8df10) {
      %10 = "tensor.dim"(%arg0, %8) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.maxui'(0x55db9aa8dff0) {
      %11 = "arith.maxui"(%9, %10) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa8e550) {
      %12 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa8e5f0) {
      %13 = "tensor.dim"(%arg0, %12) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa8e6d0) {
      %14 = "tensor.dim"(%arg0, %12) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.maxui'(0x55db9aa8e7b0) {
      %15 = "arith.maxui"(%13, %14) : (index, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa93550) {
      %16 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x55db9aa93ab0) {
      %17 = "arith.cmpi"(%16, %4) <{predicate = 0 : i64}> : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa93ff0) {
      %100 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa94090) {
      %101 = "tensor.dim"(%arg0, %100) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa94170) {
      %102 = "arith.constant"() <{value = 2 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa94210) {
      %103 = "tensor.dim"(%arg0, %102) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa942f0) {
      %104 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa94390) {
      %105 = "tensor.dim"(%arg0, %104) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9aa69a50) {
      %106 = "tensor.empty"(%3, %101, %103, %105) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aa8bf60) {
      "linalg.yield"(%arg18) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9aa73210) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa73180) {
      "scf.yield"(%107) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa5b820) {
      "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.if'(0x55db9aa734e0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa96ad0) {
      %19 = "tensor.dim"(%18, %4) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x55db9aa96bb0) {
      %20 = "arith.cmpi"(%19, %4) <{predicate = 0 : i64}> : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa97120) {
      %92 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa971c0) {
      %93 = "tensor.dim"(%18, %92) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa972a0) {
      %94 = "arith.constant"() <{value = 2 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa97340) {
      %95 = "tensor.dim"(%18, %94) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa97420) {
      %96 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa974c0) {
      %97 = "tensor.dim"(%18, %96) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9a9123e0) {
      %98 = "tensor.empty"(%93, %7, %95, %97) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aa97b50) {
      "linalg.yield"(%arg16) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9a912210) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa97c30) {
      "scf.yield"(%99) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa97d50) {
      "scf.yield"(%18) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.if'(0x55db9aa97e10) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa97f70) {
      %22 = "tensor.dim"(%21, %8) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x55db9aa98050) {
      %23 = "arith.cmpi"(%22, %4) <{predicate = 0 : i64}> : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa985c0) {
      %84 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa98660) {
      %85 = "tensor.dim"(%21, %84) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa98740) {
      %86 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa987e0) {
      %87 = "tensor.dim"(%21, %86) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa988c0) {
      %88 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa98960) {
      %89 = "tensor.dim"(%21, %88) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9a910790) {
      %90 = "tensor.empty"(%85, %87, %11, %89) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aa9a000) {
      "linalg.yield"(%arg14) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9a8fc4d0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9a0e0) {
      "scf.yield"(%91) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9a200) {
      "scf.yield"(%21) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.if'(0x55db9aa9a2c0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9a420) {
      %25 = "tensor.dim"(%24, %12) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x55db9aa9a500) {
      %26 = "arith.cmpi"(%25, %4) <{predicate = 0 : i64}> : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9aa70) {
      %76 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9ab10) {
      %77 = "tensor.dim"(%24, %76) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9abf0) {
      %78 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9ac90) {
      %79 = "tensor.dim"(%24, %78) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9ad70) {
      %80 = "arith.constant"() <{value = 2 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9ae10) {
      %81 = "tensor.dim"(%24, %80) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9a8e7330) {
      %82 = "tensor.empty"(%77, %79, %81, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aa9b4a0) {
      "linalg.yield"(%arg12) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9aa9b560) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9b670) {
      "scf.yield"(%83) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9b790) {
      "scf.yield"(%24) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.if'(0x55db9aa9b850) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9b940) {
      %28 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x55db9aa9ba20) {
      %29 = "arith.cmpi"(%28, %4) <{predicate = 0 : i64}> : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9bb80) {
      %68 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9bc20) {
      %69 = "tensor.dim"(%arg0, %68) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9bd00) {
      %70 = "arith.constant"() <{value = 2 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9bda0) {
      %71 = "tensor.dim"(%arg0, %70) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9be80) {
      %72 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9bf20) {
      %73 = "tensor.dim"(%arg0, %72) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9aa9c000) {
      %74 = "tensor.empty"(%3, %69, %71, %73) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aa9c220) {
      "linalg.yield"(%arg10) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9aa9c2e0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9c3f0) {
      "scf.yield"(%75) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9c510) {
      "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.if'(0x55db9aa9c5d0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9c6c0) {
      %31 = "tensor.dim"(%30, %4) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x55db9aa9c7a0) {
      %32 = "arith.cmpi"(%31, %4) <{predicate = 0 : i64}> : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9c900) {
      %60 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9c9a0) {
      %61 = "tensor.dim"(%30, %60) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9ca80) {
      %62 = "arith.constant"() <{value = 2 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9cb20) {
      %63 = "tensor.dim"(%30, %62) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9cc00) {
      %64 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9cca0) {
      %65 = "tensor.dim"(%30, %64) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9aa9cd80) {
      %66 = "tensor.empty"(%61, %7, %63, %65) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aa9d6a0) {
      "linalg.yield"(%arg8) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9aa9d760) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9d870) {
      "scf.yield"(%67) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9d990) {
      "scf.yield"(%30) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.if'(0x55db9aa9da50) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9db40) {
      %34 = "tensor.dim"(%33, %8) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x55db9aa9dc20) {
      %35 = "arith.cmpi"(%34, %4) <{predicate = 0 : i64}> : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9dd80) {
      %52 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9de20) {
      %53 = "tensor.dim"(%33, %52) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9df00) {
      %54 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9dfa0) {
      %55 = "tensor.dim"(%33, %54) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9e080) {
      %56 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9e120) {
      %57 = "tensor.dim"(%33, %56) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9aa9e200) {
      %58 = "tensor.empty"(%53, %55, %11, %57) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aa9e420) {
      "linalg.yield"(%arg6) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9aa9e4e0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9e5f0) {
      "scf.yield"(%59) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9e710) {
      "scf.yield"(%33) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.if'(0x55db9aa9e7d0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9e8c0) {
      %37 = "tensor.dim"(%36, %12) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.cmpi'(0x55db9aa9e9a0) {
      %38 = "arith.cmpi"(%37, %4) <{predicate = 0 : i64}> : (index, index) -> i1

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9eb00) {
      %44 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9eba0) {
      %45 = "tensor.dim"(%36, %44) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9ec80) {
      %46 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9ed20) {
      %47 = "tensor.dim"(%36, %46) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x55db9aa9ee00) {
      %48 = "arith.constant"() <{value = 2 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x55db9aa9eea0) {
      %49 = "tensor.dim"(%36, %48) : (tensor<?x?x?x?xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9aa9ef80) {
      %50 = "tensor.empty"(%45, %47, %49, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aa9f1a0) {
      "linalg.yield"(%arg4) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9aa9f260) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9f370) {
      "scf.yield"(%51) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.yield'(0x55db9aa9f490) {
      "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'scf.if'(0x55db9aa9f550) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x55db9aa9f640) {
      %40 = "tensor.empty"(%3, %7, %11, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addf'(0x55db9aa9f910) {
      %43 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x55db9aaa02d0) {
      "linalg.yield"(%43) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x55db9a8fc5c0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NRegions<2>::Impl<mlir::TypeID::get<mlir::OpTrait::NRegions<2>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlockImplicitTerminator<mlir::scf::YieldOp>::Impl<mlir::TypeID::get<mlir::OpTrait::SingleBlockImplicitTerminator<mlir::scf::YieldOp>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface::Trait<mlir::TypeID::get<mlir::RegionBranchOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::InferTypeOpAdaptor<mlir::TypeID::get<mlir::OpTrait::InferTypeOpAdaptor>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::scf::ExecuteRegionOp, mlir::scf::ForOp, mlir::scf::IfOp, mlir::scf::IndexSwitchOp, mlir::scf::WhileOp>::Impl<mlir::TypeID::get<mlir::OpTrait::HasParent<mlir::scf::ExecuteRegionOp, mlir::scf::ForOp, mlir::scf::IfOp, mlir::scf::IndexSwitchOp, mlir::scf::WhileOp>::Impl>()::Empty>)
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %dim_0 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %0 = arith.maxui %dim, %dim_0 : index
  %c1 = arith.constant 1 : index
  %dim_1 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %dim_2 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %1 = arith.maxui %dim_1, %dim_2 : index
  %c2 = arith.constant 2 : index
  %dim_3 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %dim_4 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %2 = arith.maxui %dim_3, %dim_4 : index
  %c3 = arith.constant 3 : index
  %dim_5 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %dim_6 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %3 = arith.maxui %dim_5, %dim_6 : index
  %dim_7 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %4 = arith.cmpi eq, %dim_7, %c1 : index
  %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
    %c1_15 = arith.constant 1 : index
    %dim_16 = tensor.dim %arg0, %c1_15 : tensor<?x?x?x?xf32>
    %c2_17 = arith.constant 2 : index
    %dim_18 = tensor.dim %arg0, %c2_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %arg0, %c3_19 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%0, %dim_16, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_8 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
  %6 = arith.cmpi eq, %dim_8, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %5, %c0_15 : tensor<?x?x?x?xf32>
    %c2_17 = arith.constant 2 : index
    %dim_18 = tensor.dim %5, %c2_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %5, %c3_19 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%dim_16, %1, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %5 : tensor<?x?x?x?xf32>
  }
  %dim_9 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
  %8 = arith.cmpi eq, %dim_9, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %7, %c0_15 : tensor<?x?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %dim_18 = tensor.dim %7, %c1_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %7, %c3_19 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%dim_16, %dim_18, %2, %dim_20) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %dim_10 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
  %10 = arith.cmpi eq, %dim_10, %c1 : index
  %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %9, %c0_15 : tensor<?x?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %dim_18 = tensor.dim %9, %c1_17 : tensor<?x?x?x?xf32>
    %c2_19 = arith.constant 2 : index
    %dim_20 = tensor.dim %9, %c2_19 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%dim_16, %dim_18, %dim_20, %3) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %9 : tensor<?x?x?x?xf32>
  }
  %dim_11 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %12 = arith.cmpi eq, %dim_11, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %c1_15 = arith.constant 1 : index
    %dim_16 = tensor.dim %arg0, %c1_15 : tensor<?x?x?x?xf32>
    %c2_17 = arith.constant 2 : index
    %dim_18 = tensor.dim %arg0, %c2_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %arg0, %c3_19 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%0, %dim_16, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_12 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
  %14 = arith.cmpi eq, %dim_12, %c1 : index
  %15 = scf.if %14 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %13, %c0_15 : tensor<?x?x?x?xf32>
    %c2_17 = arith.constant 2 : index
    %dim_18 = tensor.dim %13, %c2_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %13, %c3_19 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%dim_16, %1, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_13 = tensor.dim %15, %c2 : tensor<?x?x?x?xf32>
  %16 = arith.cmpi eq, %dim_13, %c1 : index
  %17 = scf.if %16 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %15, %c0_15 : tensor<?x?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %dim_18 = tensor.dim %15, %c1_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %15, %c3_19 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%dim_16, %dim_18, %2, %dim_20) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %15 : tensor<?x?x?x?xf32>
  }
  %dim_14 = tensor.dim %17, %c3 : tensor<?x?x?x?xf32>
  %18 = arith.cmpi eq, %dim_14, %c1 : index
  %19 = scf.if %18 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %17, %c0_15 : tensor<?x?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %dim_18 = tensor.dim %17, %c1_17 : tensor<?x?x?x?xf32>
    %c2_19 = arith.constant 2 : index
    %dim_20 = tensor.dim %17, %c2_19 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%dim_16, %dim_18, %dim_20, %3) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %17 : tensor<?x?x?x?xf32>
  }
  %20 = tensor.empty(%0, %1, %2, %3) : tensor<?x?x?x?xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11, %19 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%20 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %in_15: f32, %out: f32):
    %23 = arith.addf %in, %in_15 : f32
    linalg.yield %23 : f32
  } -> tensor<?x?x?x?xf32>
  %22 = tosa.add %arg0, %arg0 : (tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%22) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %22 : tensor<?x?x?x?xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%42) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%42) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After TosaToLinalg (tosa-to-linalg) //----- //
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %dim_0 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %0 = arith.maxui %dim, %dim_0 : index
  %c1 = arith.constant 1 : index
  %dim_1 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %dim_2 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %1 = arith.maxui %dim_1, %dim_2 : index
  %c2 = arith.constant 2 : index
  %dim_3 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %dim_4 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %2 = arith.maxui %dim_3, %dim_4 : index
  %c3 = arith.constant 3 : index
  %dim_5 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %dim_6 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %3 = arith.maxui %dim_5, %dim_6 : index
  %dim_7 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %4 = arith.cmpi eq, %dim_7, %c1 : index
  %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
    %c1_15 = arith.constant 1 : index
    %dim_16 = tensor.dim %arg0, %c1_15 : tensor<?x?x?x?xf32>
    %c2_17 = arith.constant 2 : index
    %dim_18 = tensor.dim %arg0, %c2_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %arg0, %c3_19 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%0, %dim_16, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_8 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
  %6 = arith.cmpi eq, %dim_8, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %5, %c0_15 : tensor<?x?x?x?xf32>
    %c2_17 = arith.constant 2 : index
    %dim_18 = tensor.dim %5, %c2_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %5, %c3_19 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%dim_16, %1, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %5 : tensor<?x?x?x?xf32>
  }
  %dim_9 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
  %8 = arith.cmpi eq, %dim_9, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %7, %c0_15 : tensor<?x?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %dim_18 = tensor.dim %7, %c1_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %7, %c3_19 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%dim_16, %dim_18, %2, %dim_20) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %dim_10 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
  %10 = arith.cmpi eq, %dim_10, %c1 : index
  %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %9, %c0_15 : tensor<?x?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %dim_18 = tensor.dim %9, %c1_17 : tensor<?x?x?x?xf32>
    %c2_19 = arith.constant 2 : index
    %dim_20 = tensor.dim %9, %c2_19 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%dim_16, %dim_18, %dim_20, %3) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %9 : tensor<?x?x?x?xf32>
  }
  %dim_11 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %12 = arith.cmpi eq, %dim_11, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %c1_15 = arith.constant 1 : index
    %dim_16 = tensor.dim %arg0, %c1_15 : tensor<?x?x?x?xf32>
    %c2_17 = arith.constant 2 : index
    %dim_18 = tensor.dim %arg0, %c2_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %arg0, %c3_19 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%0, %dim_16, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_12 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
  %14 = arith.cmpi eq, %dim_12, %c1 : index
  %15 = scf.if %14 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %13, %c0_15 : tensor<?x?x?x?xf32>
    %c2_17 = arith.constant 2 : index
    %dim_18 = tensor.dim %13, %c2_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %13, %c3_19 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%dim_16, %1, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_13 = tensor.dim %15, %c2 : tensor<?x?x?x?xf32>
  %16 = arith.cmpi eq, %dim_13, %c1 : index
  %17 = scf.if %16 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %15, %c0_15 : tensor<?x?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %dim_18 = tensor.dim %15, %c1_17 : tensor<?x?x?x?xf32>
    %c3_19 = arith.constant 3 : index
    %dim_20 = tensor.dim %15, %c3_19 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%dim_16, %dim_18, %2, %dim_20) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %15 : tensor<?x?x?x?xf32>
  }
  %dim_14 = tensor.dim %17, %c3 : tensor<?x?x?x?xf32>
  %18 = arith.cmpi eq, %dim_14, %c1 : index
  %19 = scf.if %18 -> (tensor<?x?x?x?xf32>) {
    %c0_15 = arith.constant 0 : index
    %dim_16 = tensor.dim %17, %c0_15 : tensor<?x?x?x?xf32>
    %c1_17 = arith.constant 1 : index
    %dim_18 = tensor.dim %17, %c1_17 : tensor<?x?x?x?xf32>
    %c2_19 = arith.constant 2 : index
    %dim_20 = tensor.dim %17, %c2_19 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%dim_16, %dim_18, %dim_20, %3) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %17 : tensor<?x?x?x?xf32>
  }
  %20 = tensor.empty(%0, %1, %2, %3) : tensor<?x?x?x?xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11, %19 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%20 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %in_15: f32, %out: f32):
    %22 = arith.addf %in, %in_15 : f32
    linalg.yield %22 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%21) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %21 : tensor<?x?x?x?xf32>
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::IndexType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::IndexType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedDimOpInterface::Trait<mlir::TypeID::get<mlir::ShapedDimOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultType<mlir::TypeID::get<mlir::OpTrait::SameOperandsAndResultType>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface::Trait<mlir::TypeID::get<mlir::VectorUnrollOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Elementwise<mlir::TypeID::get<mlir::OpTrait::Elementwise>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Scalarizable<mlir::TypeID::get<mlir::OpTrait::Scalarizable>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Vectorizable<mlir::TypeID::get<mlir::OpTrait::Vectorizable>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Tensorizable<mlir::TypeID::get<mlir::OpTrait::Tensorizable>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameTypeOperands<mlir::TypeID::get<mlir::OpTrait::SameTypeOperands>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface::Trait<mlir::TypeID::get<mlir::arith::ArithFastMathInterface::Trait>()::Empty>)

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x55db9a9287a0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8c790) {
  %0 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9a927bc0) {
  %1 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9a91ec00) {
  %2 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9a91ecb0) {
  %3 = "arith.maxui"(%1, %2) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8da10) {
  %4 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8da80) {
  %5 = "tensor.dim"(%arg0, %4) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8db60) {
  %6 = "tensor.dim"(%arg0, %4) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8dcb0) {
  %7 = "arith.maxui"(%5, %6) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8dd90) {
  %8 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8de30) {
  %9 = "tensor.dim"(%arg0, %8) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8df10) {
  %10 = "tensor.dim"(%arg0, %8) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8dff0) {
  %11 = "arith.maxui"(%9, %10) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8e550) {
  %12 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %13 = "tensor.dim"(%arg0, %12) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8e6d0) {
  %14 = "tensor.dim"(%arg0, %12) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8e7b0) {
  %15 = "arith.maxui"(%13, %14) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa93550) {
  %16 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %17 = "arith.cmpi"(%16, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa734e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa93ff0) {
  %99 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa94090) {
  %100 = "tensor.dim"(%arg0, %99) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa94170) {
  %101 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa94210) {
  %102 = "tensor.dim"(%arg0, %101) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa942f0) {
  %103 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa94390) {
  %104 = "tensor.dim"(%arg0, %103) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa69a50) {
  %105 = "tensor.empty"(%3, %100, %102, %104) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa73210) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg18) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%106) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %19 = "tensor.dim"(%18, %4) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %20 = "arith.cmpi"(%19, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa97e10) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa97120) {
  %91 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa971c0) {
  %92 = "tensor.dim"(%18, %91) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa972a0) {
  %93 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa97340) {
  %94 = "tensor.dim"(%18, %93) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa97420) {
  %95 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa974c0) {
  %96 = "tensor.dim"(%18, %95) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a9123e0) {
  %97 = "tensor.empty"(%92, %7, %94, %96) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a912210) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg16) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%98) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%18) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa97f70) {
  %22 = "tensor.dim"(%21, %8) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa98050) {
  %23 = "arith.cmpi"(%22, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9a2c0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa985c0) {
  %83 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa98660) {
  %84 = "tensor.dim"(%21, %83) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa98740) {
  %85 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa987e0) {
  %86 = "tensor.dim"(%21, %85) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa988c0) {
  %87 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa98960) {
  %88 = "tensor.dim"(%21, %87) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a910790) {
  %89 = "tensor.empty"(%84, %86, %11, %88) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a8fc4d0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg14) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%90) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%21) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9a420) {
  %25 = "tensor.dim"(%24, %12) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %26 = "arith.cmpi"(%25, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9b850) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9aa70) {
  %75 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %76 = "tensor.dim"(%24, %75) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9abf0) {
  %77 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %78 = "tensor.dim"(%24, %77) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9ad70) {
  %79 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %80 = "tensor.dim"(%24, %79) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a8e7330) {
  %81 = "tensor.empty"(%76, %78, %80, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9b560) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg12) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%82) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%24) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9b940) {
  %28 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9ba20) {
  %29 = "arith.cmpi"(%28, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9c5d0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9bb80) {
  %67 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9bc20) {
  %68 = "tensor.dim"(%arg0, %67) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9bd00) {
  %69 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9bda0) {
  %70 = "tensor.dim"(%arg0, %69) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9be80) {
  %71 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9bf20) {
  %72 = "tensor.dim"(%arg0, %71) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9c000) {
  %73 = "tensor.empty"(%3, %68, %70, %72) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9c2e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9c220) {
  "linalg.yield"(%arg10) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9c3f0) {
  "scf.yield"(%74) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9c510) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9c6c0) {
  %31 = "tensor.dim"(%30, %4) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9c7a0) {
  %32 = "arith.cmpi"(%31, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9da50) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9c900) {
  %59 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9c9a0) {
  %60 = "tensor.dim"(%30, %59) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9ca80) {
  %61 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9cb20) {
  %62 = "tensor.dim"(%30, %61) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9cc00) {
  %63 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9cca0) {
  %64 = "tensor.dim"(%30, %63) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9cd80) {
  %65 = "tensor.empty"(%60, %7, %62, %64) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9d760) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9d6a0) {
  "linalg.yield"(%arg8) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9d870) {
  "scf.yield"(%66) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9d990) {
  "scf.yield"(%30) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9db40) {
  %34 = "tensor.dim"(%33, %8) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9dc20) {
  %35 = "arith.cmpi"(%34, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9e7d0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9dd80) {
  %51 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9de20) {
  %52 = "tensor.dim"(%33, %51) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9df00) {
  %53 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9dfa0) {
  %54 = "tensor.dim"(%33, %53) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9e080) {
  %55 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9e120) {
  %56 = "tensor.dim"(%33, %55) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9e200) {
  %57 = "tensor.empty"(%52, %54, %11, %56) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9e4e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9e420) {
  "linalg.yield"(%arg6) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9e5f0) {
  "scf.yield"(%58) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9e710) {
  "scf.yield"(%33) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9e8c0) {
  %37 = "tensor.dim"(%36, %12) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9e9a0) {
  %38 = "arith.cmpi"(%37, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9f550) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9eb00) {
  %43 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9eba0) {
  %44 = "tensor.dim"(%36, %43) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9ec80) {
  %45 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ed20) {
  %46 = "tensor.dim"(%36, %45) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9ee00) {
  %47 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9eea0) {
  %48 = "tensor.dim"(%36, %47) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9ef80) {
  %49 = "tensor.empty"(%44, %46, %48, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9f260) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9f1a0) {
  "linalg.yield"(%arg4) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9f370) {
  "scf.yield"(%50) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9f490) {
  "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9f640) {
  %40 = "tensor.empty"(%3, %7, %11, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a8fc5c0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x55db9aa9f910) {
  %42 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%42) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%41) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%41) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ConvertLLHToTensorPass (convert-llh-to-tensor) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %c0 = arith.constant 0 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %0 = arith.maxui %dim, %dim_0 : index
    %c1 = arith.constant 1 : index
    %dim_1 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %dim_2 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %1 = arith.maxui %dim_1, %dim_2 : index
    %c2 = arith.constant 2 : index
    %dim_3 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %dim_4 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %2 = arith.maxui %dim_3, %dim_4 : index
    %c3 = arith.constant 3 : index
    %dim_5 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %dim_6 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %3 = arith.maxui %dim_5, %dim_6 : index
    %dim_7 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %4 = arith.cmpi eq, %dim_7, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %c1_15 = arith.constant 1 : index
      %dim_16 = tensor.dim %arg0, %c1_15 : tensor<?x?x?x?xf32>
      %c2_17 = arith.constant 2 : index
      %dim_18 = tensor.dim %arg0, %c2_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %arg0, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%0, %dim_16, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_8 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_8, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %5, %c0_15 : tensor<?x?x?x?xf32>
      %c2_17 = arith.constant 2 : index
      %dim_18 = tensor.dim %5, %c2_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %5, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %1, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %dim_9 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
    %8 = arith.cmpi eq, %dim_9, %c1 : index
    %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %7, %c0_15 : tensor<?x?x?x?xf32>
      %c1_17 = arith.constant 1 : index
      %dim_18 = tensor.dim %7, %c1_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %7, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %dim_18, %2, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %7 : tensor<?x?x?x?xf32>
    }
    %dim_10 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
    %10 = arith.cmpi eq, %dim_10, %c1 : index
    %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %9, %c0_15 : tensor<?x?x?x?xf32>
      %c1_17 = arith.constant 1 : index
      %dim_18 = tensor.dim %9, %c1_17 : tensor<?x?x?x?xf32>
      %c2_19 = arith.constant 2 : index
      %dim_20 = tensor.dim %9, %c2_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %dim_18, %dim_20, %3) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %9 : tensor<?x?x?x?xf32>
    }
    %dim_11 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %12 = arith.cmpi eq, %dim_11, %c1 : index
    %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
      %c1_15 = arith.constant 1 : index
      %dim_16 = tensor.dim %arg0, %c1_15 : tensor<?x?x?x?xf32>
      %c2_17 = arith.constant 2 : index
      %dim_18 = tensor.dim %arg0, %c2_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %arg0, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%0, %dim_16, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_12 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
    %14 = arith.cmpi eq, %dim_12, %c1 : index
    %15 = scf.if %14 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %13, %c0_15 : tensor<?x?x?x?xf32>
      %c2_17 = arith.constant 2 : index
      %dim_18 = tensor.dim %13, %c2_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %13, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %1, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %13 : tensor<?x?x?x?xf32>
    }
    %dim_13 = tensor.dim %15, %c2 : tensor<?x?x?x?xf32>
    %16 = arith.cmpi eq, %dim_13, %c1 : index
    %17 = scf.if %16 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %15, %c0_15 : tensor<?x?x?x?xf32>
      %c1_17 = arith.constant 1 : index
      %dim_18 = tensor.dim %15, %c1_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %15, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %dim_18, %2, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %15 : tensor<?x?x?x?xf32>
    }
    %dim_14 = tensor.dim %17, %c3 : tensor<?x?x?x?xf32>
    %18 = arith.cmpi eq, %dim_14, %c1 : index
    %19 = scf.if %18 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %17, %c0_15 : tensor<?x?x?x?xf32>
      %c1_17 = arith.constant 1 : index
      %dim_18 = tensor.dim %17, %c1_17 : tensor<?x?x?x?xf32>
      %c2_19 = arith.constant 2 : index
      %dim_20 = tensor.dim %17, %c2_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %dim_18, %dim_20, %3) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %17 : tensor<?x?x?x?xf32>
    }
    %20 = tensor.empty(%0, %1, %2, %3) : tensor<?x?x?x?xf32>
    %21 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11, %19 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%20 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %in_15: f32, %out: f32):
      %22 = arith.addf %in, %in_15 : f32
      linalg.yield %22 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%21) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %21 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x55db9a9287a0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8c790) {
  %0 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9a927bc0) {
  %1 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9a91ec00) {
  %2 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9a91ecb0) {
  %3 = "arith.maxui"(%1, %2) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8da10) {
  %4 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8da80) {
  %5 = "tensor.dim"(%arg0, %4) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8db60) {
  %6 = "tensor.dim"(%arg0, %4) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8dcb0) {
  %7 = "arith.maxui"(%5, %6) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8dd90) {
  %8 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8de30) {
  %9 = "tensor.dim"(%arg0, %8) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8df10) {
  %10 = "tensor.dim"(%arg0, %8) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8dff0) {
  %11 = "arith.maxui"(%9, %10) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8e550) {
  %12 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %13 = "tensor.dim"(%arg0, %12) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8e6d0) {
  %14 = "tensor.dim"(%arg0, %12) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8e7b0) {
  %15 = "arith.maxui"(%13, %14) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa93550) {
  %16 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %17 = "arith.cmpi"(%16, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa734e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa93ff0) {
  %99 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa94090) {
  %100 = "tensor.dim"(%arg0, %99) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa94170) {
  %101 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa94210) {
  %102 = "tensor.dim"(%arg0, %101) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa942f0) {
  %103 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa94390) {
  %104 = "tensor.dim"(%arg0, %103) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa69a50) {
  %105 = "tensor.empty"(%3, %100, %102, %104) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa73210) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg18) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%106) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %19 = "tensor.dim"(%18, %4) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %20 = "arith.cmpi"(%19, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa97e10) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa97120) {
  %91 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa971c0) {
  %92 = "tensor.dim"(%18, %91) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa972a0) {
  %93 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa97340) {
  %94 = "tensor.dim"(%18, %93) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa97420) {
  %95 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa974c0) {
  %96 = "tensor.dim"(%18, %95) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a9123e0) {
  %97 = "tensor.empty"(%92, %7, %94, %96) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a912210) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg16) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%98) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%18) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa97f70) {
  %22 = "tensor.dim"(%21, %8) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa98050) {
  %23 = "arith.cmpi"(%22, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9a2c0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa985c0) {
  %83 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa98660) {
  %84 = "tensor.dim"(%21, %83) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa98740) {
  %85 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa987e0) {
  %86 = "tensor.dim"(%21, %85) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa988c0) {
  %87 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa98960) {
  %88 = "tensor.dim"(%21, %87) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a910790) {
  %89 = "tensor.empty"(%84, %86, %11, %88) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a8fc4d0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg14) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%90) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%21) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9a420) {
  %25 = "tensor.dim"(%24, %12) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %26 = "arith.cmpi"(%25, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9b850) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9aa70) {
  %75 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %76 = "tensor.dim"(%24, %75) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9abf0) {
  %77 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %78 = "tensor.dim"(%24, %77) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9ad70) {
  %79 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %80 = "tensor.dim"(%24, %79) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a8e7330) {
  %81 = "tensor.empty"(%76, %78, %80, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9b560) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg12) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%82) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%24) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9b940) {
  %28 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9ba20) {
  %29 = "arith.cmpi"(%28, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9c5d0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9bb80) {
  %67 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9bc20) {
  %68 = "tensor.dim"(%arg0, %67) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9bd00) {
  %69 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9bda0) {
  %70 = "tensor.dim"(%arg0, %69) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9be80) {
  %71 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9bf20) {
  %72 = "tensor.dim"(%arg0, %71) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9c000) {
  %73 = "tensor.empty"(%3, %68, %70, %72) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9c2e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9c220) {
  "linalg.yield"(%arg10) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9c3f0) {
  "scf.yield"(%74) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9c510) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9c6c0) {
  %31 = "tensor.dim"(%30, %4) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9c7a0) {
  %32 = "arith.cmpi"(%31, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9da50) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9c900) {
  %59 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9c9a0) {
  %60 = "tensor.dim"(%30, %59) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9ca80) {
  %61 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9cb20) {
  %62 = "tensor.dim"(%30, %61) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9cc00) {
  %63 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9cca0) {
  %64 = "tensor.dim"(%30, %63) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9cd80) {
  %65 = "tensor.empty"(%60, %7, %62, %64) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9d760) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9d6a0) {
  "linalg.yield"(%arg8) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9d870) {
  "scf.yield"(%66) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9d990) {
  "scf.yield"(%30) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9db40) {
  %34 = "tensor.dim"(%33, %8) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9dc20) {
  %35 = "arith.cmpi"(%34, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9e7d0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9dd80) {
  %51 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9de20) {
  %52 = "tensor.dim"(%33, %51) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9df00) {
  %53 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9dfa0) {
  %54 = "tensor.dim"(%33, %53) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9e080) {
  %55 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9e120) {
  %56 = "tensor.dim"(%33, %55) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9e200) {
  %57 = "tensor.empty"(%52, %54, %11, %56) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9e4e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9e420) {
  "linalg.yield"(%arg6) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9e5f0) {
  "scf.yield"(%58) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9e710) {
  "scf.yield"(%33) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9e8c0) {
  %37 = "tensor.dim"(%36, %12) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9e9a0) {
  %38 = "arith.cmpi"(%37, %4) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9f550) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9eb00) {
  %43 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9eba0) {
  %44 = "tensor.dim"(%36, %43) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9ec80) {
  %45 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ed20) {
  %46 = "tensor.dim"(%36, %45) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa9ee00) {
  %47 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9eea0) {
  %48 = "tensor.dim"(%36, %47) : (tensor<?x?x?x?xf32>, index) -> index

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9ef80) {
  %49 = "tensor.empty"(%44, %46, %48, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9f260) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9f1a0) {
  "linalg.yield"(%arg4) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9f370) {
  "scf.yield"(%50) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9f490) {
  "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9f640) {
  %40 = "tensor.empty"(%3, %7, %11, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a8fc5c0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x55db9aa9f910) {
  %42 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%42) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%41) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%41) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
// -----// IR Dump After TosaToArith (tosa-to-arith) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %c0 = arith.constant 0 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %0 = arith.maxui %dim, %dim_0 : index
    %c1 = arith.constant 1 : index
    %dim_1 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %dim_2 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %1 = arith.maxui %dim_1, %dim_2 : index
    %c2 = arith.constant 2 : index
    %dim_3 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %dim_4 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %2 = arith.maxui %dim_3, %dim_4 : index
    %c3 = arith.constant 3 : index
    %dim_5 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %dim_6 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %3 = arith.maxui %dim_5, %dim_6 : index
    %dim_7 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %4 = arith.cmpi eq, %dim_7, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %c1_15 = arith.constant 1 : index
      %dim_16 = tensor.dim %arg0, %c1_15 : tensor<?x?x?x?xf32>
      %c2_17 = arith.constant 2 : index
      %dim_18 = tensor.dim %arg0, %c2_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %arg0, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%0, %dim_16, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_8 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_8, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %5, %c0_15 : tensor<?x?x?x?xf32>
      %c2_17 = arith.constant 2 : index
      %dim_18 = tensor.dim %5, %c2_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %5, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %1, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %dim_9 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
    %8 = arith.cmpi eq, %dim_9, %c1 : index
    %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %7, %c0_15 : tensor<?x?x?x?xf32>
      %c1_17 = arith.constant 1 : index
      %dim_18 = tensor.dim %7, %c1_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %7, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %dim_18, %2, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %7 : tensor<?x?x?x?xf32>
    }
    %dim_10 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
    %10 = arith.cmpi eq, %dim_10, %c1 : index
    %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %9, %c0_15 : tensor<?x?x?x?xf32>
      %c1_17 = arith.constant 1 : index
      %dim_18 = tensor.dim %9, %c1_17 : tensor<?x?x?x?xf32>
      %c2_19 = arith.constant 2 : index
      %dim_20 = tensor.dim %9, %c2_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %dim_18, %dim_20, %3) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %9 : tensor<?x?x?x?xf32>
    }
    %dim_11 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %12 = arith.cmpi eq, %dim_11, %c1 : index
    %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
      %c1_15 = arith.constant 1 : index
      %dim_16 = tensor.dim %arg0, %c1_15 : tensor<?x?x?x?xf32>
      %c2_17 = arith.constant 2 : index
      %dim_18 = tensor.dim %arg0, %c2_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %arg0, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%0, %dim_16, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_12 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
    %14 = arith.cmpi eq, %dim_12, %c1 : index
    %15 = scf.if %14 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %13, %c0_15 : tensor<?x?x?x?xf32>
      %c2_17 = arith.constant 2 : index
      %dim_18 = tensor.dim %13, %c2_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %13, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %1, %dim_18, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %13 : tensor<?x?x?x?xf32>
    }
    %dim_13 = tensor.dim %15, %c2 : tensor<?x?x?x?xf32>
    %16 = arith.cmpi eq, %dim_13, %c1 : index
    %17 = scf.if %16 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %15, %c0_15 : tensor<?x?x?x?xf32>
      %c1_17 = arith.constant 1 : index
      %dim_18 = tensor.dim %15, %c1_17 : tensor<?x?x?x?xf32>
      %c3_19 = arith.constant 3 : index
      %dim_20 = tensor.dim %15, %c3_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %dim_18, %2, %dim_20) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %15 : tensor<?x?x?x?xf32>
    }
    %dim_14 = tensor.dim %17, %c3 : tensor<?x?x?x?xf32>
    %18 = arith.cmpi eq, %dim_14, %c1 : index
    %19 = scf.if %18 -> (tensor<?x?x?x?xf32>) {
      %c0_15 = arith.constant 0 : index
      %dim_16 = tensor.dim %17, %c0_15 : tensor<?x?x?x?xf32>
      %c1_17 = arith.constant 1 : index
      %dim_18 = tensor.dim %17, %c1_17 : tensor<?x?x?x?xf32>
      %c2_19 = arith.constant 2 : index
      %dim_20 = tensor.dim %17, %c2_19 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_16, %dim_18, %dim_20, %3) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %17 : tensor<?x?x?x?xf32>
    }
    %20 = tensor.empty(%0, %1, %2, %3) : tensor<?x?x?x?xf32>
    %21 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11, %19 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%20 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %in_15: f32, %out: f32):
      %22 = arith.addf %in, %in_15 : f32
      linalg.yield %22 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%21) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %21 : tensor<?x?x?x?xf32>
  }
}


** Replace : 'arith.constant'(0x55db9aa93ff0)
** Modified: 'tensor.dim'(0x55db9aa94090)
** Erase   : 'arith.constant'(0x55db9aa93ff0)
** Replace : 'arith.constant'(0x55db9aa94170)
** Modified: 'tensor.dim'(0x55db9aa94210)
** Erase   : 'arith.constant'(0x55db9aa94170)
** Replace : 'arith.constant'(0x55db9aa942f0)
** Modified: 'tensor.dim'(0x55db9aa94390)
** Erase   : 'arith.constant'(0x55db9aa942f0)
** Replace : 'arith.constant'(0x55db9aa97120)
** Modified: 'tensor.dim'(0x55db9aa971c0)
** Erase   : 'arith.constant'(0x55db9aa97120)
** Replace : 'arith.constant'(0x55db9aa972a0)
** Modified: 'tensor.dim'(0x55db9aa97340)
** Erase   : 'arith.constant'(0x55db9aa972a0)
** Replace : 'arith.constant'(0x55db9aa97420)
** Modified: 'tensor.dim'(0x55db9aa974c0)
** Erase   : 'arith.constant'(0x55db9aa97420)
** Replace : 'arith.constant'(0x55db9aa985c0)
** Modified: 'tensor.dim'(0x55db9aa98660)
** Erase   : 'arith.constant'(0x55db9aa985c0)
** Replace : 'arith.constant'(0x55db9aa98740)
** Modified: 'tensor.dim'(0x55db9aa987e0)
** Erase   : 'arith.constant'(0x55db9aa98740)
** Replace : 'arith.constant'(0x55db9aa988c0)
** Modified: 'tensor.dim'(0x55db9aa98960)
** Erase   : 'arith.constant'(0x55db9aa988c0)
** Replace : 'arith.constant'(0x55db9aa9aa70)
** Modified: 'tensor.dim'(0x55db9aa9ab10)
** Erase   : 'arith.constant'(0x55db9aa9aa70)
** Replace : 'arith.constant'(0x55db9aa9abf0)
** Modified: 'tensor.dim'(0x55db9aa9ac90)
** Erase   : 'arith.constant'(0x55db9aa9abf0)
** Replace : 'arith.constant'(0x55db9aa9ad70)
** Modified: 'tensor.dim'(0x55db9aa9ae10)
** Erase   : 'arith.constant'(0x55db9aa9ad70)
** Replace : 'arith.constant'(0x55db9aa9bb80)
** Modified: 'tensor.dim'(0x55db9aa9bc20)
** Erase   : 'arith.constant'(0x55db9aa9bb80)
** Replace : 'arith.constant'(0x55db9aa9bd00)
** Modified: 'tensor.dim'(0x55db9aa9bda0)
** Erase   : 'arith.constant'(0x55db9aa9bd00)
** Replace : 'arith.constant'(0x55db9aa9be80)
** Modified: 'tensor.dim'(0x55db9aa9bf20)
** Erase   : 'arith.constant'(0x55db9aa9be80)
** Replace : 'arith.constant'(0x55db9aa9c900)
** Modified: 'tensor.dim'(0x55db9aa9c9a0)
** Erase   : 'arith.constant'(0x55db9aa9c900)
** Replace : 'arith.constant'(0x55db9aa9ca80)
** Modified: 'tensor.dim'(0x55db9aa9cb20)
** Erase   : 'arith.constant'(0x55db9aa9ca80)
** Replace : 'arith.constant'(0x55db9aa9cc00)
** Modified: 'tensor.dim'(0x55db9aa9cca0)
** Erase   : 'arith.constant'(0x55db9aa9cc00)
** Replace : 'arith.constant'(0x55db9aa9dd80)
** Modified: 'tensor.dim'(0x55db9aa9de20)
** Erase   : 'arith.constant'(0x55db9aa9dd80)
** Replace : 'arith.constant'(0x55db9aa9df00)
** Modified: 'tensor.dim'(0x55db9aa9dfa0)
** Erase   : 'arith.constant'(0x55db9aa9df00)
** Replace : 'arith.constant'(0x55db9aa9e080)
** Modified: 'tensor.dim'(0x55db9aa9e120)
** Erase   : 'arith.constant'(0x55db9aa9e080)
** Replace : 'arith.constant'(0x55db9aa9eb00)
** Modified: 'tensor.dim'(0x55db9aa9eba0)
** Erase   : 'arith.constant'(0x55db9aa9eb00)
** Replace : 'arith.constant'(0x55db9aa9ec80)
** Modified: 'tensor.dim'(0x55db9aa9ed20)
** Erase   : 'arith.constant'(0x55db9aa9ec80)
** Replace : 'arith.constant'(0x55db9aa9ee00)
** Modified: 'tensor.dim'(0x55db9aa9eea0)
** Erase   : 'arith.constant'(0x55db9aa9ee00)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%41) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%41) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%42) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc5c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x55db9aa9f910) {
  %42 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %40 = "tensor.empty"(%6, %9, %12, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9f490) {
  "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9f370) {
  "scf.yield"(%47) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9f260) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9f1a0) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9ef80) {
  %46 = "tensor.empty"(%43, %44, %45, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9eea0) {
  %45 = "tensor.dim"(%36, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ed20) {
  %44 = "tensor.dim"(%36, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9f550) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9eba0) {
  %43 = "tensor.dim"(%36, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9e9a0) {
  %38 = "arith.cmpi"(%37, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9e8c0) {
  %37 = "tensor.dim"(%36, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9e710) {
  "scf.yield"(%33) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9e5f0) {
  "scf.yield"(%52) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9e4e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9e420) {
  "linalg.yield"(%arg6) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9e200) {
  %51 = "tensor.empty"(%48, %49, %12, %50) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9e120) {
  %50 = "tensor.dim"(%33, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9dfa0) {
  %49 = "tensor.dim"(%33, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9e7d0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9de20) {
  %48 = "tensor.dim"(%33, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9dc20) {
  %35 = "arith.cmpi"(%34, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9db40) {
  %34 = "tensor.dim"(%33, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9d990) {
  "scf.yield"(%30) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9d870) {
  "scf.yield"(%57) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9d760) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9d6a0) {
  "linalg.yield"(%arg8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9cd80) {
  %56 = "tensor.empty"(%53, %9, %54, %55) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9cca0) {
  %55 = "tensor.dim"(%30, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9cb20) {
  %54 = "tensor.dim"(%30, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9da50) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9c9a0) {
  %53 = "tensor.dim"(%30, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9c7a0) {
  %32 = "arith.cmpi"(%31, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9c6c0) {
  %31 = "tensor.dim"(%30, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9c510) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9c3f0) {
  "scf.yield"(%62) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9c2e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9c220) {
  "linalg.yield"(%arg10) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9c000) {
  %61 = "tensor.empty"(%6, %58, %59, %60) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9bf20) {
  %60 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9bda0) {
  %59 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9c5d0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9bc20) {
  %58 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9ba20) {
  %29 = "arith.cmpi"(%28, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9b940) {
  %28 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%24) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%67) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9b560) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a8e7330) {
  %66 = "tensor.empty"(%63, %64, %65, %15) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %65 = "tensor.dim"(%24, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %64 = "tensor.dim"(%24, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9b850) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %63 = "tensor.dim"(%24, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %26 = "arith.cmpi"(%25, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9a420) {
  %25 = "tensor.dim"(%24, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%21) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%72) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc4d0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a910790) {
  %71 = "tensor.empty"(%68, %69, %12, %70) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98960) {
  %70 = "tensor.dim"(%21, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa987e0) {
  %69 = "tensor.dim"(%21, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9a2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98660) {
  %68 = "tensor.dim"(%21, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa98050) {
  %23 = "arith.cmpi"(%22, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97f70) {
  %22 = "tensor.dim"(%21, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%18) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%77) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a912210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a9123e0) {
  %76 = "tensor.empty"(%73, %9, %74, %75) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa974c0) {
  %75 = "tensor.dim"(%18, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97340) {
  %74 = "tensor.dim"(%18, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa97e10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa971c0) {
  %73 = "tensor.dim"(%18, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %20 = "arith.cmpi"(%19, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %19 = "tensor.dim"(%18, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%82) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa73210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg18) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa69a50) {
  %81 = "tensor.empty"(%6, %78, %79, %80) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa94390) {
  %80 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa94210) {
  %79 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa734e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa94090) {
  %78 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %17 = "arith.cmpi"(%16, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa93550) {
  %16 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxui'(0x55db9aa8e7b0) {
  %15 = "arith.maxui"(%13, %14) : (index, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8e6d0) {
  %14 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %13 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8e550) {
  %0 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxui'(0x55db9aa8dff0) {
  %12 = "arith.maxui"(%10, %11) : (index, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8df10) {
  %11 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8de30) {
  %10 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8dd90) {
  %1 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxui'(0x55db9aa8dcb0) {
  %9 = "arith.maxui"(%7, %8) : (index, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8db60) {
  %8 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8da80) {
  %7 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8da10) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxui'(0x55db9a91ecb0) {
  %6 = "arith.maxui"(%4, %5) : (index, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9a91ec00) {
  %5 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9a927bc0) {
  %4 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8c790) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After FoldTensorSubsetOps (fold-tensor-subset-ops) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %0 = arith.maxui %dim, %dim_0 : index
    %dim_1 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %dim_2 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %1 = arith.maxui %dim_1, %dim_2 : index
    %dim_3 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %dim_4 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %2 = arith.maxui %dim_3, %dim_4 : index
    %dim_5 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %dim_6 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %3 = arith.maxui %dim_5, %dim_6 : index
    %dim_7 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %4 = arith.cmpi eq, %dim_7, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %dim_15 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
      %dim_16 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
      %dim_17 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%0, %dim_15, %dim_16, %dim_17) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_8 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_8, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %dim_15 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
      %dim_16 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
      %dim_17 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_15, %1, %dim_16, %dim_17) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %dim_9 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
    %8 = arith.cmpi eq, %dim_9, %c1 : index
    %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
      %dim_15 = tensor.dim %7, %c0 : tensor<?x?x?x?xf32>
      %dim_16 = tensor.dim %7, %c1 : tensor<?x?x?x?xf32>
      %dim_17 = tensor.dim %7, %c3 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_15, %dim_16, %2, %dim_17) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %7 : tensor<?x?x?x?xf32>
    }
    %dim_10 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
    %10 = arith.cmpi eq, %dim_10, %c1 : index
    %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
      %dim_15 = tensor.dim %9, %c0 : tensor<?x?x?x?xf32>
      %dim_16 = tensor.dim %9, %c1 : tensor<?x?x?x?xf32>
      %dim_17 = tensor.dim %9, %c2 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_15, %dim_16, %dim_17, %3) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %9 : tensor<?x?x?x?xf32>
    }
    %dim_11 = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %12 = arith.cmpi eq, %dim_11, %c1 : index
    %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
      %dim_15 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
      %dim_16 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
      %dim_17 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%0, %dim_15, %dim_16, %dim_17) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_12 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
    %14 = arith.cmpi eq, %dim_12, %c1 : index
    %15 = scf.if %14 -> (tensor<?x?x?x?xf32>) {
      %dim_15 = tensor.dim %13, %c0 : tensor<?x?x?x?xf32>
      %dim_16 = tensor.dim %13, %c2 : tensor<?x?x?x?xf32>
      %dim_17 = tensor.dim %13, %c3 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_15, %1, %dim_16, %dim_17) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %13 : tensor<?x?x?x?xf32>
    }
    %dim_13 = tensor.dim %15, %c2 : tensor<?x?x?x?xf32>
    %16 = arith.cmpi eq, %dim_13, %c1 : index
    %17 = scf.if %16 -> (tensor<?x?x?x?xf32>) {
      %dim_15 = tensor.dim %15, %c0 : tensor<?x?x?x?xf32>
      %dim_16 = tensor.dim %15, %c1 : tensor<?x?x?x?xf32>
      %dim_17 = tensor.dim %15, %c3 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_15, %dim_16, %2, %dim_17) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %15 : tensor<?x?x?x?xf32>
    }
    %dim_14 = tensor.dim %17, %c3 : tensor<?x?x?x?xf32>
    %18 = arith.cmpi eq, %dim_14, %c1 : index
    %19 = scf.if %18 -> (tensor<?x?x?x?xf32>) {
      %dim_15 = tensor.dim %17, %c0 : tensor<?x?x?x?xf32>
      %dim_16 = tensor.dim %17, %c1 : tensor<?x?x?x?xf32>
      %dim_17 = tensor.dim %17, %c2 : tensor<?x?x?x?xf32>
      %22 = tensor.empty(%dim_15, %dim_16, %dim_17, %3) : tensor<?x?x?x?xf32>
      %23 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %23 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %17 : tensor<?x?x?x?xf32>
    }
    %20 = tensor.empty(%0, %1, %2, %3) : tensor<?x?x?x?xf32>
    %21 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11, %19 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%20 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %in_15: f32, %out: f32):
      %22 = arith.addf %in, %in_15 : f32
      linalg.yield %22 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%21) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %21 : tensor<?x?x?x?xf32>
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::DominanceInfo)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PostDominanceInfo)
// -----// IR Dump After CSE (cse) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %0 = arith.maxui %dim, %dim : index
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %1 = arith.maxui %dim_0, %dim_0 : index
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %2 = arith.maxui %dim_1, %dim_1 : index
    %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %3 = arith.maxui %dim_2, %dim_2 : index
    %4 = arith.cmpi eq, %dim, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %14 = tensor.empty(%0, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %15 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_3 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_3, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
      %14 = tensor.empty(%dim_6, %1, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
      %15 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %15 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %dim_4 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
    %8 = arith.cmpi eq, %dim_4, %c1 : index
    %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %7, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %7, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %7, %c3 : tensor<?x?x?x?xf32>
      %14 = tensor.empty(%dim_6, %dim_7, %2, %dim_8) : tensor<?x?x?x?xf32>
      %15 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %15 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %7 : tensor<?x?x?x?xf32>
    }
    %dim_5 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
    %10 = arith.cmpi eq, %dim_5, %c1 : index
    %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %9, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %9, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %9, %c2 : tensor<?x?x?x?xf32>
      %14 = tensor.empty(%dim_6, %dim_7, %dim_8, %3) : tensor<?x?x?x?xf32>
      %15 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %15 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %9 : tensor<?x?x?x?xf32>
    }
    %12 = tensor.empty(%0, %1, %2, %3) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11, %11 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %14 = arith.addf %in, %in_6 : f32
      linalg.yield %14 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%13) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %13 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x55db9a9287a0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8e550) {
  %0 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8dd90) {
  %1 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8da10) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8c790) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9a927bc0) {
  %4 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9a91ecb0) {
  %5 = "arith.maxui"(%4, %4) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8da80) {
  %6 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8dcb0) {
  %7 = "arith.maxui"(%6, %6) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8de30) {
  %8 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8dff0) {
  %9 = "arith.maxui"(%8, %8) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %10 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxui'(0x55db9aa8e7b0) {
  %11 = "arith.maxui"(%10, %10) : (index, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %12 = "arith.cmpi"(%4, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa734e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa69a50) {
  %41 = "tensor.empty"(%5, %6, %8, %10) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa73210) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg10) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%42) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %14 = "tensor.dim"(%13, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %15 = "arith.cmpi"(%14, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa97e10) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa971c0) {
  %36 = "tensor.dim"(%13, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa97340) {
  %37 = "tensor.dim"(%13, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa974c0) {
  %38 = "tensor.dim"(%13, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a9123e0) {
  %39 = "tensor.empty"(%36, %7, %37, %38) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a912210) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg8) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%40) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%13) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa97f70) {
  %17 = "tensor.dim"(%16, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa98050) {
  %18 = "arith.cmpi"(%17, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9a2c0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa98660) {
  %31 = "tensor.dim"(%16, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa987e0) {
  %32 = "tensor.dim"(%16, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa98960) {
  %33 = "tensor.dim"(%16, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a910790) {
  %34 = "tensor.empty"(%31, %32, %9, %33) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a8fc4d0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg6) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%35) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%16) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9a420) {
  %20 = "tensor.dim"(%19, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %21 = "arith.cmpi"(%20, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9b850) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %26 = "tensor.dim"(%19, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %27 = "tensor.dim"(%19, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %28 = "tensor.dim"(%19, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a8e7330) {
  %29 = "tensor.empty"(%26, %27, %28, %11) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9b560) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%30) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%19) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9f640) {
  %23 = "tensor.empty"(%5, %7, %9, %11) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a8fc5c0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x55db9aa9f910) {
  %25 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%25) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%24) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%24) : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
// -----// IR Dump After ConvertTensorToLinalg (convert-tensor-to-linalg) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %0 = arith.maxui %dim, %dim : index
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %1 = arith.maxui %dim_0, %dim_0 : index
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %2 = arith.maxui %dim_1, %dim_1 : index
    %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %3 = arith.maxui %dim_2, %dim_2 : index
    %4 = arith.cmpi eq, %dim, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %14 = tensor.empty(%0, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %15 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_3 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_3, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
      %14 = tensor.empty(%dim_6, %1, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
      %15 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %15 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %dim_4 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
    %8 = arith.cmpi eq, %dim_4, %c1 : index
    %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %7, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %7, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %7, %c3 : tensor<?x?x?x?xf32>
      %14 = tensor.empty(%dim_6, %dim_7, %2, %dim_8) : tensor<?x?x?x?xf32>
      %15 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %15 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %7 : tensor<?x?x?x?xf32>
    }
    %dim_5 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
    %10 = arith.cmpi eq, %dim_5, %c1 : index
    %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %9, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %9, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %9, %c2 : tensor<?x?x?x?xf32>
      %14 = tensor.empty(%dim_6, %dim_7, %dim_8, %3) : tensor<?x?x?x?xf32>
      %15 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %15 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %9 : tensor<?x?x?x?xf32>
    }
    %12 = tensor.empty(%0, %1, %2, %3) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11, %11 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %14 = arith.addf %in, %in_6 : f32
      linalg.yield %14 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%13) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %13 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%24) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%24) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%25) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc5c0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x55db9aa9f910) {
  %25 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %23 = "tensor.empty"(%5, %7, %9, %11) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%19) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%30) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9b560) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a8e7330) {
  %29 = "tensor.empty"(%26, %27, %28, %11) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %28 = "tensor.dim"(%19, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %27 = "tensor.dim"(%19, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9b850) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %26 = "tensor.dim"(%19, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %21 = "arith.cmpi"(%20, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9a420) {
  %20 = "tensor.dim"(%19, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%16) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%35) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc4d0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg6) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a910790) {
  %34 = "tensor.empty"(%31, %32, %9, %33) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98960) {
  %33 = "tensor.dim"(%16, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa987e0) {
  %32 = "tensor.dim"(%16, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9a2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98660) {
  %31 = "tensor.dim"(%16, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa98050) {
  %18 = "arith.cmpi"(%17, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97f70) {
  %17 = "tensor.dim"(%16, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%13) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%40) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a912210) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a9123e0) {
  %39 = "tensor.empty"(%36, %7, %37, %38) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa974c0) {
  %38 = "tensor.dim"(%13, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97340) {
  %37 = "tensor.dim"(%13, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa97e10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa971c0) {
  %36 = "tensor.dim"(%13, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %15 = "arith.cmpi"(%14, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %14 = "tensor.dim"(%13, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%42) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa73210) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg10) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa734e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa69a50) {
  %41 = "tensor.empty"(%5, %6, %8, %10) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %12 = "arith.cmpi"(%4, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxui'(0x55db9aa8e7b0) {
  %11 = "arith.maxui"(%10, %10) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'arith.maxui'(0x55db9aa8e7b0)
** Modified: 'tensor.empty'(0x55db9aa9f640)
** Modified: 'tensor.empty'(0x55db9a8e7330)
** Erase   : 'arith.maxui'(0x55db9aa8e7b0)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %0 = arith.maxui %dim, %dim : index
  %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %1 = arith.maxui %dim_0, %dim_0 : index
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %2 = arith.maxui %dim_1, %dim_1 : index
  %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = scf.if %3 -> (tensor<?x?x?x?xf32>) {
    %13 = tensor.empty(%0, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %14 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %4, %c1 : tensor<?x?x?x?xf32>
  %5 = arith.cmpi eq, %dim_3, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %4, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %4, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %4, %c3 : tensor<?x?x?x?xf32>
    %13 = tensor.empty(%dim_6, %1, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%4 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %14 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %4 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %6, %c2 : tensor<?x?x?x?xf32>
  %7 = arith.cmpi eq, %dim_4, %c1 : index
  %8 = scf.if %7 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %6, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %6, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %6, %c3 : tensor<?x?x?x?xf32>
    %13 = tensor.empty(%dim_6, %dim_7, %2, %dim_8) : tensor<?x?x?x?xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %14 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %8, %c3 : tensor<?x?x?x?xf32>
  %9 = arith.cmpi eq, %dim_5, %c1 : index
  %10 = scf.if %9 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %8, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %8, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %8, %c2 : tensor<?x?x?x?xf32>
    %13 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %14 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %8 : tensor<?x?x?x?xf32>
  }
  %11 = tensor.empty(%0, %1, %2, %dim_2) : tensor<?x?x?x?xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10, %10 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %13 = arith.addf %in, %in_6 : f32
    linalg.yield %13 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%12) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %12 : tensor<?x?x?x?xf32>
}



//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9b850) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a8e7330) {
  %28 = "tensor.empty"(%25, %26, %27, %10) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %22 = "tensor.empty"(%5, %7, %9, %10) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %10 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxui'(0x55db9aa8dff0) {
  %9 = "arith.maxui"(%8, %8) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'arith.maxui'(0x55db9aa8dff0)
** Modified: 'tensor.empty'(0x55db9aa9f640)
** Modified: 'tensor.empty'(0x55db9a910790)
** Erase   : 'arith.maxui'(0x55db9aa8dff0)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %0 = arith.maxui %dim, %dim : index
  %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %1 = arith.maxui %dim_0, %dim_0 : index
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %2 = arith.cmpi eq, %dim, %c1 : index
  %3 = scf.if %2 -> (tensor<?x?x?x?xf32>) {
    %12 = tensor.empty(%0, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %13 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>
  %4 = arith.cmpi eq, %dim_3, %c1 : index
  %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>
    %12 = tensor.empty(%dim_6, %1, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %13 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %3 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
  %6 = arith.cmpi eq, %dim_4, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
    %12 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %13 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %5 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %7, %c3 : tensor<?x?x?x?xf32>
  %8 = arith.cmpi eq, %dim_5, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %7, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %7, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
    %12 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %13 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %10 = tensor.empty(%0, %1, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9, %9 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %12 = arith.addf %in, %in_6 : f32
    linalg.yield %12 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%11) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %11 : tensor<?x?x?x?xf32>
}



//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9a2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a910790) {
  %32 = "tensor.empty"(%29, %30, %8, %31) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %21 = "tensor.empty"(%5, %7, %8, %9) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8de30) {
  %8 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxui'(0x55db9aa8dcb0) {
  %7 = "arith.maxui"(%6, %6) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'arith.maxui'(0x55db9aa8dcb0)
** Modified: 'tensor.empty'(0x55db9aa9f640)
** Modified: 'tensor.empty'(0x55db9a9123e0)
** Erase   : 'arith.maxui'(0x55db9aa8dcb0)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %0 = arith.maxui %dim, %dim : index
  %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %1 = arith.cmpi eq, %dim, %c1 : index
  %2 = scf.if %1 -> (tensor<?x?x?x?xf32>) {
    %11 = tensor.empty(%0, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %12 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %2, %c1 : tensor<?x?x?x?xf32>
  %3 = arith.cmpi eq, %dim_3, %c1 : index
  %4 = scf.if %3 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %2, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %2, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %2, %c3 : tensor<?x?x?x?xf32>
    %11 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%2 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %12 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %2 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %4, %c2 : tensor<?x?x?x?xf32>
  %5 = arith.cmpi eq, %dim_4, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %4, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %4, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %4, %c3 : tensor<?x?x?x?xf32>
    %11 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%4 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %12 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %4 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %6, %c3 : tensor<?x?x?x?xf32>
  %7 = arith.cmpi eq, %dim_5, %c1 : index
  %8 = scf.if %7 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %6, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %6, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %6, %c2 : tensor<?x?x?x?xf32>
    %11 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %12 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %9 = tensor.empty(%0, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8, %8 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%9 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %11 = arith.addf %in, %in_6 : f32
    linalg.yield %11 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%10) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %10 : tensor<?x?x?x?xf32>
}



//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa97e10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a9123e0) {
  %36 = "tensor.empty"(%33, %6, %34, %35) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %20 = "tensor.empty"(%5, %6, %7, %8) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8da80) {
  %6 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxui'(0x55db9a91ecb0) {
  %5 = "arith.maxui"(%4, %4) : (index, index) -> index

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'arith.maxui'(0x55db9a91ecb0)
** Modified: 'tensor.empty'(0x55db9aa9f640)
** Modified: 'tensor.empty'(0x55db9aa69a50)
** Erase   : 'arith.maxui'(0x55db9a91ecb0)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %0 = arith.cmpi eq, %dim, %c1 : index
  %1 = scf.if %0 -> (tensor<?x?x?x?xf32>) {
    %10 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %11 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %1, %c1 : tensor<?x?x?x?xf32>
  %2 = arith.cmpi eq, %dim_3, %c1 : index
  %3 = scf.if %2 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %1, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %1, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %1, %c3 : tensor<?x?x?x?xf32>
    %10 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %11 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %1 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>
  %4 = arith.cmpi eq, %dim_4, %c1 : index
  %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>
    %10 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %11 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %3 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
  %6 = arith.cmpi eq, %dim_5, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
    %10 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %11 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %5 : tensor<?x?x?x?xf32>
  }
  %8 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7, %7 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %10 = arith.addf %in, %in_6 : f32
    linalg.yield %10 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %9 : tensor<?x?x?x?xf32>
}



//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa734e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa69a50) {
  %37 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %19 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9a927bc0) {
  %4 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8c790) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8da10) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8dd90) {
  %1 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8e550) {
  %0 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%20) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%20) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%21) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc5c0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x55db9aa9f910) {
  %21 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %19 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%15) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%26) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9b560) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a8e7330) {
  %25 = "tensor.empty"(%22, %23, %24, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %24 = "tensor.dim"(%15, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %23 = "tensor.dim"(%15, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9b850) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %22 = "tensor.dim"(%15, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %17 = "arith.cmpi"(%16, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9a420) {
  %16 = "tensor.dim"(%15, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%12) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%31) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc4d0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg6) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a910790) {
  %30 = "tensor.empty"(%27, %28, %6, %29) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98960) {
  %29 = "tensor.dim"(%12, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa987e0) {
  %28 = "tensor.dim"(%12, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9a2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98660) {
  %27 = "tensor.dim"(%12, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa98050) {
  %14 = "arith.cmpi"(%13, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97f70) {
  %13 = "tensor.dim"(%12, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%9) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a912210) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a9123e0) {
  %35 = "tensor.empty"(%32, %5, %33, %34) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa974c0) {
  %34 = "tensor.dim"(%9, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97340) {
  %33 = "tensor.dim"(%9, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa97e10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa971c0) {
  %32 = "tensor.dim"(%9, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %11 = "arith.cmpi"(%10, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %10 = "tensor.dim"(%9, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%38) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa73210) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg10) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa734e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa69a50) {
  %37 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %8 = "arith.cmpi"(%4, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %7 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8de30) {
  %6 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8da80) {
  %5 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9a927bc0) {
  %4 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8c790) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8da10) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8dd90) {
  %1 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8e550) {
  %0 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After LinalgGeneralizeNamedOpsPass (linalg-generalize-named-ops) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %0 = arith.cmpi eq, %dim, %c1 : index
    %1 = scf.if %0 -> (tensor<?x?x?x?xf32>) {
      %10 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_3 = tensor.dim %1, %c1 : tensor<?x?x?x?xf32>
    %2 = arith.cmpi eq, %dim_3, %c1 : index
    %3 = scf.if %2 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %1, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %1, %c2 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %1, %c3 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %1 : tensor<?x?x?x?xf32>
    }
    %dim_4 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>
    %4 = arith.cmpi eq, %dim_4, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %3 : tensor<?x?x?x?xf32>
    }
    %dim_5 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_5, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %8 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %9 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7, %7 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %10 = arith.addf %in, %in_6 : f32
      linalg.yield %10 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %9 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x55db9a9287a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8e550) {
  %0 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8dd90) {
  %1 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8da10) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8c790) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9a927bc0) {
  %4 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8da80) {
  %5 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8de30) {
  %6 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %7 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %8 = "arith.cmpi"(%4, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa734e0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa69a50) {
  %37 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa73210) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg10) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%38) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %10 = "tensor.dim"(%9, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %11 = "arith.cmpi"(%10, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa97e10) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa971c0) {
  %32 = "tensor.dim"(%9, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa97340) {
  %33 = "tensor.dim"(%9, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa974c0) {
  %34 = "tensor.dim"(%9, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a9123e0) {
  %35 = "tensor.empty"(%32, %5, %33, %34) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a912210) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg8) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%9) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa97f70) {
  %13 = "tensor.dim"(%12, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa98050) {
  %14 = "arith.cmpi"(%13, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9a2c0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa98660) {
  %27 = "tensor.dim"(%12, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa987e0) {
  %28 = "tensor.dim"(%12, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa98960) {
  %29 = "tensor.dim"(%12, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a910790) {
  %30 = "tensor.empty"(%27, %28, %6, %29) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a8fc4d0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg6) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%31) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%12) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9a420) {
  %16 = "tensor.dim"(%15, %0) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %17 = "arith.cmpi"(%16, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9b850) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %22 = "tensor.dim"(%15, %3) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %23 = "tensor.dim"(%15, %2) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %24 = "tensor.dim"(%15, %1) : (tensor<?x?x?x?xf32>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9a8e7330) {
  %25 = "tensor.empty"(%22, %23, %24, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9b560) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%26) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%15) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x55db9aa9f640) {
  %19 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a8fc5c0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x55db9aa9f910) {
  %21 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%21) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%20) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%20) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ConvertElementwiseToLinalgPass (convert-elementwise-to-linalg) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %0 = arith.cmpi eq, %dim, %c1 : index
    %1 = scf.if %0 -> (tensor<?x?x?x?xf32>) {
      %10 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_3 = tensor.dim %1, %c1 : tensor<?x?x?x?xf32>
    %2 = arith.cmpi eq, %dim_3, %c1 : index
    %3 = scf.if %2 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %1, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %1, %c2 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %1, %c3 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %1 : tensor<?x?x?x?xf32>
    }
    %dim_4 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>
    %4 = arith.cmpi eq, %dim_4, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %3 : tensor<?x?x?x?xf32>
    }
    %dim_5 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_5, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %8 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %9 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7, %7 : tensor<?x?x?x?xf32>, tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %10 = arith.addf %in, %in_6 : f32
      linalg.yield %10 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %9 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8e550) {
  %0 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8dd90) {
  %1 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8da10) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8c790) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9a927bc0) {
  %4 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8da80) {
  %5 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8de30) {
  %6 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %7 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %8 = "arith.cmpi"(%4, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa734e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa69a50) {
  %37 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa73210) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg10) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%38) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %10 = "tensor.dim"(%9, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %11 = "arith.cmpi"(%10, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa97e10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa971c0) {
  %32 = "tensor.dim"(%9, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97340) {
  %33 = "tensor.dim"(%9, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa974c0) {
  %34 = "tensor.dim"(%9, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a9123e0) {
  %35 = "tensor.empty"(%32, %5, %33, %34) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a912210) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%9) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97f70) {
  %13 = "tensor.dim"(%12, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa98050) {
  %14 = "arith.cmpi"(%13, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9a2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98660) {
  %27 = "tensor.dim"(%12, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa987e0) {
  %28 = "tensor.dim"(%12, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98960) {
  %29 = "tensor.dim"(%12, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a910790) {
  %30 = "tensor.empty"(%27, %28, %6, %29) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc4d0) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg6) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%31) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%12) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9a420) {
  %16 = "tensor.dim"(%15, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %17 = "arith.cmpi"(%16, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9b850) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %22 = "tensor.dim"(%15, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %23 = "tensor.dim"(%15, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %24 = "tensor.dim"(%15, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a8e7330) {
  %25 = "tensor.empty"(%22, %23, %24, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9b560) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%26) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%15) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %19 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc5c0) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
    ** Insert  : 'linalg.generic'(0x55db9aa9cd80)
    ** Modified: 'arith.addf'(0x55db9aa9f910)
    ** Modified: 'arith.addf'(0x55db9aa9f910)
    ** Insert  : 'arith.addf'(0x55db9aa9f910)
    ** Insert  : 'linalg.yield'(0x55db9aaa02d0)
    ** Replace : 'linalg.generic'(0x55db9a8fc5c0)
    ** Modified: 'llh.encoding_bind'(0x55db9aa6ddd0)
    ** Modified: 'func.return'(0x55db9a928210)
    ** Erase   : 'linalg.generic'(0x55db9a8fc5c0)
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
  %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %0 = arith.cmpi eq, %dim, %c1 : index
  %1 = scf.if %0 -> (tensor<?x?x?x?xf32>) {
    %10 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %11 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %1, %c1 : tensor<?x?x?x?xf32>
  %2 = arith.cmpi eq, %dim_3, %c1 : index
  %3 = scf.if %2 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %1, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %1, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %1, %c3 : tensor<?x?x?x?xf32>
    %10 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %11 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %1 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>
  %4 = arith.cmpi eq, %dim_4, %c1 : index
  %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>
    %10 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %11 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %3 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
  %6 = arith.cmpi eq, %dim_5, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
    %10 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %11 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %5 : tensor<?x?x?x?xf32>
  }
  %8 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.addf %in, %in : f32
    linalg.yield %10 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %9 : tensor<?x?x?x?xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %19 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9b850) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9cd80) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x55db9aa9f910) {
  %21 = "arith.addf"(%arg1, %arg1) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%21) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%20) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%20) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8e550) {
  %0 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8dd90) {
  %1 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8da10) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8c790) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9a927bc0) {
  %4 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8da80) {
  %5 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8de30) {
  %6 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %7 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %8 = "arith.cmpi"(%4, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa734e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa69a50) {
  %37 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa73210) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%38) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %10 = "tensor.dim"(%9, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %11 = "arith.cmpi"(%10, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa97e10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa971c0) {
  %32 = "tensor.dim"(%9, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97340) {
  %33 = "tensor.dim"(%9, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa974c0) {
  %34 = "tensor.dim"(%9, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a9123e0) {
  %35 = "tensor.empty"(%32, %5, %33, %34) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a912210) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg7) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%9) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97f70) {
  %13 = "tensor.dim"(%12, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa98050) {
  %14 = "arith.cmpi"(%13, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9a2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98660) {
  %27 = "tensor.dim"(%12, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa987e0) {
  %28 = "tensor.dim"(%12, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98960) {
  %29 = "tensor.dim"(%12, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a910790) {
  %30 = "tensor.empty"(%27, %28, %6, %29) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc4d0) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%31) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%12) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9a420) {
  %16 = "tensor.dim"(%15, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %17 = "arith.cmpi"(%16, %2) <{predicate = 0 : i64}> : (index, index) -> i1

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9b850) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %22 = "tensor.dim"(%15, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %23 = "tensor.dim"(%15, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %24 = "tensor.dim"(%15, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a8e7330) {
  %25 = "tensor.empty"(%22, %23, %24, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9b560) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg3) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%26) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%15) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %19 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9cd80) {

  * Pattern {anonymous}::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FuseElementwiseOps"
"{anonymous}::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldFillWithGenericOp"
"{anonymous}::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldScalarOrSplatConstant"
"{anonymous}::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveOutsDependency"
"{anonymous}::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults"
"{anonymous}::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::RemoveUnusedCycleInGenericOp"
"{anonymous}::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldWithProducerReshapeOpByExpansion"
"{anonymous}::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "{anonymous}::FoldConstantTranspose"
"{anonymous}::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x55db9aa9f910) {
  %21 = "arith.addf"(%arg1, %arg1) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%21) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%20) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%20) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After LinalgElementwiseOpFusionPass (linalg-fuse-elementwise-ops) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %0 = arith.cmpi eq, %dim, %c1 : index
    %1 = scf.if %0 -> (tensor<?x?x?x?xf32>) {
      %10 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_3 = tensor.dim %1, %c1 : tensor<?x?x?x?xf32>
    %2 = arith.cmpi eq, %dim_3, %c1 : index
    %3 = scf.if %2 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %1, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %1, %c2 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %1, %c3 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %1 : tensor<?x?x?x?xf32>
    }
    %dim_4 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>
    %4 = arith.cmpi eq, %dim_4, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %3 : tensor<?x?x?x?xf32>
    }
    %dim_5 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_5, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %8 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %9 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      %10 = arith.addf %in, %in : f32
      linalg.yield %10 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %9 : tensor<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x55db9a928290) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8e550) {
  %0 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8dd90) {
  %1 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8da10) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x55db9aa8c790) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9a927bc0) {
  %4 = "tensor.dim"(%arg0, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8da80) {
  %5 = "tensor.dim"(%arg0, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8de30) {
  %6 = "tensor.dim"(%arg0, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa8e5f0) {
  %7 = "tensor.dim"(%arg0, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %8 = "arith.cmpi"(%4, %2) <{predicate = 0 : i64}> : (index, index) -> i1


  * Pattern {anonymous}::CmpIExtSI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "{anonymous}::CmpIExtSI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtSIOp type
"{anonymous}::CmpIExtSI" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::CmpIExtUI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "{anonymous}::CmpIExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"{anonymous}::CmpIExtUI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa734e0) {

  * Pattern {anonymous}::CombineIfs : 'scf.if -> ()' {
Trying to match "{anonymous}::CombineIfs"
"{anonymous}::CombineIfs" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::CombineNestedIfs : 'scf.if -> ()' {
Trying to match "{anonymous}::CombineNestedIfs"
"{anonymous}::CombineNestedIfs" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ConditionPropagation : 'scf.if -> ()' {
Trying to match "{anonymous}::ConditionPropagation"
"{anonymous}::ConditionPropagation" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ConvertTrivialIfToSelect : 'scf.if -> ()' {
Trying to match "{anonymous}::ConvertTrivialIfToSelect"
"{anonymous}::ConvertTrivialIfToSelect" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveEmptyElseBranch : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveEmptyElseBranch"
"{anonymous}::RemoveEmptyElseBranch" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveStaticCondition : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveStaticCondition"
"{anonymous}::RemoveStaticCondition" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedResults : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveUnusedResults"
"{anonymous}::RemoveUnusedResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ReplaceIfYieldWithConditionOrValue : 'scf.if -> ()' {
Trying to match "{anonymous}::ReplaceIfYieldWithConditionOrValue"
"{anonymous}::ReplaceIfYieldWithConditionOrValue" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa69a50) {
  %37 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa73210) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%38) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%arg0) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa96ad0) {
  %10 = "tensor.dim"(%9, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %11 = "arith.cmpi"(%10, %2) <{predicate = 0 : i64}> : (index, index) -> i1


  * Pattern {anonymous}::CmpIExtSI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "{anonymous}::CmpIExtSI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtSIOp type
"{anonymous}::CmpIExtSI" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::CmpIExtUI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "{anonymous}::CmpIExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"{anonymous}::CmpIExtUI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa97e10) {

  * Pattern {anonymous}::CombineIfs : 'scf.if -> ()' {
Trying to match "{anonymous}::CombineIfs"
"{anonymous}::CombineIfs" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::CombineNestedIfs : 'scf.if -> ()' {
Trying to match "{anonymous}::CombineNestedIfs"
"{anonymous}::CombineNestedIfs" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ConditionPropagation : 'scf.if -> ()' {
Trying to match "{anonymous}::ConditionPropagation"
"{anonymous}::ConditionPropagation" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ConvertTrivialIfToSelect : 'scf.if -> ()' {
Trying to match "{anonymous}::ConvertTrivialIfToSelect"
"{anonymous}::ConvertTrivialIfToSelect" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveEmptyElseBranch : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveEmptyElseBranch"
"{anonymous}::RemoveEmptyElseBranch" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveStaticCondition : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveStaticCondition"
"{anonymous}::RemoveStaticCondition" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedResults : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveUnusedResults"
"{anonymous}::RemoveUnusedResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ReplaceIfYieldWithConditionOrValue : 'scf.if -> ()' {
Trying to match "{anonymous}::ReplaceIfYieldWithConditionOrValue"
"{anonymous}::ReplaceIfYieldWithConditionOrValue" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa971c0) {
  %32 = "tensor.dim"(%9, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97340) {
  %33 = "tensor.dim"(%9, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa974c0) {
  %34 = "tensor.dim"(%9, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a9123e0) {
  %35 = "tensor.empty"(%32, %5, %33, %34) : (index, index, index, index) -> tensor<?x?x?x?xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a912210) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg7) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%36) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%9) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa97f70) {
  %13 = "tensor.dim"(%12, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa98050) {
  %14 = "arith.cmpi"(%13, %2) <{predicate = 0 : i64}> : (index, index) -> i1


  * Pattern {anonymous}::CmpIExtSI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "{anonymous}::CmpIExtSI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtSIOp type
"{anonymous}::CmpIExtSI" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::CmpIExtUI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "{anonymous}::CmpIExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"{anonymous}::CmpIExtUI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9a2c0) {

  * Pattern {anonymous}::CombineIfs : 'scf.if -> ()' {
Trying to match "{anonymous}::CombineIfs"
"{anonymous}::CombineIfs" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::CombineNestedIfs : 'scf.if -> ()' {
Trying to match "{anonymous}::CombineNestedIfs"
"{anonymous}::CombineNestedIfs" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ConditionPropagation : 'scf.if -> ()' {
Trying to match "{anonymous}::ConditionPropagation"
"{anonymous}::ConditionPropagation" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ConvertTrivialIfToSelect : 'scf.if -> ()' {
Trying to match "{anonymous}::ConvertTrivialIfToSelect"
"{anonymous}::ConvertTrivialIfToSelect" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveEmptyElseBranch : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveEmptyElseBranch"
"{anonymous}::RemoveEmptyElseBranch" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveStaticCondition : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveStaticCondition"
"{anonymous}::RemoveStaticCondition" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedResults : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveUnusedResults"
"{anonymous}::RemoveUnusedResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ReplaceIfYieldWithConditionOrValue : 'scf.if -> ()' {
Trying to match "{anonymous}::ReplaceIfYieldWithConditionOrValue"
"{anonymous}::ReplaceIfYieldWithConditionOrValue" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98660) {
  %27 = "tensor.dim"(%12, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa987e0) {
  %28 = "tensor.dim"(%12, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa98960) {
  %29 = "tensor.dim"(%12, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a910790) {
  %30 = "tensor.empty"(%27, %28, %6, %29) : (index, index, index, index) -> tensor<?x?x?x?xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9a8fc4d0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%31) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%12) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9a420) {
  %16 = "tensor.dim"(%15, %0) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %17 = "arith.cmpi"(%16, %2) <{predicate = 0 : i64}> : (index, index) -> i1


  * Pattern {anonymous}::CmpIExtSI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "{anonymous}::CmpIExtSI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtSIOp type
"{anonymous}::CmpIExtSI" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::CmpIExtUI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "{anonymous}::CmpIExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"{anonymous}::CmpIExtUI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.if'(0x55db9aa9b850) {

  * Pattern {anonymous}::CombineIfs : 'scf.if -> ()' {
Trying to match "{anonymous}::CombineIfs"
"{anonymous}::CombineIfs" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::CombineNestedIfs : 'scf.if -> ()' {
Trying to match "{anonymous}::CombineNestedIfs"
"{anonymous}::CombineNestedIfs" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ConditionPropagation : 'scf.if -> ()' {
Trying to match "{anonymous}::ConditionPropagation"
"{anonymous}::ConditionPropagation" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ConvertTrivialIfToSelect : 'scf.if -> ()' {
Trying to match "{anonymous}::ConvertTrivialIfToSelect"
"{anonymous}::ConvertTrivialIfToSelect" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveEmptyElseBranch : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveEmptyElseBranch"
"{anonymous}::RemoveEmptyElseBranch" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveStaticCondition : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveStaticCondition"
"{anonymous}::RemoveStaticCondition" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::RemoveUnusedResults : 'scf.if -> ()' {
Trying to match "{anonymous}::RemoveUnusedResults"
"{anonymous}::RemoveUnusedResults" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::ReplaceIfYieldWithConditionOrValue : 'scf.if -> ()' {
Trying to match "{anonymous}::ReplaceIfYieldWithConditionOrValue"
"{anonymous}::ReplaceIfYieldWithConditionOrValue" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ab10) {
  %22 = "tensor.dim"(%15, %3) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ac90) {
  %23 = "tensor.dim"(%15, %2) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.dim'(0x55db9aa9ae10) {
  %24 = "tensor.dim"(%15, %1) : (tensor<?x?x?x?xf32>, index) -> index


  * Pattern {anonymous}::DimOfForallOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfForallOp"
"{anonymous}::DimOfForallOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfToTensorFolder : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfToTensorFolder"
"{anonymous}::DimOfToTensorFolder" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfAllocTensorOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfAllocTensorOp"
"{anonymous}::FoldDimOfAllocTensorOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfExpandShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfExpandShape"
"{anonymous}::FoldDimOfExpandShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldDimOfCollapseShape : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldDimOfCollapseShape"
"{anonymous}::FoldDimOfCollapseShape" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::FoldEmptyTensorWithDimOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::FoldEmptyTensorWithDimOp"
"{anonymous}::FoldEmptyTensorWithDimOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfCastOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfCastOp"
"{anonymous}::DimOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfDestStyleOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfDestStyleOp"
"{anonymous}::DimOfDestStyleOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::DimOfReshapeOp : 'tensor.dim -> ()' {
Trying to match "{anonymous}::DimOfReshapeOp"
"{anonymous}::DimOfReshapeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9a8e7330) {
  %25 = "tensor.empty"(%22, %23, %24, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9b560) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg3) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%26) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%15) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x55db9aa9f640) {
  %19 = "tensor.empty"(%4, %5, %6, %7) : (index, index, index, index) -> tensor<?x?x?x?xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x55db9aa9cd80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseDeadLinalgOp"
"{anonymous}::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "{anonymous}::InferStaticShapeOfOperands"
"{anonymous}::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern {anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>"
"{anonymous}::EraseIdentityLinalgOp<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x55db9aa9f910) {
  %21 = "arith.addf"(%arg1, %arg1) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%21) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%20) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%20) : (tensor<?x?x?x?xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
    %0 = arith.cmpi eq, %dim, %c1 : index
    %1 = scf.if %0 -> (tensor<?x?x?x?xf32>) {
      %10 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %arg0 : tensor<?x?x?x?xf32>
    }
    %dim_3 = tensor.dim %1, %c1 : tensor<?x?x?x?xf32>
    %2 = arith.cmpi eq, %dim_3, %c1 : index
    %3 = scf.if %2 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %1, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %1, %c2 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %1, %c3 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %1 : tensor<?x?x?x?xf32>
    }
    %dim_4 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>
    %4 = arith.cmpi eq, %dim_4, %c1 : index
    %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %3 : tensor<?x?x?x?xf32>
    }
    %dim_5 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
    %6 = arith.cmpi eq, %dim_5, %c1 : index
    %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
      %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
      %dim_8 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
      %10 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
      %11 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      } -> tensor<?x?x?x?xf32>
      scf.yield %11 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %5 : tensor<?x?x?x?xf32>
    }
    %8 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %9 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      %10 = arith.addf %in, %in : f32
      linalg.yield %10 : f32
    } -> tensor<?x?x?x?xf32>
    "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    return %9 : tensor<?x?x?x?xf32>
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::func_ext::FuncAnalysisState)

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of func.return %9 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of func.return %9 : tensor<?x?x?x?xf32>
//===-------------------------------------------===//
Analyzing operand #0 of scf.yield %11 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- useDominance = 1
  * definition = %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read happens before write

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are in mutually exclusive regions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of scf.yield %11 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- useDominance = 1
  * definition = %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of scf.yield %1 : tensor<?x?x?x?xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of scf.yield %11 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- useDominance = 1
  * definition = %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of scf.yield %3 : tensor<?x?x?x?xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of scf.yield %11 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %11 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- useDominance = 1
  * definition = %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of scf.yield %5 : tensor<?x?x?x?xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of func.return %9 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of func.return %9 : tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  unConflictingWrite = operand 1 of %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>

- useDominance = 1
  * definition = %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>
    no conflict: definition and write are same

- check conflict:
  uRead = operand 0 of func.return %9 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 1 of %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>

- useDominance = 1
  * definition = %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  unConflictingWrite = operand 1 of %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>

- useDominance = 1
  * definition = %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 0 of "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- useDominance = 1
  no conflict: read and write are same use

- check conflict:
  uRead = operand 0 of func.return %9 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 1 of %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>

- useDominance = 1
  * definition = %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 0 of "llh.encoding_bind"(%9) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- useDominance = 1
  * definition = %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%8 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  %10 = arith.addf %in, %in : f32
  linalg.yield %10 : f32
} -> tensor<?x?x?x?xf32>
  => RaW CONFLICT FOUND
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_8 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_5 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %5 : tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_7 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_4 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_8 = tensor.dim %1, %c3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_7 = tensor.dim %1, %c2 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_6 = tensor.dim %1, %c0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_3 = tensor.dim %1, %c1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read happens before write

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are in mutually exclusive regions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read happens before write

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are in mutually exclusive regions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read happens before write

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are in mutually exclusive regions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf32>

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use

- check conflict:
  uRead = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read happens before write

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are in mutually exclusive regions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- check conflict:
  uRead = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>
  unConflictingWrite = operand 0 of scf.yield %arg0 : tensor<?x?x?x?xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 0 of "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

- useDominance = 1
  * definition = <block argument> of type 'tensor<?x?x?x?xf32>' at index: 0
  => RaW CONFLICT FOUND
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %0, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf32>
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %1 = arith.cmpi eq, %dim, %c1 : index
  %2 = scf.if %1 -> (tensor<?x?x?x?xf32>) {
    %11 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %12 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %2, %c1 : tensor<?x?x?x?xf32>
  %3 = arith.cmpi eq, %dim_3, %c1 : index
  %4 = scf.if %3 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %2, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %2, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %2, %c3 : tensor<?x?x?x?xf32>
    %11 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%2 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %12 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %2 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %4, %c2 : tensor<?x?x?x?xf32>
  %5 = arith.cmpi eq, %dim_4, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %4, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %4, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %4, %c3 : tensor<?x?x?x?xf32>
    %11 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%4 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %12 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %4 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %6, %c3 : tensor<?x?x?x?xf32>
  %7 = arith.cmpi eq, %dim_5, %c1 : index
  %8 = scf.if %7 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %6, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %6, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %6, %c2 : tensor<?x?x?x?xf32>
    %11 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %12 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %9 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : tensor<?x?x?x?xf32>) outs(%9 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %11 = arith.addf %in, %in : f32
    linalg.yield %11 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%10) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %10 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %1, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %0, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf32>
  %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %2 = arith.cmpi eq, %dim, %c1 : index
  %3 = scf.if %2 -> (tensor<?x?x?x?xf32>) {
    %12 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %13 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %3, %c1 : tensor<?x?x?x?xf32>
  %4 = arith.cmpi eq, %dim_3, %c1 : index
  %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %3, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %3, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %3, %c3 : tensor<?x?x?x?xf32>
    %12 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %13 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %3 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
  %6 = arith.cmpi eq, %dim_4, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
    %12 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %13 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %5 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %7, %c3 : tensor<?x?x?x?xf32>
  %8 = arith.cmpi eq, %dim_5, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %7, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %7, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
    %12 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %13 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %10 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%10 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %12 = arith.addf %in, %in : f32
    linalg.yield %12 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%11) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %11 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %2, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %0, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf32>
  %3 = arith.cmpi eq, %dim, %c1 : index
  %4 = scf.if %3 -> (tensor<?x?x?x?xf32>) {
    %13 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %14 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %4, %c1 : tensor<?x?x?x?xf32>
  %5 = arith.cmpi eq, %dim_3, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %4, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %4, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %4, %c3 : tensor<?x?x?x?xf32>
    %13 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%4 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %14 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %4 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %6, %c2 : tensor<?x?x?x?xf32>
  %7 = arith.cmpi eq, %dim_4, %c1 : index
  %8 = scf.if %7 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %6, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %6, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %6, %c3 : tensor<?x?x?x?xf32>
    %13 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %14 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %8, %c3 : tensor<?x?x?x?xf32>
  %9 = arith.cmpi eq, %dim_5, %c1 : index
  %10 = scf.if %9 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %8, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %8, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %8, %c2 : tensor<?x?x?x?xf32>
    %13 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %14 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %8 : tensor<?x?x?x?xf32>
  }
  %11 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10 : tensor<?x?x?x?xf32>) outs(%11 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %13 = arith.addf %in, %in : f32
    linalg.yield %13 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%12) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %12 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %3, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %2, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %1, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %0, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = arith.cmpi eq, %dim, %c1 : index
  %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
    %14 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %15 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
  %6 = arith.cmpi eq, %dim_3, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
    %14 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %15 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %5 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
  %8 = arith.cmpi eq, %dim_4, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %7, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %7, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %7, %c3 : tensor<?x?x?x?xf32>
    %14 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %15 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
  %10 = arith.cmpi eq, %dim_5, %c1 : index
  %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %9, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %9, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %9, %c2 : tensor<?x?x?x?xf32>
    %14 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %15 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %9 : tensor<?x?x?x?xf32>
  }
  %12 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %14 = arith.addf %in, %in : f32
    linalg.yield %14 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%13) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %13 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::detail::AllocTensorOpGenericAdaptorBase::Properties)
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %3, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %2, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %1, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %0, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = arith.cmpi eq, %dim, %c1 : index
  %5 = scf.if %4 -> (tensor<?x?x?x?xf32>) {
    %14 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %15 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %5, %c1 : tensor<?x?x?x?xf32>
  %6 = arith.cmpi eq, %dim_3, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %5, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %5, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %5, %c3 : tensor<?x?x?x?xf32>
    %14 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%5 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %15 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %5 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %7, %c2 : tensor<?x?x?x?xf32>
  %8 = arith.cmpi eq, %dim_4, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %7, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %7, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %7, %c3 : tensor<?x?x?x?xf32>
    %14 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %15 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
  %10 = arith.cmpi eq, %dim_5, %c1 : index
  %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %9, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %9, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %9, %c2 : tensor<?x?x?x?xf32>
    %14 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %15 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %9 : tensor<?x?x?x?xf32>
  }
  %12 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11 : tensor<?x?x?x?xf32>) outs(%12 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %14 = arith.addf %in, %in : f32
    linalg.yield %14 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%13) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %13 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SideEffects::DefaultResource)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Read)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Write)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Allocate)
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %15 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %16 = bufferization.to_memref %15 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%16 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %17 = bufferization.to_tensor %16 : memref<?x?x?x?xf32>
    scf.yield %17 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %dim_3 = tensor.dim %6, %c1 : tensor<?x?x?x?xf32>
  %7 = arith.cmpi eq, %dim_3, %c1 : index
  %8 = scf.if %7 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %6, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %6, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %6, %c3 : tensor<?x?x?x?xf32>
    %15 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%15 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %16 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %8, %c2 : tensor<?x?x?x?xf32>
  %9 = arith.cmpi eq, %dim_4, %c1 : index
  %10 = scf.if %9 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %8, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %8, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %8, %c3 : tensor<?x?x?x?xf32>
    %15 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : tensor<?x?x?x?xf32>) outs(%15 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %16 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %8 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %10, %c3 : tensor<?x?x?x?xf32>
  %11 = arith.cmpi eq, %dim_5, %c1 : index
  %12 = scf.if %11 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %10, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %10, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %10, %c2 : tensor<?x?x?x?xf32>
    %15 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10 : tensor<?x?x?x?xf32>) outs(%15 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %16 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %10 : tensor<?x?x?x?xf32>
  }
  %13 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%12 : tensor<?x?x?x?xf32>) outs(%13 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.addf %in, %in : f32
    linalg.yield %15 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%14) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %14 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %16 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %17 = bufferization.to_memref %16 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%17 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %18 = bufferization.to_tensor %17 : memref<?x?x?x?xf32>
    scf.yield %18 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %7, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = arith.cmpi eq, %dim_3, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %6, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %6, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %6, %c3 : tensor<?x?x?x?xf32>
    %16 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%16 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %17 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %9, %c2 : tensor<?x?x?x?xf32>
  %10 = arith.cmpi eq, %dim_4, %c1 : index
  %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %9, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %9, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %9, %c3 : tensor<?x?x?x?xf32>
    %16 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<?x?x?x?xf32>) outs(%16 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %17 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %9 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %11, %c3 : tensor<?x?x?x?xf32>
  %12 = arith.cmpi eq, %dim_5, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %11, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %11, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %11, %c2 : tensor<?x?x?x?xf32>
    %16 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11 : tensor<?x?x?x?xf32>) outs(%16 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %17 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %11 : tensor<?x?x?x?xf32>
  }
  %14 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%14 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %in : f32
    linalg.yield %16 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%15) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %15 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %17 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %18 = bufferization.to_memref %17 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%18 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %19 = bufferization.to_tensor %18 : memref<?x?x?x?xf32>
    scf.yield %19 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %8, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = arith.cmpi eq, %dim_3, %c1 : index
  %10 = scf.if %9 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %7, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = tensor.dim %6, %c2 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %6, %c3 : tensor<?x?x?x?xf32>
    %17 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%17 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %18 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %10, %c2 : tensor<?x?x?x?xf32>
  %11 = arith.cmpi eq, %dim_4, %c1 : index
  %12 = scf.if %11 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %10, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %10, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %10, %c3 : tensor<?x?x?x?xf32>
    %17 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10 : tensor<?x?x?x?xf32>) outs(%17 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %18 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %10 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %12, %c3 : tensor<?x?x?x?xf32>
  %13 = arith.cmpi eq, %dim_5, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %12, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %12, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %12, %c2 : tensor<?x?x?x?xf32>
    %17 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%12 : tensor<?x?x?x?xf32>) outs(%17 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %18 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %12 : tensor<?x?x?x?xf32>
  }
  %15 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : tensor<?x?x?x?xf32>) outs(%15 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %17 = arith.addf %in, %in : f32
    linalg.yield %17 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%16) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %16 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %18 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %19 = bufferization.to_memref %18 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%19 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<?x?x?x?xf32>
    scf.yield %20 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %9, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = arith.cmpi eq, %dim_3, %c1 : index
  %11 = scf.if %10 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %8, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %7, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = tensor.dim %6, %c3 : tensor<?x?x?x?xf32>
    %18 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%18 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %19 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %11, %c2 : tensor<?x?x?x?xf32>
  %12 = arith.cmpi eq, %dim_4, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %11, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %11, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %11, %c3 : tensor<?x?x?x?xf32>
    %18 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%11 : tensor<?x?x?x?xf32>) outs(%18 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %19 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %11 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %13, %c3 : tensor<?x?x?x?xf32>
  %14 = arith.cmpi eq, %dim_5, %c1 : index
  %15 = scf.if %14 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %13, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %13, %c2 : tensor<?x?x?x?xf32>
    %18 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%18 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %19 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %16 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : tensor<?x?x?x?xf32>) outs(%16 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %18 = arith.addf %in, %in : f32
    linalg.yield %18 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%17) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %17 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %19 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %20 = bufferization.to_memref %19 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%20 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %21 = bufferization.to_tensor %20 : memref<?x?x?x?xf32>
    scf.yield %21 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %10, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = arith.cmpi eq, %dim_3, %c1 : index
  %12 = scf.if %11 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %9, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %8, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %7, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %19 = tensor.empty(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%19 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %20 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %12, %c2 : tensor<?x?x?x?xf32>
  %13 = arith.cmpi eq, %dim_4, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %12, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %12, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %12, %c3 : tensor<?x?x?x?xf32>
    %19 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%12 : tensor<?x?x?x?xf32>) outs(%19 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %20 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %12 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %14, %c3 : tensor<?x?x?x?xf32>
  %15 = arith.cmpi eq, %dim_5, %c1 : index
  %16 = scf.if %15 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %14, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %14, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %14, %c2 : tensor<?x?x?x?xf32>
    %19 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : tensor<?x?x?x?xf32>) outs(%19 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %20 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %14 : tensor<?x?x?x?xf32>
  }
  %17 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%16 : tensor<?x?x?x?xf32>) outs(%17 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %19 = arith.addf %in, %in : f32
    linalg.yield %19 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%18) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %18 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %19 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %20 = bufferization.to_memref %19 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%20 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %21 = bufferization.to_tensor %20 : memref<?x?x?x?xf32>
    scf.yield %21 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %10, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = arith.cmpi eq, %dim_3, %c1 : index
  %12 = scf.if %11 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %9, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %8, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %7, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %19 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%6 : tensor<?x?x?x?xf32>) outs(%19 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %20 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %12, %c2 : tensor<?x?x?x?xf32>
  %13 = arith.cmpi eq, %dim_4, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %12, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %12, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %12, %c3 : tensor<?x?x?x?xf32>
    %19 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%12 : tensor<?x?x?x?xf32>) outs(%19 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %20 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %12 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %14, %c3 : tensor<?x?x?x?xf32>
  %15 = arith.cmpi eq, %dim_5, %c1 : index
  %16 = scf.if %15 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %14, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %14, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %14, %c2 : tensor<?x?x?x?xf32>
    %19 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : tensor<?x?x?x?xf32>) outs(%19 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %20 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %14 : tensor<?x?x?x?xf32>
  }
  %17 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%16 : tensor<?x?x?x?xf32>) outs(%17 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %19 = arith.addf %in, %in : f32
    linalg.yield %19 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%18) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %18 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %20 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%21 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %22 = bufferization.to_tensor %21 : memref<?x?x?x?xf32>
    scf.yield %22 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %20 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%21 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %22 = bufferization.to_tensor %21 : memref<?x?x?x?xf32>
    scf.yield %22 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %dim_4 = tensor.dim %13, %c2 : tensor<?x?x?x?xf32>
  %14 = arith.cmpi eq, %dim_4, %c1 : index
  %15 = scf.if %14 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %13, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %13, %c3 : tensor<?x?x?x?xf32>
    %20 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%20 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %21 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %15, %c3 : tensor<?x?x?x?xf32>
  %16 = arith.cmpi eq, %dim_5, %c1 : index
  %17 = scf.if %16 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %15, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %15, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %15, %c2 : tensor<?x?x?x?xf32>
    %20 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : tensor<?x?x?x?xf32>) outs(%20 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %21 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %15 : tensor<?x?x?x?xf32>
  }
  %18 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : tensor<?x?x?x?xf32>) outs(%18 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %in : f32
    linalg.yield %20 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%19) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %19 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %21 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %22 = bufferization.to_memref %21 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%22 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %22 : memref<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %21 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %22 = bufferization.to_memref %21 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%22 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %22 : memref<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %14, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = arith.cmpi eq, %dim_4, %c1 : index
  %16 = scf.if %15 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %13, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %13, %c3 : tensor<?x?x?x?xf32>
    %21 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %22 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%21 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %22 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %16, %c3 : tensor<?x?x?x?xf32>
  %17 = arith.cmpi eq, %dim_5, %c1 : index
  %18 = scf.if %17 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %16, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %16, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %16, %c2 : tensor<?x?x?x?xf32>
    %21 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %22 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%16 : tensor<?x?x?x?xf32>) outs(%21 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %22 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %16 : tensor<?x?x?x?xf32>
  }
  %19 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%18 : tensor<?x?x?x?xf32>) outs(%19 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %21 = arith.addf %in, %in : f32
    linalg.yield %21 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%20) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %20 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %22 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %23 = bufferization.to_memref %22 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%23 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %24 = bufferization.to_tensor %23 : memref<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %22 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %23 = bufferization.to_memref %22 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%23 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %24 = bufferization.to_tensor %23 : memref<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %15, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = arith.cmpi eq, %dim_4, %c1 : index
  %17 = scf.if %16 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %14, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = tensor.dim %13, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %13, %c3 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %17, %c3 : tensor<?x?x?x?xf32>
  %18 = arith.cmpi eq, %dim_5, %c1 : index
  %19 = scf.if %18 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %17, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %17, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %17, %c2 : tensor<?x?x?x?xf32>
    %22 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %23 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %17 : tensor<?x?x?x?xf32>
  }
  %20 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%19 : tensor<?x?x?x?xf32>) outs(%20 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %22 = arith.addf %in, %in : f32
    linalg.yield %22 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%21) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %21 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %23 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %24 = bufferization.to_memref %23 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%24 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %25 = bufferization.to_tensor %24 : memref<?x?x?x?xf32>
    scf.yield %25 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %23 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %24 = bufferization.to_memref %23 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%24 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %25 = bufferization.to_tensor %24 : memref<?x?x?x?xf32>
    scf.yield %25 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %16, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = arith.cmpi eq, %dim_4, %c1 : index
  %18 = scf.if %17 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %15, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %14, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = tensor.dim %13, %c3 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %18, %c3 : tensor<?x?x?x?xf32>
  %19 = arith.cmpi eq, %dim_5, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %18, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %18, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %18, %c2 : tensor<?x?x?x?xf32>
    %23 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%18 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %24 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %18 : tensor<?x?x?x?xf32>
  }
  %21 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %22 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%20 : tensor<?x?x?x?xf32>) outs(%21 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %23 = arith.addf %in, %in : f32
    linalg.yield %23 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%22) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %22 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %24 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %25 = bufferization.to_memref %24 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%25 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %25 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %24 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %25 = bufferization.to_memref %24 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%25 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %25 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %17, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = arith.cmpi eq, %dim_4, %c1 : index
  %19 = scf.if %18 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %16, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %15, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %14, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %24 = tensor.empty(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%24 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %25 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %19, %c3 : tensor<?x?x?x?xf32>
  %20 = arith.cmpi eq, %dim_5, %c1 : index
  %21 = scf.if %20 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %19, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %19, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %19, %c2 : tensor<?x?x?x?xf32>
    %24 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%19 : tensor<?x?x?x?xf32>) outs(%24 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %25 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %19 : tensor<?x?x?x?xf32>
  }
  %22 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%21 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %24 = arith.addf %in, %in : f32
    linalg.yield %24 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%23) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %23 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %24 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %25 = bufferization.to_memref %24 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%25 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %25 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %24 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %25 = bufferization.to_memref %24 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%25 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %25 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %17, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = arith.cmpi eq, %dim_4, %c1 : index
  %19 = scf.if %18 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %16, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %15, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %14, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %24 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%13 : tensor<?x?x?x?xf32>) outs(%24 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %25 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %19, %c3 : tensor<?x?x?x?xf32>
  %20 = arith.cmpi eq, %dim_5, %c1 : index
  %21 = scf.if %20 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %19, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %19, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %19, %c2 : tensor<?x?x?x?xf32>
    %24 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%19 : tensor<?x?x?x?xf32>) outs(%24 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %25 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %19 : tensor<?x?x?x?xf32>
  }
  %22 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%21 : tensor<?x?x?x?xf32>) outs(%22 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %24 = arith.addf %in, %in : f32
    linalg.yield %24 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%23) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %23 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %25 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %26 = bufferization.to_memref %25 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%26 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %27 = bufferization.to_tensor %26 : memref<?x?x?x?xf32>
    scf.yield %27 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %25 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %26 = bufferization.to_memref %25 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%26 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %27 = bufferization.to_tensor %26 : memref<?x?x?x?xf32>
    scf.yield %27 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %25 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %26 = bufferization.to_memref %25 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%26 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %27 = bufferization.to_tensor %26 : memref<?x?x?x?xf32>
    scf.yield %27 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %dim_5 = tensor.dim %20, %c3 : tensor<?x?x?x?xf32>
  %21 = arith.cmpi eq, %dim_5, %c1 : index
  %22 = scf.if %21 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %20, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %20, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %20, %c2 : tensor<?x?x?x?xf32>
    %25 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %26 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%20 : tensor<?x?x?x?xf32>) outs(%25 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %23 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%22 : tensor<?x?x?x?xf32>) outs(%23 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %25 = arith.addf %in, %in : f32
    linalg.yield %25 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%24) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %24 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %26 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %27 = bufferization.to_memref %26 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%27 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %28 = bufferization.to_tensor %27 : memref<?x?x?x?xf32>
    scf.yield %28 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %26 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %27 = bufferization.to_memref %26 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%27 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %28 = bufferization.to_tensor %27 : memref<?x?x?x?xf32>
    scf.yield %28 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %26 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %27 = bufferization.to_memref %26 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%27 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %28 = bufferization.to_tensor %27 : memref<?x?x?x?xf32>
    scf.yield %28 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %21, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = arith.cmpi eq, %dim_5, %c1 : index
  %23 = scf.if %22 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = tensor.dim %20, %c0 : tensor<?x?x?x?xf32>
    %dim_7 = tensor.dim %20, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %20, %c2 : tensor<?x?x?x?xf32>
    %26 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %27 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%20 : tensor<?x?x?x?xf32>) outs(%26 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %27 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %24 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%23 : tensor<?x?x?x?xf32>) outs(%24 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %in : f32
    linalg.yield %26 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%25) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %25 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %27 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %28 = bufferization.to_memref %27 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%28 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %29 = bufferization.to_tensor %28 : memref<?x?x?x?xf32>
    scf.yield %29 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %27 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %28 = bufferization.to_memref %27 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%28 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %29 = bufferization.to_tensor %28 : memref<?x?x?x?xf32>
    scf.yield %29 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %27 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %28 = bufferization.to_memref %27 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%28 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %29 = bufferization.to_tensor %28 : memref<?x?x?x?xf32>
    scf.yield %29 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %22, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = arith.cmpi eq, %dim_5, %c1 : index
  %24 = scf.if %23 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %21, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = tensor.dim %20, %c1 : tensor<?x?x?x?xf32>
    %dim_8 = tensor.dim %20, %c2 : tensor<?x?x?x?xf32>
    %27 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %28 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%20 : tensor<?x?x?x?xf32>) outs(%27 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %28 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %25 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %26 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%24 : tensor<?x?x?x?xf32>) outs(%25 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %27 = arith.addf %in, %in : f32
    linalg.yield %27 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%26) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %26 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %28 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%29 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %30 = bufferization.to_tensor %29 : memref<?x?x?x?xf32>
    scf.yield %30 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %28 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%29 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %30 = bufferization.to_tensor %29 : memref<?x?x?x?xf32>
    scf.yield %30 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %28 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%29 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %30 = bufferization.to_tensor %29 : memref<?x?x?x?xf32>
    scf.yield %30 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %23, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = arith.cmpi eq, %dim_5, %c1 : index
  %25 = scf.if %24 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %22, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %21, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = tensor.dim %20, %c2 : tensor<?x?x?x?xf32>
    %28 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %29 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%20 : tensor<?x?x?x?xf32>) outs(%28 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %29 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %26 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %27 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%25 : tensor<?x?x?x?xf32>) outs(%26 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.addf %in, %in : f32
    linalg.yield %28 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%27) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %27 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.dim
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %29 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %30 = bufferization.to_memref %29 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%30 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %31 = bufferization.to_tensor %30 : memref<?x?x?x?xf32>
    scf.yield %31 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %29 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %30 = bufferization.to_memref %29 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%30 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %31 = bufferization.to_tensor %30 : memref<?x?x?x?xf32>
    scf.yield %31 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %29 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %30 = bufferization.to_memref %29 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%30 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %31 = bufferization.to_tensor %30 : memref<?x?x?x?xf32>
    scf.yield %31 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %24, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = arith.cmpi eq, %dim_5, %c1 : index
  %26 = scf.if %25 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %23, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %22, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %21, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %29 = tensor.empty(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %30 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%20 : tensor<?x?x?x?xf32>) outs(%29 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %30 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %27 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %28 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%26 : tensor<?x?x?x?xf32>) outs(%27 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %29 = arith.addf %in, %in : f32
    linalg.yield %29 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%28) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %28 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %29 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %30 = bufferization.to_memref %29 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%30 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %31 = bufferization.to_tensor %30 : memref<?x?x?x?xf32>
    scf.yield %31 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %29 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %30 = bufferization.to_memref %29 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%30 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %31 = bufferization.to_tensor %30 : memref<?x?x?x?xf32>
    scf.yield %31 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %29 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %30 = bufferization.to_memref %29 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%30 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %31 = bufferization.to_tensor %30 : memref<?x?x?x?xf32>
    scf.yield %31 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %24, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = arith.cmpi eq, %dim_5, %c1 : index
  %26 = scf.if %25 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %23, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %22, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %21, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %29 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %30 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%20 : tensor<?x?x?x?xf32>) outs(%29 : tensor<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<?x?x?x?xf32>
    scf.yield %30 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %27 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %28 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%26 : tensor<?x?x?x?xf32>) outs(%27 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %29 = arith.addf %in, %in : f32
    linalg.yield %29 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%28) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %28 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %30 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
    scf.yield %32 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %30 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
    scf.yield %32 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %30 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
    scf.yield %32 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %25, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = arith.cmpi eq, %dim_5, %c1 : index
  %27 = scf.if %26 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %24, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %23, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %22, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %30 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
    scf.yield %32 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %28 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %29 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%27 : tensor<?x?x?x?xf32>) outs(%28 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %30 = arith.addf %in, %in : f32
    linalg.yield %30 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%29) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %29 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %30 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
    scf.yield %32 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %30 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
    scf.yield %32 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %30 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
    scf.yield %32 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %25, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = arith.cmpi eq, %dim_5, %c1 : index
  %27 = scf.if %26 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %24, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %23, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %22, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %30 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
    scf.yield %32 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %28 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %29 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%27 : tensor<?x?x?x?xf32>) outs(%28 : tensor<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %30 = arith.addf %in, %in : f32
    linalg.yield %30 : f32
  } -> tensor<?x?x?x?xf32>
  "llh.encoding_bind"(%29) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %29 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %32 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%33 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %34 = bufferization.to_tensor %33 : memref<?x?x?x?xf32>
    scf.yield %34 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %32 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%33 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %34 = bufferization.to_tensor %33 : memref<?x?x?x?xf32>
    scf.yield %34 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %32 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%33 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %34 = bufferization.to_tensor %33 : memref<?x?x?x?xf32>
    scf.yield %34 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %25, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = arith.cmpi eq, %dim_5, %c1 : index
  %27 = scf.if %26 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %24, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %23, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %22, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %32 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%33 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %34 = bufferization.to_tensor %33 : memref<?x?x?x?xf32>
    scf.yield %34 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %28 = bufferization.to_memref %27 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %29 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %30 = bufferization.to_memref %29 : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%28 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%30 : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %32 = arith.addf %in, %in : f32
    linalg.yield %32 : f32
  }
  %31 = bufferization.to_tensor %30 : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%31) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %31 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: func.return
func.func @main(%arg0: tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%arg0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %4, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %3, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %2, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %1, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = arith.cmpi eq, %dim, %c1 : index
  %6 = scf.if %5 -> (tensor<?x?x?x?xf32>) {
    %32 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%33 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %34 = bufferization.to_tensor %33 : memref<?x?x?x?xf32>
    scf.yield %34 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %arg0 : tensor<?x?x?x?xf32>
  }
  %7 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %6 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %11, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = arith.cmpi eq, %dim_3, %c1 : index
  %13 = scf.if %12 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %10, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %9, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %8, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %32 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%33 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %34 = bufferization.to_tensor %33 : memref<?x?x?x?xf32>
    scf.yield %34 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %6 : tensor<?x?x?x?xf32>
  }
  %14 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %13 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = arith.cmpi eq, %dim_4, %c1 : index
  %20 = scf.if %19 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %17, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %16, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %15, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %32 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%33 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %34 = bufferization.to_tensor %33 : memref<?x?x?x?xf32>
    scf.yield %34 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %13 : tensor<?x?x?x?xf32>
  }
  %21 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %20 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %25, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = arith.cmpi eq, %dim_5, %c1 : index
  %27 = scf.if %26 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %24, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %23, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %22, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %32 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%33 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %34 = bufferization.to_tensor %33 : memref<?x?x?x?xf32>
    scf.yield %34 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %20 : tensor<?x?x?x?xf32>
  }
  %28 = bufferization.to_memref %27 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %29 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %30 = bufferization.to_memref %29 : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%28 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%30 : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %32 = arith.addf %in, %in : f32
    linalg.yield %32 : f32
  }
  %31 = bufferization.to_tensor %30 : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%31) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  return %31 : tensor<?x?x?x?xf32>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: func.func
func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>> attributes {entrance} {
  %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %5, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %4, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %3, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %2, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %6 = arith.cmpi eq, %dim, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %34 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %0 : tensor<?x?x?x?xf32>
  }
  %8 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %13 = arith.cmpi eq, %dim_3, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %11, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %10, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %9, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %15 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %19, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %20 = arith.cmpi eq, %dim_4, %c1 : index
  %21 = scf.if %20 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %18, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %17, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %16, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %14 : tensor<?x?x?x?xf32>
  }
  %22 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %26, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %27 = arith.cmpi eq, %dim_5, %c1 : index
  %28 = scf.if %27 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %25, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %24, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %23, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%22 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %21 : tensor<?x?x?x?xf32>
  }
  %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %30 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%29 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %34 = arith.addf %in, %in : f32
    linalg.yield %34 : f32
  }
  %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%32) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  return %33 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>> attributes {entrance} {
  %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %5, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %4, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %3, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %2, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %6 = arith.cmpi eq, %dim, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %0 : tensor<?x?x?x?xf32>
  }
  %8 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %13 = arith.cmpi eq, %dim_3, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %11, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %10, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %9, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_0, %dim_7, %dim_8) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %15 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %19, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %20 = arith.cmpi eq, %dim_4, %c1 : index
  %21 = scf.if %20 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %18, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %17, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %16, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %14 : tensor<?x?x?x?xf32>
  }
  %22 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %26, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %27 = arith.cmpi eq, %dim_5, %c1 : index
  %28 = scf.if %27 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %25, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %24, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %23, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%22 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %21 : tensor<?x?x?x?xf32>
  }
  %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %30 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%29 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %34 = arith.addf %in, %in : f32
    linalg.yield %34 : f32
  }
  %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%32) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  return %33 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>> attributes {entrance} {
  %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %5, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %4, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %3, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %2, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %6 = arith.cmpi eq, %dim, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %0 : tensor<?x?x?x?xf32>
  }
  %8 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %13 = arith.cmpi eq, %dim_3, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %11, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %10, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %9, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %15 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %19, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %20 = arith.cmpi eq, %dim_4, %c1 : index
  %21 = scf.if %20 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %18, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %17, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %16, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_1, %dim_8) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %14 : tensor<?x?x?x?xf32>
  }
  %22 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %26, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %27 = arith.cmpi eq, %dim_5, %c1 : index
  %28 = scf.if %27 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %25, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %24, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %23, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%22 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %21 : tensor<?x?x?x?xf32>
  }
  %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %30 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%29 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %34 = arith.addf %in, %in : f32
    linalg.yield %34 : f32
  }
  %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%32) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  return %33 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>> attributes {entrance} {
  %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %5, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %4, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %3, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %2, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %6 = arith.cmpi eq, %dim, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %0 : tensor<?x?x?x?xf32>
  }
  %8 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %13 = arith.cmpi eq, %dim_3, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %11, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %10, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %9, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %15 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %19, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %20 = arith.cmpi eq, %dim_4, %c1 : index
  %21 = scf.if %20 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %18, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %17, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %16, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim_6, %dim_7, %dim_1, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %14 : tensor<?x?x?x?xf32>
  }
  %22 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %26, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %27 = arith.cmpi eq, %dim_5, %c1 : index
  %28 = scf.if %27 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %25, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %24, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %23, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %34 = bufferization.alloc_tensor(%dim_6, %dim_7, %dim_8, %dim_2) : tensor<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%22 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %21 : tensor<?x?x?x?xf32>
  }
  %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %30 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%29 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %34 = arith.addf %in, %in : f32
    linalg.yield %34 : f32
  }
  %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%32) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  return %33 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>> attributes {entrance} {
  %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %5, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %4, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %3, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %2, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %6 = arith.cmpi eq, %dim, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %0 : tensor<?x?x?x?xf32>
  }
  %8 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %13 = arith.cmpi eq, %dim_3, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %11, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %10, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %9, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %15 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %19, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %20 = arith.cmpi eq, %dim_4, %c1 : index
  %21 = scf.if %20 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %18, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %17, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %16, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim_6, %dim_7, %dim_1, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %14 : tensor<?x?x?x?xf32>
  }
  %22 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %26, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %27 = arith.cmpi eq, %dim_5, %c1 : index
  %28 = scf.if %27 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %25, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %24, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %23, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim_6, %dim_7, %dim_8, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%22 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %21 : tensor<?x?x?x?xf32>
  }
  %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %30 = bufferization.alloc_tensor(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
  %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%29 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %34 = arith.addf %in, %in : f32
    linalg.yield %34 : f32
  }
  %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%32) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  return %33 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>> attributes {entrance} {
  %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %2 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %3 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = bufferization.to_memref %0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %5, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %4, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %3, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %2, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %6 = arith.cmpi eq, %dim, %c1 : index
  %7 = scf.if %6 -> (tensor<?x?x?x?xf32>) {
    %alloc_6 = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc_6 : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %0 : tensor<?x?x?x?xf32>
  }
  %8 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %9 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %10 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = bufferization.to_memref %7 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %13 = arith.cmpi eq, %dim_3, %c1 : index
  %14 = scf.if %13 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %11, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %10, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %9, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %7 : tensor<?x?x?x?xf32>
  }
  %15 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %16 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %17 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = bufferization.to_memref %14 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %19, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %20 = arith.cmpi eq, %dim_4, %c1 : index
  %21 = scf.if %20 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %18, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %17, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %16, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_1, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%15 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %14 : tensor<?x?x?x?xf32>
  }
  %22 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %23 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %24 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %25 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %26 = bufferization.to_memref %21 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %26, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %27 = arith.cmpi eq, %dim_5, %c1 : index
  %28 = scf.if %27 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %25, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %24, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %23, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_8, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    %34 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    %35 = bufferization.to_memref %34 : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%22 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%35 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %36 = bufferization.to_tensor %35 : memref<?x?x?x?xf32>
    scf.yield %36 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %21 : tensor<?x?x?x?xf32>
  }
  %29 = bufferization.to_memref %28 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
  %30 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
  %31 = bufferization.to_memref %30 : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%29 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%31 : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %34 = arith.addf %in, %in : f32
    linalg.yield %34 : f32
  }
  %32 = bufferization.to_tensor %31 : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%32) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %33 = bufferization.to_memref %32 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  return %33 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
}
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface::Trait<mlir::TypeID::get<mlir::bufferization::BufferizableOpInterface::Trait>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultShape<mlir::TypeID::get<mlir::OpTrait::SameOperandsAndResultShape>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::BaseMemRefType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::BaseMemRefType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::MemRefType>::Impl<mlir::TypeID::get<mlir::OpTrait::OneTypedResult<mlir::MemRefType>::Impl>()::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ViewLikeOpInterface::Trait<mlir::TypeID::get<mlir::ViewLikeOpInterface::Trait>()::Empty>)
// -----// IR Dump After OneShotBufferize (one-shot-bufferize) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32> attributes {entrance} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = memref.dim %arg0, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_2 = memref.dim %arg0, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %1 = arith.cmpi eq, %dim, %c1 : index
    %2 = scf.if %1 -> (tensor<?x?x?x?xf32>) {
      %alloc_6 = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_6 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_6 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %0 : tensor<?x?x?x?xf32>
    }
    %3 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %4 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %5 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %6 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %7 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_3 = memref.dim %7, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %8 = arith.cmpi eq, %dim_3, %c1 : index
    %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %6, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %5, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %4, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %2 : tensor<?x?x?x?xf32>
    }
    %10 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %11 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %12 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %13 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %14 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_4 = memref.dim %14, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %15 = arith.cmpi eq, %dim_4, %c1 : index
    %16 = scf.if %15 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %13, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %11, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_1, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %9 : tensor<?x?x?x?xf32>
    }
    %17 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %18 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %19 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %20 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %21 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_5 = memref.dim %21, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %22 = arith.cmpi eq, %dim_5, %c1 : index
    %23 = scf.if %22 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %20, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %19, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_8, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %16 : tensor<?x?x?x?xf32>
    }
    %24 = bufferization.to_memref %23 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%24 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %in : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    "llh.encoding_bind"(%25) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %cast = memref.cast %alloc : memref<?x?x?x?xf32> to memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    return %alloc : memref<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x55db9a9287a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa5a8e0) {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69b60) {
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa738d0) {
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.symbolic_int'(0x55db9aa69bb0) {
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_tensor'(0x55db9aa9c240) {
  %0 = "bufferization.to_tensor"(%arg0) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8e550) {
  %1 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8dd90) {
  %2 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8da10) {
  %3 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8c790) {
  %4 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa93550) {
  %5 = "memref.dim"(%arg0, %4) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9a927bc0) {
  %6 = "memref.dim"(%arg0, %3) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa8da80) {
  %7 = "memref.dim"(%arg0, %2) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa8de30) {
  %8 = "memref.dim"(%arg0, %1) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa93ab0) {
  %9 = "arith.cmpi"(%5, %3) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa734e0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.alloc'(0x55db9aaa3710) {
  %55 = "memref.alloc"(%5, %6, %7, %8) <{alignment = 64 : i64, operandSegmentSizes = array<i32: 4, 0>}> : (index, index, index, index) -> memref<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a960660) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa8bf60) {
  "linalg.yield"(%arg9) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_tensor'(0x55db9aa73ea0) {
  %56 = "bufferization.to_tensor"(%55) : (memref<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa73180) {
  "scf.yield"(%56) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa5b820) {
  "scf.yield"(%0) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa98710) {
  %11 = "bufferization.to_memref"(%10) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa9ad40) {
  %12 = "bufferization.to_memref"(%10) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa9d6b0) {
  %13 = "bufferization.to_memref"(%10) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa988d0) {
  %14 = "bufferization.to_memref"(%10) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa9f110) {
  %15 = "bufferization.to_memref"(%10) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa8e5f0) {
  %16 = "memref.dim"(%15, %3) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa96bb0) {
  %17 = "arith.cmpi"(%16, %3) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa97e10) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa96ad0) {
  %50 = "memref.dim"(%14, %4) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa971c0) {
  %51 = "memref.dim"(%13, %2) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa97340) {
  %52 = "memref.dim"(%12, %1) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.alloc'(0x55db9aa8fe50) {
  %53 = "memref.alloc"(%50, %6, %51, %52) <{alignment = 64 : i64, operandSegmentSizes = array<i32: 4, 0>}> : (index, index, index, index) -> memref<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa93fe0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa97b50) {
  "linalg.yield"(%arg7) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_tensor'(0x55db9aaa3010) {
  %54 = "bufferization.to_tensor"(%53) : (memref<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97c30) {
  "scf.yield"(%54) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa97d50) {
  "scf.yield"(%10) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa973f0) {
  %19 = "bufferization.to_memref"(%18) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa78ab0) {
  %20 = "bufferization.to_memref"(%18) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa9abc0) {
  %21 = "bufferization.to_memref"(%18) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa9c3d0) {
  %22 = "bufferization.to_memref"(%18) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa9d850) {
  %23 = "bufferization.to_memref"(%18) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa974c0) {
  %24 = "memref.dim"(%23, %2) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa98050) {
  %25 = "arith.cmpi"(%24, %3) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9a2c0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa97f70) {
  %45 = "memref.dim"(%22, %4) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa98660) {
  %46 = "memref.dim"(%21, %3) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa987e0) {
  %47 = "memref.dim"(%20, %1) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.alloc'(0x55db9aa9d080) {
  %48 = "memref.alloc"(%45, %46, %7, %47) <{alignment = 64 : i64, operandSegmentSizes = array<i32: 4, 0>}> : (index, index, index, index) -> memref<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9c4e0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9a000) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_tensor'(0x55db9aa8dcb0) {
  %49 = "bufferization.to_tensor"(%48) : (memref<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a0e0) {
  "scf.yield"(%49) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9a200) {
  "scf.yield"(%18) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9a910790) {
  %27 = "bufferization.to_memref"(%26) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9a9123e0) {
  %28 = "bufferization.to_memref"(%26) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa69a50) {
  %29 = "bufferization.to_memref"(%26) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9a912210) {
  %30 = "bufferization.to_memref"(%26) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa73210) {
  %31 = "bufferization.to_memref"(%26) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa98960) {
  %32 = "memref.dim"(%31, %1) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x55db9aa9a500) {
  %33 = "arith.cmpi"(%32, %3) <{predicate = 0 : i64}> : (index, index) -> i1

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.if'(0x55db9aa9b850) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa9a420) {
  %40 = "memref.dim"(%30, %4) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa9ab10) {
  %41 = "memref.dim"(%29, %3) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.dim'(0x55db9aa9ac90) {
  %42 = "memref.dim"(%28, %2) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>, index) -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.alloc'(0x55db9aa9cf80) {
  %43 = "memref.alloc"(%40, %41, %42, %8) <{alignment = 64 : i64, operandSegmentSizes = array<i32: 4, 0>}> : (index, index, index, index) -> memref<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9a957cf0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aa9b4a0) {
  "linalg.yield"(%arg3) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_tensor'(0x55db9a8e7330) {
  %44 = "bufferization.to_tensor"(%43) : (memref<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b670) {
  "scf.yield"(%44) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x55db9aa9b790) {
  "scf.yield"(%26) : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_memref'(0x55db9aa9cc50) {
  %35 = "bufferization.to_memref"(%34) : (tensor<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.alloc'(0x55db9aaa6580) {
  %36 = "memref.alloc"(%5, %6, %7, %8) <{alignment = 64 : i64, operandSegmentSizes = array<i32: 4, 0>}> : (index, index, index, index) -> memref<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x55db9aa9bec0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x55db9aa9f910) {
  %39 = "arith.addf"(%arg1, %arg1) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x55db9aaa02d0) {
  "linalg.yield"(%39) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_tensor'(0x55db9aa9c1a0) {
  %37 = "bufferization.to_tensor"(%36) : (memref<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa6ddd0) {
  "llh.encoding_bind"(%37) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'memref.cast'(0x55db9aa96920) {
  %38 = "memref.cast"(%36) : (memref<?x?x?x?xf32>) -> memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x55db9a928210) {
  "func.return"(%36) : (memref<?x?x?x?xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After FuncBufferize (func-bufferize) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32> attributes {entrance} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = memref.dim %arg0, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_2 = memref.dim %arg0, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %1 = arith.cmpi eq, %dim, %c1 : index
    %2 = scf.if %1 -> (tensor<?x?x?x?xf32>) {
      %alloc_6 = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_6 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_6 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %0 : tensor<?x?x?x?xf32>
    }
    %3 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %4 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %5 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %6 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %7 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_3 = memref.dim %7, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %8 = arith.cmpi eq, %dim_3, %c1 : index
    %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %6, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %5, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %4, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %2 : tensor<?x?x?x?xf32>
    }
    %10 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %11 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %12 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %13 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %14 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_4 = memref.dim %14, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %15 = arith.cmpi eq, %dim_4, %c1 : index
    %16 = scf.if %15 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %13, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %11, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_1, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %9 : tensor<?x?x?x?xf32>
    }
    %17 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %18 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %19 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %20 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %21 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_5 = memref.dim %21, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %22 = arith.cmpi eq, %dim_5, %c1 : index
    %23 = scf.if %22 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %20, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %19, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_8, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %16 : tensor<?x?x?x?xf32>
    }
    %24 = bufferization.to_memref %23 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%24 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %in : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    "llh.encoding_bind"(%25) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %cast = memref.cast %alloc : memref<?x?x?x?xf32> to memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    return %alloc : memref<?x?x?x?xf32>
  }
}


// -----// IR Dump After BufferResultsToOutParams (buffer-results-to-out-params) //----- //
#map = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>
module attributes {builtin.gloabal_layout = "NCHW"} {
  "llh.symbolic_int"() <{sym_name = "s3"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s2"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s1"}> : () -> ()
  "llh.symbolic_int"() <{sym_name = "s0"}> : () -> ()
  func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32> attributes {entrance} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %dim = memref.dim %arg0, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_2 = memref.dim %arg0, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %1 = arith.cmpi eq, %dim, %c1 : index
    %2 = scf.if %1 -> (tensor<?x?x?x?xf32>) {
      %alloc_6 = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_6 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_6 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %0 : tensor<?x?x?x?xf32>
    }
    %3 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %4 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %5 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %6 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %7 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_3 = memref.dim %7, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %8 = arith.cmpi eq, %dim_3, %c1 : index
    %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %6, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %5, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %4, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map2, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %2 : tensor<?x?x?x?xf32>
    }
    %10 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %11 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %12 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %13 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %14 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_4 = memref.dim %14, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %15 = arith.cmpi eq, %dim_4, %c1 : index
    %16 = scf.if %15 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %13, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %11, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_1, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map3, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %9 : tensor<?x?x?x?xf32>
    }
    %17 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %18 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %19 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %20 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %21 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_5 = memref.dim %21, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %22 = arith.cmpi eq, %dim_5, %c1 : index
    %23 = scf.if %22 -> (tensor<?x?x?x?xf32>) {
      %dim_6 = memref.dim %20, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_7 = memref.dim %19, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %dim_8 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
      %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_8, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
      linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
      %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
      scf.yield %26 : tensor<?x?x?x?xf32>
    } else {
      scf.yield %16 : tensor<?x?x?x?xf32>
    }
    %24 = bufferization.to_memref %23 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%24 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %in : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
    "llh.encoding_bind"(%25) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
    %cast = memref.cast %alloc : memref<?x?x?x?xf32> to memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    return %alloc : memref<?x?x?x?xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x55db9a928290) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'bufferization.to_tensor'(0x55db9aa9c240) {
  %0 = "bufferization.to_tensor"(%arg0) : (memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> tensor<?x?x?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'bufferization.to_tensor -> ()' {
Trying to match "{anonymous}::BufferizeToTensorOp"
    ** Replace : 'bufferization.to_tensor'(0x55db9aa9c240)
"{anonymous}::BufferizeToTensorOp" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %arg0, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = arith.cmpi eq, %dim, %c1 : index
  %2 = scf.if %1 -> (tensor<?x?x?x?xf32>) {
    %alloc_6 = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_6 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %alloc_6 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %0 : tensor<?x?x?x?xf32>
  }
  %3 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %6 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %7 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %7, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = arith.cmpi eq, %dim_3, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %6, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %5, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %4, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %2 : tensor<?x?x?x?xf32>
  }
  %10 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %13 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %14 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %14, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = arith.cmpi eq, %dim_4, %c1 : index
  %16 = scf.if %15 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %13, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %11, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_1, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %9 : tensor<?x?x?x?xf32>
  }
  %17 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %20 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %21 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %21, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = arith.cmpi eq, %dim_5, %c1 : index
  %23 = scf.if %22 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %20, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %19, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_8, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %16 : tensor<?x?x?x?xf32>
  }
  %24 = bufferization.to_memref %23 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%24 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %in : f32
    linalg.yield %26 : f32
  }
  %25 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%25) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %cast = memref.cast %alloc : memref<?x?x?x?xf32> to memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  return %alloc : memref<?x?x?x?xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8e550) {
  %1 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8dd90) {
  %2 = "arith.constant"() <{value = 2 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8da10) {
  %3 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x55db9aa8c790) {
  %4 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'llh.encoding_bind'(0x55db9aa5a860) {
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
/home/lfr/LLCompiler/test/model_ir/Add.mlir:2:3: error: failed to legalize operation 'llh.encoding_bind'
  func.func @main(%0 : tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>  attributes {"entrance"}{
  ^
/home/lfr/LLCompiler/test/model_ir/Add.mlir:2:3: note: see current operation: "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
// -----// IR Dump After FinalizingBufferize Failed (finalizing-bufferize) //----- //
func.func @main(%arg0: memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) -> memref<?x?x?x?xf32> attributes {entrance} {
  %0 = bufferization.to_tensor %arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  "llh.encoding_bind"(%0) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_2 = memref.dim %arg0, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %1 = arith.cmpi eq, %dim, %c1 : index
  %2 = scf.if %1 -> (tensor<?x?x?x?xf32>) {
    %alloc_6 = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_6 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %alloc_6 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %0 : tensor<?x?x?x?xf32>
  }
  %3 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %4 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %5 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %6 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %7 = bufferization.to_memref %2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_3 = memref.dim %7, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %8 = arith.cmpi eq, %dim_3, %c1 : index
  %9 = scf.if %8 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %6, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %5, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %4, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_0, %dim_7, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %2 : tensor<?x?x?x?xf32>
  }
  %10 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %11 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %12 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %13 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %14 = bufferization.to_memref %9 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_4 = memref.dim %14, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %15 = arith.cmpi eq, %dim_4, %c1 : index
  %16 = scf.if %15 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %13, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %12, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %11, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_1, %dim_8) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%10 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %9 : tensor<?x?x?x?xf32>
  }
  %17 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %18 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %19 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %20 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %21 = bufferization.to_memref %16 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %dim_5 = memref.dim %21, %c3 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %22 = arith.cmpi eq, %dim_5, %c1 : index
  %23 = scf.if %22 -> (tensor<?x?x?x?xf32>) {
    %dim_6 = memref.dim %20, %c0 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_7 = memref.dim %19, %c1 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %dim_8 = memref.dim %18, %c2 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
    %alloc_9 = memref.alloc(%dim_6, %dim_7, %dim_8, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%17 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc_9 : memref<?x?x?x?xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %26 = bufferization.to_tensor %alloc_9 : memref<?x?x?x?xf32>
    scf.yield %26 : tensor<?x?x?x?xf32>
  } else {
    scf.yield %16 : tensor<?x?x?x?xf32>
  }
  %24 = bufferization.to_memref %23 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  %alloc = memref.alloc(%dim, %dim_0, %dim_1, %dim_2) {alignment = 64 : i64} : memref<?x?x?x?xf32>
  linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%24 : memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>) outs(%alloc : memref<?x?x?x?xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %in : f32
    linalg.yield %26 : f32
  }
  %25 = bufferization.to_tensor %alloc : memref<?x?x?x?xf32>
  "llh.encoding_bind"(%25) <{encoding = #llh.encoding<shapes = @s0, @s1, @s2, @s3>}> : (tensor<?x?x?x?xf32>) -> ()
  %cast = memref.cast %alloc : memref<?x?x?x?xf32> to memref<?x?x?x?xf32, strided<[?, ?, ?, ?], offset: ?>>
  return %alloc : memref<?x?x?x?xf32>
}

