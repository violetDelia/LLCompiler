# [RFC] Add TanOp to the StableHLO specification

Status: Review<br/>
Initial version: 03/12/2024<br/>
Last updated: 03/15/2024<br/>
Discussion thread: [GitHub](https://github.com/openxla/stablehlo/pull/2101)

## Motivation

Several features have been added to MHLO in the past year, which frameworks want
to leverage and members of the community have made requests for them as well.
This includes `TanOp`.

Frameworks and Compilers both want the `tan` op. Jax has [`jnp.tan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tan.html),
PyTorch has [`torch.tan`](https://pytorch.org/docs/stable/generated/torch.tan.html).
On the Compilers side, XLA has [`mhlo.tan`](https://github.com/tensorflow/mlir-hlo/blob/master/mhlo/IR/hlo_ops.td#L633).
StableHLO doesn't support `tan` op, but there is an open ticket which requests
adding this feature in
[#1358](https://github.com/openxla/stablehlo/issues/1358)

## Proposed Specification

### tan

#### Semantics

Performs element-wise tangent operation on the `operand` tensor and
produces a `result` tensor. Depending on the element type, does the following:

* For floats: `tan` from IEEE-754.
* For complex numbers: complex tangent.
* For quantized types: `dequantize_op_quantize(tan, operand, type(result))`.

#### Inputs

| Label | Name      | Type                                                                    | Constraints |
|-------|-----------|-------------------------------------------------------------------------|-------------|
| (I1)  | `operand` | tensor of floating-point or complex type or per-tensor quantized tensor | (C1)        |

#### Outputs

| Name     | Type                                                                    | Constraints |
|----------|-------------------------------------------------------------------------|-------------|
| `result` | tensor of floating-point or complex type or per-tensor quantized tensor | (C1)        |

#### Constraints

* (C1) `baseline_type(operand) = baseline_type(result)`.

#### Examples

```mlir
// %operand: [
//            [0.0, 1.57079632],       // [0, pi/2]
//            [3.14159265, 4.71238898] // [pi, 3pi/2]
//           ]
%result = "stablehlo.tan"(%operand) : (tensor<2x2xf64>) -> tensor<2x2xf64>
// %result: [
//           [0.0, 1.63312e+16],
//           [0.0, 5.44375e+15]
//          ]
```

## TanOp and Compatibility

`TanOp` can be represented in StableHLO, with some potential
numerics differences from multiple operations
(like `TanOp(x) = DivOp(SineOp(x), CosineOp(x)`).
If there is a use case, forward compatibility is possible via decomposition,
and the underlying math can be pattern matched into `TanOp`. We propose to make
this an opt in pass that doesn't live in the IR upgrade/downgrade pipeline.
There may be users who want the ops to remain separate.
